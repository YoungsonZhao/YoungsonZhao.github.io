<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="msvalidate.01" content="3caebbd6f719430c88b5a8dc11df5a98">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Helvetica:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://blog.zhaoyongsheng.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"default"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":true,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: true,
    lazyload: true,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本文主要是OpenAI的GPT系列论文的阅读笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="Roadmap To Learn Large Language Model Of GPT Series">
<meta property="og:url" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/index.html">
<meta property="og:site_name" content="Dr. Zhao&#39;s Blog">
<meta property="og:description" content="本文主要是OpenAI的GPT系列论文的阅读笔记。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/In-Context-Examples-Accuracy.png">
<meta property="og:image" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Few-One-Zero-Shot.png">
<meta property="og:image" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/GPT-3-Model-Parameters.png">
<meta property="og:image" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Performance-wrt-Model-Size.png">
<meta property="article:published_time" content="2024-12-21T08:33:09.000Z">
<meta property="article:modified_time" content="2024-12-21T08:33:09.000Z">
<meta property="article:author" content="Dr. Zhao">
<meta property="article:tag" content="GPT">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/In-Context-Examples-Accuracy.png">

<link rel="canonical" href="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Roadmap To Learn Large Language Model Of GPT Series | Dr. Zhao's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153610516-2"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-153610516-2');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?01357a85fc2ab892b21c9ec34514f480";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dr. Zhao's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">Focus on Scientific Research</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Dr. Zhao">
      <meta itemprop="description" content="A technical blog for sharing articles related to Computer Vision, Robotics & Deep Learning.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dr. Zhao's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          Roadmap To Learn Large Language Model Of GPT Series
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-21 16:33:09" itemprop="dateCreated datePublished" datetime="2024-12-21T16:33:09+08:00">2024-12-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Notes</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes/Deep-Learning/Papers/" itemprop="url" rel="index">
                    <span itemprop="name">Papers</span>
                  </a>
                </span>
            </span>

          
            <span id="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/" class="post-meta-item leancloud_visitors" data-flag-title="Roadmap To Learn Large Language Model Of GPT Series" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>13 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <html><head></head><body><p>本文主要是OpenAI的GPT系列论文的阅读笔记。</p>
<span id="more"></span>
<h2 id="GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="GPT-1: Improving Language Understanding by Generative Pre-Training"></a>GPT-1: Improving Language Understanding by Generative Pre-Training</h2><p>GPT-1的论文在2018年6月发布，GPT-1的模型参数量达到了1.17亿，训练数据量达到了40GB，训练成本达到了12.5万美元。</p>
<h2 id="GPT-2-Language-Models-are-Unsupervised-Multitask-Learners"><a href="#GPT-2-Language-Models-are-Unsupervised-Multitask-Learners" class="headerlink" title="GPT-2: Language Models are Unsupervised Multitask Learners"></a>GPT-2: Language Models are Unsupervised Multitask Learners</h2><p>GPT-2的论文在2018年11月发布，GPT-2的模型参数量达到了1.5亿，训练数据量达到了1.5TB，训练成本达到了100万美元。</p>
<h2 id="GPT-3-Language-Models-are-Few-Shot-Learners"><a href="#GPT-3-Language-Models-are-Few-Shot-Learners" class="headerlink" title="GPT-3: Language Models are Few-Shot Learners"></a>GPT-3: Language Models are Few-Shot Learners</h2><p>GPT-3的论文在2021年12月发布，GPT-3的模型参数量达到了1750亿，训练数据量达到了45TB，训练成本达到了1200万美元。GPT-3主要解决的是语言模型中的少样本学习问题。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>In-Context Learning<br>In-Context Learning (ICL)是将预训练模型的文本输入作为一种任务具象化的形式，将自然语言指令或者示例作为条件，然后预测后面输出的文本。</li>
<li>Zero-shot VS. One-shot VS. Few-shot Learning<br>取决于模型推理时输入的示例数量。</li>
</ul>
<p><img alt="In-Context示例数量、模型大小与模型准确度的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/In-Context-Examples-Accuracy.png"><br>从上图可以的出结论：</p>
<ol>
<li>模型参数量越大，模型的准确度越高。</li>
<li>不需要梯度训练和微调，仅仅提供更多的In-Context示例，也能显著提升模型的准确度。</li>
<li>模型参数量越大，In-Context示例数量的边际收益越大。</li>
</ol>
<ul>
<li>模型参数量<br>GPT-3最大模型的参数量是175B，还有125M和13B的小模型。</li>
</ul>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>GPT-3的预训练方法包括模型、数据和训练与GPT-2的预训练方法一致，仅仅是提升了模型和数据的规模和复杂度，并延长了训练时间。有以下四个点需要重点讨论：</p>
<ul>
<li><ol>
<li>Fine-tuning<br>Fine-tuning的好处是通过监督样本微调，可以显著提升在指定基准任务上的表现。<br>Fine-tuning的缺点是针对每一个任务，都需要准备数量足够多的监督样本，会降低模型在监督数据分布之外的泛化能力。<br>GPT-3更关注任务无关的性能表现，因此没有使用Fine-tuning。</li>
</ol>
</li>
<li><ol start="2">
<li>Few-Shot Learning<br>Few-Shot Learning指的是在模型推理阶段提供几个示例作为条件，并没有用这几个示例做微调和训练。<br>一个完整的示例包含了上下文和期望输出。<br>示例的数量从10到100不等，取决于GPT-3模型的上下文长度。<br>Few-Shot Learning的优点是显著降低了对于任务相关数据的依赖，并避免了模型受窄分布数据影响而泛化性降低的问题。</li>
</ol>
</li>
<li><ol start="3">
<li>One-Shot Learning<br>One-Shot Learning与Few-Shot Learning类似，在模型推理时只提供一个示例。它更接近于人类完成语言任务的方式。</li>
</ol>
</li>
<li><ol start="4">
<li>Zero-Shot Learning<br>Zero-Shot Learning指的是在模型推理时没有提供任何示例作为条件，仅仅提供语言指令描述任务。</li>
</ol>
</li>
</ul>
<p><img alt="Few-Shot VS. One-Shot VS. Zero-Shot VS. Fine-tuning" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Few-One-Zero-Shot.png"></p>
<h4 id="Models-and-Architectures"><a href="#Models-and-Architectures" class="headerlink" title="Models and Architectures"></a>Models and Architectures</h4><p>GPT-3的模型架构与GPT-2的模型架构类似，包括参数初始化方法、预正则化方法、可逆的Token化方法，不同的是Transformer中的Attention模式。GPT-3的Attention模式与Sparse Transformer类似。<br>为了验证不同模型大小对性能的影响，GPT-3总共训练了3个参数量级8个模型，具体如下图所是：<br><img alt="GPT-3模型参数量" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/GPT-3-Model-Parameters.png"><br>GPT-3使用的Context Window为2048。<br>$$embed_size = n_{heads} \times d_{head}$$<br>前向神经网络的参数量是Transformer特征向量的4倍，$d_{ff} = 4 \times embed_size$。</p>
<h4 id="Training-Datasets"><a href="#Training-Datasets" class="headerlink" title="Training Datasets"></a>Training Datasets</h4><p>用于训练GPT-3的数据集是Common Crawl,共计包含1TB字数的文本数据。为了保障数据集的质量做了如下处理：</p>
<ul>
<li><ol>
<li>根据与高质量语料的相似度进行过滤</li>
</ol>
</li>
<li><ol start="2">
<li>在文件层面进行了模糊去重</li>
</ol>
</li>
<li><ol start="3">
<li>添加了一些高质量的参考语料，包括扩展版本的WebText数据集、两个互联网书库（Books1和Book2）以及英文版的维基百科。<br>Common Crawl数据集涵盖了2016-2019年的Web数据，过滤前的文本数据约45TB，过滤后有570GB，约有4000亿个Tokens。<br>在训练的时候，质量越高的数据被采样的概率越高，共计用了3000亿个Tokens进行训练（Common Crawl数据集的采样率小于1）。</li>
</ol>
</li>
</ul>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>大参数量的模型需要使用大的Batch Size和小的Learning Rate进行训练。<br>GPT-3是使用V100集群进行分布式训练的，使用了混合模型并行化的方式来节省内存，包括矩阵相乘并行化和层间并行化。</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>当模型结构、训练数据和训练方法高效时，模型的性能表现与算力成正相关。</p>
<h4 id="Language-Modeling-Cloze-and-Completion-Tasks"><a href="#Language-Modeling-Cloze-and-Completion-Tasks" class="headerlink" title="Language Modeling, Cloze, and Completion Tasks"></a>Language Modeling, Cloze, and Completion Tasks</h4><h5 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h5><p>GPT-3仅仅使用Zero-shot Learning进行语言建模,就在PTB数据集上拿到了SOTA。</p>
<h5 id="LAMBDA"><a href="#LAMBDA" class="headerlink" title="LAMBDA"></a>LAMBDA</h5><p>该数据集预测一段话最后一个字，用此用于测试长范围的语言模型性能。<br>该数据集的测试分析表明，随着模型参数量的增加，模型的Few-Shot性能会显著提升。<br><img alt="模型性能与参数量的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Performance-wrt-Model-Size.png"></p>
<h4 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h4><p>GPT-2由于模型能力的限制，只在英文数据集上进行了训练。GPT-3的训练数据集有93%的英文数据，有7%的其他语言数据，包括法语、德语和罗马语。</p>
<h4 id="Winograd-Style-Tasks"><a href="#Winograd-Style-Tasks" class="headerlink" title="Winograd-Style Tasks"></a>Winograd-Style Tasks</h4><p>Winograd-Style Tasks是NLP中的经典任务，主要是根据上下文的语义决定代词的含义。</p>
<h4 id="Common-Sense-Reasoning"><a href="#Common-Sense-Reasoning" class="headerlink" title="Common Sense Reasoning"></a>Common Sense Reasoning</h4><ul>
<li>PhysicalQA：物理知识推理</li>
<li>ARC：3-9年级科学考试中的多选题</li>
<li>OpenBookQA：阅读理解</li>
</ul>
<h4 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h4><h4 id="SuperGLUE"><a href="#SuperGLUE" class="headerlink" title="SuperGLUE"></a>SuperGLUE</h4><h4 id="Natural-Language-Inference"><a href="#Natural-Language-Inference" class="headerlink" title="Natural Language Inference"></a>Natural Language Inference</h4><h4 id="Synthetic-and-Qualitative-Tasks"><a href="#Synthetic-and-Qualitative-Tasks" class="headerlink" title="Synthetic and Qualitative Tasks"></a>Synthetic and Qualitative Tasks</h4><h5 id="Arithmetic"><a href="#Arithmetic" class="headerlink" title="Arithmetic"></a>Arithmetic</h5><h5 id="Word-Scrambling-and-Manipulation-Tasks"><a href="#Word-Scrambling-and-Manipulation-Tasks" class="headerlink" title="Word Scrambling and Manipulation Tasks"></a>Word Scrambling and Manipulation Tasks</h5><ul>
<li>Cycle letters in word</li>
<li>Anagrams of all but first and last character</li>
<li>Anagrams of all but first and last two characters</li>
<li>Random insertion in word</li>
<li>Reversed words</li>
</ul>
<h5 id="SAT-Analogies"><a href="#SAT-Analogies" class="headerlink" title="SAT Analogies"></a>SAT Analogies</h5><h5 id="News-Article-Generation"><a href="#News-Article-Generation" class="headerlink" title="News Article Generation"></a>News Article Generation</h5><p>通过测试人类是否能区分文章是人写的还是大模型写的来评估大模型在这方面的能力。175B的GPT-3生成的文章参与测试的80个人判断准确率仅52%，意味着GPT-3生成的文章已经基本达到人类水平。</p>
<h5 id="Correcting-English-Grammar"><a href="#Correcting-English-Grammar" class="headerlink" title="Correcting English Grammar"></a>Correcting English Grammar</h5><h4 id="Measuring-and-Preventing-Memorization-of-Benchmarks"><a href="#Measuring-and-Preventing-Memorization-of-Benchmarks" class="headerlink" title="Measuring and Preventing Memorization of Benchmarks"></a>Measuring and Preventing Memorization of Benchmarks</h4><p>由于训练模型的数据来自互联网，因此基准测试集中的测试数据很可能也包含在模型训练数据中。</p>
<h2 id="GPT-4-GPT-4-Technical-Report"><a href="#GPT-4-GPT-4-Technical-Report" class="headerlink" title="GPT-4: GPT-4 Technical Report"></a>GPT-4: GPT-4 Technical Report</h2><p>GPT-4是一个多模态大模型，可以同时处理文本和图像，并生成文本。<br>GPT-4是使用RLHF（Reinforcement Learning from Human Feedback）进行微调的。<br>GPT-4的贡献在于OpenAI开发了深度学习栈基础设置和最优化算法，让不同尺度的模型表现变得可以预测。基于这项技术，OpenAI可以以1/1000的算力训练和评估模型。<br>GPT-4训练的数据集来自2021年9月之前的数据。<br>Rule based reward models(RBRM)是GPT-4模型Zero-shot的分类器。这个模型在RLHF微调阶段提供额外的信号，来校准GPT-4的输出行为。<br>RBRM的输入包含三部分：1. Prompt；2. 模型的输出；3. 人类定义的评估规则集合。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li><a target="_blank" rel="noopener" href="https://commoncrawl.org/get-started">Common Crawl</a><ul>
<li>Over 250 billion pages spanning 17 years.</li>
<li>Free and open corpus since 2007.</li>
<li>Cited in over 10,000 research papers.</li>
<li>3–5 billion new pages added each month.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/dataset/webtext">WebText</a><ul>
<li>WebText is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. The authors scraped all outbound links from Reddit which received at least 3 karma.</li>
<li>WebText contains the text subset of these 45 million links. It consists of over 8 million documents for a total of 40 GB of text.</li>
<li>All Wikipedia documents were removed from WebText since it is a common data source for other datasets.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a><ul>
<li>The release of a beta version of Open WebText – an open source effort to reproduce OpenAI’s WebText dataset.</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Skylion007/openwebtext">Download</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://catalog.ldc.upenn.edu/LDC99T42">Penn TreeBank(PTB)</a><ul>
<li>The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC95T7) and Treebank-3 (LDC99T42) releases of PTB. Treebank-2 includes the raw text for each story. Three “map” files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://zenodo.org/records/2630551#.YFJVaWT7S_w">LAMBDA</a><ul>
<li>We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://rowanzellers.com/hellaswag/">HellaSwag</a><ul>
<li>Pick the best ending to the context.</li>
<li>We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Salesforce/wikitext">WikiText-103</a><ul>
<li>The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.</li>
<li>Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger.</li>
<li>The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.gutenberg.org/">Books1</a><ul>
<li>OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.gutenberg.org/">Books2</a><ul>
<li>OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LSDSem/story_cloze">StoryCloze</a><ul>
<li>This dataset proposes a new framework for evaluating story understanding and script learning: the ‘Story Cloze Test’. This test requires a system to choose the correct ending to a four-sentence story. </li>
<li>It contains a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. </li>
<li>This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a><ul>
<li>To help spur development in open-domain question answering, we have created the Natural Questions (NQ) corpus, along with a challenge website based on this data.</li>
<li>The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.</li>
<li>The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">WebQuestions</a><ul>
<li>This dataset consists of 6,642 question/answer pairs.</li>
<li>The questions are supposed to be answerable by Freebase, a large knowledge graph.</li>
<li>The questions are mostly centered around a single named entity.</li>
<li>The questions are popular ones asked on the web (at least in 2013).</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a><ul>
<li>TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples.</li>
<li>TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">Winograd Schema Challenge</a><ul>
<li>The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://winogrande.allenai.org/">Winograde</a><ul>
<li>WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.</li>
</ul>
</li>
<li><a href="">PhysicalQA</a><ul>
<li>PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/arc">ARC</a></li>
<li><a target="_blank" rel="noopener" href="https://stanfordnlp.github.io/coqa/">CoQA</a></li>
<li><a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">QuAC</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/drop">DROP</a></li>
<li><a href="">RACE</a></li>
<li><a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/multinews.jsonl">MultiNews</a></li>
<li><a target="_blank" rel="noopener" href="https://openbookqa.org/">OpenBookQA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tliuhuan/COPA">COPA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/allenai/record">ReCoRD</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/boolq.jsonl">BoolQ</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/multiRC.jsonl">MultiRC</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/rte.jsonl">RTE</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/anli.jsonl">ANLI</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/webarchive/resolve/main/triviaqa.jsonl">TruthfulQA</a></li>
</ul>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU（Bilingual Evaluation Understudy）是自然语言处理 (NLP) 中一种常用的评估指标，主要用于衡量机器翻译系统生成文本的质量，由IBM的研究人员Kenney和Manning在2002年提出。BLEU通过计算生成文本与参考文本之间的相似度来评估翻译的准确性和流畅性。，来评估机器翻译的质量。</p>
<p>以下是 BLEU 的核心思想及其计算方式：</p>
<h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><ol>
<li><p><strong>N-gram 匹配</strong>：<br>BLEU 通过计算生成文本与参考文本之间的 n-gram 匹配情况来衡量质量。n-gram 是文本中连续 n 个词的序列（例如，”the cat” 是一个 2-gram）。BLEU 会分别计算 1-gram、2-gram、3-gram 等的匹配情况。</p>
</li>
<li><p>**精确率 (Precision)**：<br>BLEU 计算生成文本中 n-gram 与参考文本中 n-gram 的匹配比例。例如，如果生成文本包含 10 个 2-gram，而其中 6 个在参考文本中出现，则 2-gram 的精确率为 60%。</p>
</li>
<li><p>**惩罚机制 (Brevity Penalty)**：<br>为了避免生成系统通过过于简短的输出（例如，重复参考中的高频词）获得高分，BLEU 引入了长度惩罚。如果生成文本的长度明显短于参考文本，BLEU 分数会降低。</p>
</li>
</ol>
<h4 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h4><ol>
<li><p><strong>计算 n-gram 的精确率</strong>：<br>$$<br>P_n = \frac{\text{生成文本中与参考文本匹配的 n-gram 数}}{\text{生成文本中的 n-gram 总数}}<br>$$</p>
</li>
<li><p><strong>几何平均精确率</strong>：<br>对不同 n-gram（如 1-gram 到 4-gram）的精确率取几何平均：<br>$$<br>\text{Precision}<em>{\text{geometric mean}} = \left( \prod</em>{n=1}^{N} P_n \right)^{1/N}<br>$$</p>
</li>
<li><p>**长度惩罚 (Brevity Penalty, BP)**：<br>如果生成文本长度 ( c ) 小于参考文本长度 ( r )，引入惩罚因子：<br>$$<br>BP =<br>\begin{cases}<br>1 &amp; \text{如果 } c &gt; r \<br>e^{1 - r/c} &amp; \text{如果 } c \leq r<br>\end{cases}<br>$$</p>
</li>
<li><p><strong>最终 BLEU 分数</strong>：<br>将几何平均精确率和惩罚因子相结合：<br>$$<br>BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log P_n \right)<br>$$<br>其中 ( w_n ) 是 n-gram 的权重（通常均分，如 ( w_n = 1/N )）。</p>
</li>
</ol>
<h4 id="优点与局限性"><a href="#优点与局限性" class="headerlink" title="优点与局限性"></a>优点与局限性</h4><p><strong>优点</strong>：</p>
<ul>
<li>自动化评估，计算快速。</li>
<li>与参考翻译的直接比较，适用于多种语言。</li>
</ul>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>忽略语义</strong>：BLEU 只关心 n-gram 的表面匹配，不考虑语义。</li>
<li><strong>依赖参考翻译</strong>：如果参考翻译质量不好，BLEU 分数也会受到影响。</li>
<li><strong>长文本适用性弱</strong>：对段落或整篇文章的翻译，BLEU 可能无法全面反映质量。</li>
</ol>
<h3 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h3><p>BPE（Byte Pair Encoding）是一种基于数据压缩技术的<strong>子词分割算法</strong>，广泛应用于自然语言处理（NLP）任务中，尤其是神经机器翻译和预训练语言模型。其核心思想是通过递归地合并频率最高的字符对或子词对，逐步构建适应语料分布的子词单元，从而在减少词表大小和处理未登录词（OOV）问题之间取得平衡。</p>
<h4 id="为什么需要-BPE？"><a href="#为什么需要-BPE？" class="headerlink" title="为什么需要 BPE？"></a>为什么需要 BPE？</h4><p>传统的分词方法（如按词分割或按字符分割）存在以下问题：</p>
<ol>
<li><strong>按词分割</strong>：<ul>
<li>词表会非常大，难以处理。</li>
<li>对低频词表现不好，容易导致 OOV（Out-Of-Vocabulary，未登录词）。</li>
</ul>
</li>
<li><strong>按字符分割</strong>：<ul>
<li>词表很小，但上下文关系的学习难度大。</li>
<li>生成的序列太长，影响模型效率。</li>
</ul>
</li>
</ol>
<p><strong>BPE</strong> 通过将高频的字符序列合并为子词单元，既降低了词表大小，又能捕捉常见词的整体信息，解决了上述问题。</p>
<h4 id="BPE-的基本原理"><a href="#BPE-的基本原理" class="headerlink" title="BPE 的基本原理"></a>BPE 的基本原理</h4><p>BPE 的核心思想是<strong>基于频率的合并</strong>：从初始字符开始，将出现频率最高的字符对逐步合并为新的子词单元，直到达到预定义的分词单元数量或其他停止条件。</p>
<h5 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h5><ol>
<li><p><strong>初始化</strong>：</p>
<ul>
<li>将训练语料中的每个单词分解为字符序列，并在每个单词的末尾添加特殊标记（如 <code>▁</code> 表示词的开头）。</li>
<li>例如：<code>hello world</code> → <code>h e l l o ▁ w o r l d</code></li>
</ul>
</li>
<li><p><strong>统计字符对的频率</strong>：</p>
<ul>
<li>计算语料中每对相邻字符的频率。</li>
<li>例如：<code>h e l l o</code> 中，<code>he</code>、<code>el</code>、<code>ll</code>、<code>lo</code> 是字符对，统计其出现次数。</li>
</ul>
</li>
<li><p><strong>合并频率最高的字符对</strong>：</p>
<ul>
<li>找到频率最高的字符对，将其合并为新的子词单元。</li>
<li>例如：如果 <code>ll</code> 是频率最高的对，则合并为 <code>ll</code>，更新序列为：<code>h e ll o</code>.</li>
</ul>
</li>
<li><p><strong>重复步骤 2 和 3</strong>：</p>
<ul>
<li>每次合并后重新统计频率，直到达到预定义的子词单元数量。</li>
</ul>
</li>
<li><p><strong>生成子词词表</strong>：</p>
<ul>
<li>记录所有生成的子词单元，作为最终的分词词表。</li>
</ul>
</li>
</ol>
<h5 id="算法实现："><a href="#算法实现：" class="headerlink" title="算法实现："></a>算法实现：</h5><p>BPE 的核心可以表示为以下伪代码：</p>
<figure class="highlight livecodeserver"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">输入：语料 D，目标子词表大小 V</span><br><span class="line">初始化：将 D 分解为字符序列 S</span><br><span class="line"><span class="keyword">while</span> |S| &lt; V:</span><br><span class="line">    统计 S 中所有相邻字符对的频率</span><br><span class="line">    找到频率最高的字符对 <span class="keyword">a</span>, b</span><br><span class="line">    将 <span class="keyword">a</span>, b 合并为新的子词单元 ab</span><br><span class="line">    更新 S 中的所有出现</span><br><span class="line">输出：最终的子词词表 S</span><br></pre></td></tr></tbody></table></figure>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>假设我们对以下单词应用 BPE：<code>low</code>, <code>lowest</code>, <code>new</code>, <code>newer</code>.</p>
<ol>
<li><p><strong>初始状态</strong>：</p>
<figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l o w</span><br><span class="line">l o w e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>统计字符对频率</strong>：</p>
<figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">l</span> o: <span class="number">2</span>, o w: <span class="number">2</span>, w e: <span class="number">2</span>, e s: <span class="number">1</span>, s t: <span class="number">1</span>, n e: <span class="number">2</span>, e w: <span class="number">2</span>, w e: <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>合并频率最高的对</strong>：<br>假设 <code>o w</code> 频率最高，合并为 <code>ow</code>：</p>
  <figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l ow</span><br><span class="line">l ow e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>重复以上步骤</strong>：</p>
<ul>
<li>合并后重新统计频率并合并，直到达到预定义的子词单元数量。</li>
</ul>
</li>
</ol>
<h4 id="BPE-在-NLP-中的应用"><a href="#BPE-在-NLP-中的应用" class="headerlink" title="BPE 在 NLP 中的应用"></a>BPE 在 NLP 中的应用</h4><ol>
<li><strong>机器翻译</strong>：如 Transformer 中，BPE 被用作标准分词方法，以减少 OOV 现象并提高模型的泛化能力。</li>
<li><strong>预训练模型</strong>：如 GPT 和 BERT，BPE 用于构建子词词表，兼顾词表大小与覆盖率。</li>
<li><strong>多语言任务</strong>：BPE 在多语言模型中尤其有效，能够生成语言无关的统一词表。</li>
</ol>
<h4 id="优点与局限性-1"><a href="#优点与局限性-1" class="headerlink" title="优点与局限性"></a>优点与局限性</h4><p><strong>优点</strong>：</p>
<ol>
<li><strong>词表大小灵活</strong>：通过限制词表大小 ( V )，可以在效率与表达能力之间取得平衡。</li>
<li><strong>处理未登录词（OOV）</strong>：将罕见词分解为子词单元，从而减少 OOV 的发生。</li>
<li><strong>语言无关性</strong>：BPE 是基于频率的统计方法，不依赖具体语言规则。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>固定词表</strong>：训练好的子词词表对新领域的适应性有限。</li>
<li><strong>上下文独立性</strong>：合并规则是基于全局频率的，而不考虑上下文。</li>
<li><strong>生成效率</strong>：在解码过程中，可能导致生成序列较长，影响模型推断速度。</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>BPE 是一种简单而高效的分词算法，能够生成兼具词语信息和灵活性的子词单元，在 NLP 中广泛应用，是现代语言模型的关键预处理步骤之一。</p>
<h3 id="SuperGLUE基准测试集"><a href="#SuperGLUE基准测试集" class="headerlink" title="SuperGLUE基准测试集"></a>SuperGLUE基准测试集</h3><p>SuperGLUE（<strong>A Stickier Benchmark for General-Purpose Language Understanding Systems</strong>）是自然语言处理（NLP）领域中的一个基准测试集，用于评估模型在广泛的语言理解任务上的能力。它是对 GLUE（General Language Understanding Evaluation）的改进版，专门设计来挑战基于预训练语言模型（如 BERT、GPT）在多种语言理解任务上的性能，涵盖更复杂的推理和语义理解问题。</p>
<h4 id="SuperGLUE-的核心组成"><a href="#SuperGLUE-的核心组成" class="headerlink" title="SuperGLUE 的核心组成"></a><strong>SuperGLUE 的核心组成</strong></h4><p>SuperGLUE 包含 8 个不同的任务，每个任务对应一种语言理解的核心能力。以下是每个任务的详细说明：</p>
<h5 id="1-Boolean-Question-BoolQ"><a href="#1-Boolean-Question-BoolQ" class="headerlink" title="1. Boolean Question (BoolQ)"></a>1. <strong>Boolean Question (BoolQ)</strong></h5><ul>
<li><strong>任务类型</strong>：二分类任务（Yes/No）。</li>
<li><strong>目标</strong>：根据给定的段落和问题，判断答案是“是”还是“否”。</li>
<li><strong>输入</strong>：一个段落（context）和一个自然语言问题（question）。</li>
<li><strong>输出</strong>：布尔值（Yes/No）。</li>
<li><strong>数据来源</strong>：从真实用户查询生成的问题，例如维基百科。</li>
<li><strong>挑战</strong>：<ul>
<li>问题的答案需要通过复杂推理和跨句子语义整合来确定。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="2-CommitmentBank-CB"><a href="#2-CommitmentBank-CB" class="headerlink" title="2. CommitmentBank (CB)"></a>2. <strong>CommitmentBank (CB)</strong></h5><ul>
<li><strong>任务类型</strong>：三分类任务（Entailment / Neutral / Contradiction）。</li>
<li><strong>目标</strong>：判断一个假设（hypothesis）是否从给定的文本段落（premise）中可以：<ol>
<li>推出（Entailment），</li>
<li>不确定（Neutral），</li>
<li>或矛盾（Contradiction）。</li>
</ol>
</li>
<li><strong>输入</strong>：一个前提句（premise）和一个假设句（hypothesis）。</li>
<li><strong>输出</strong>：三分类标签。</li>
<li><strong>数据来源</strong>：对语言学研究中的嵌套从句进行标注。</li>
<li><strong>挑战</strong>：<ul>
<li>涉及复杂的语义推理和句法关系。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="3-Choice-of-Plausible-Alternatives-COPA"><a href="#3-Choice-of-Plausible-Alternatives-COPA" class="headerlink" title="3. Choice of Plausible Alternatives (COPA)"></a>3. <strong>Choice of Plausible Alternatives (COPA)</strong></h5><ul>
<li><strong>任务类型</strong>：因果推理（二选一）。</li>
<li><strong>目标</strong>：判断给定的句子是否是因果链中的原因或结果，并从两个备选答案中选择一个。</li>
<li><strong>输入</strong>：一个句子和两个备选答案。</li>
<li><strong>输出</strong>：选择一个更可能的答案（1 或 2）。</li>
<li><strong>数据来源</strong>：手动创建的因果推理数据。</li>
<li><strong>挑战</strong>：<ul>
<li>涉及因果关系的推理。</li>
<li>候选答案具有一定的迷惑性。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="4-Multi-Sentence-Reading-Comprehension-ReCoRD"><a href="#4-Multi-Sentence-Reading-Comprehension-ReCoRD" class="headerlink" title="4. Multi-Sentence Reading Comprehension (ReCoRD)"></a>4. <strong>Multi-Sentence Reading Comprehension (ReCoRD)</strong></h5><ul>
<li><strong>任务类型</strong>：填空问题。</li>
<li><strong>目标</strong>：通过阅读长段落，选择填充问题空白的正确词语或短语。</li>
<li><strong>输入</strong>：一个长段落和包含空白的句子。</li>
<li><strong>输出</strong>：正确的词语或短语（从段落中选取）。</li>
<li><strong>数据来源</strong>：新闻语料库。</li>
<li><strong>挑战</strong>：<ul>
<li>涉及跨句子关系的理解。</li>
<li>候选答案可能具有语义相似性，增加了任务难度。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="5-Reading-Comprehension-with-Commonsense-Reasoning-RTE"><a href="#5-Reading-Comprehension-with-Commonsense-Reasoning-RTE" class="headerlink" title="5. Reading Comprehension with Commonsense Reasoning (RTE)"></a>5. <strong>Reading Comprehension with Commonsense Reasoning (RTE)</strong></h5><ul>
<li><strong>任务类型</strong>：二分类任务（Entailment / Not Entailment）。</li>
<li><strong>目标</strong>：判断假设（hypothesis）是否可以从前提句（premise）中推出。</li>
<li><strong>输入</strong>：前提句和假设句。</li>
<li><strong>输出</strong>：二分类标签（Entailment / Not Entailment）。</li>
<li><strong>数据来源</strong>：多个现有的自然语言推理数据集。</li>
<li><strong>挑战</strong>：<ul>
<li>推理涉及隐含的背景知识和常识。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="6-Winograd-Schema-Challenge-WiC"><a href="#6-Winograd-Schema-Challenge-WiC" class="headerlink" title="6. Winograd Schema Challenge (WiC)"></a>6. <strong>Winograd Schema Challenge (WiC)</strong></h5><ul>
<li><strong>任务类型</strong>：词义消歧。</li>
<li><strong>目标</strong>：判断某个词在两个句子中的含义是否相同。</li>
<li><strong>输入</strong>：两个句子和目标词。</li>
<li><strong>输出</strong>：布尔值（Yes/No）。</li>
<li><strong>数据来源</strong>：词义标注和上下文信息。</li>
<li><strong>挑战</strong>：<ul>
<li>涉及对多义词的准确语义判断。</li>
<li>考验模型对上下文细微差别的理解。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="7-Winograd-Schema-Challenge-WSC"><a href="#7-Winograd-Schema-Challenge-WSC" class="headerlink" title="7. Winograd Schema Challenge (WSC)"></a>7. <strong>Winograd Schema Challenge (WSC)</strong></h5><ul>
<li><strong>任务类型</strong>：共指消解。</li>
<li><strong>目标</strong>：判断代词或名词短语的指代对象。</li>
<li><strong>输入</strong>：一个句子，包含一个指代目标和两个候选对象。</li>
<li><strong>输出</strong>：正确的指代对象。</li>
<li><strong>数据来源</strong>：手工创建的共指消解数据。</li>
<li><strong>挑战</strong>：<ul>
<li>考察模型对代词解析和语义常识的理解。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="8-AX-b-BroadCoverage-Diagnostic"><a href="#8-AX-b-BroadCoverage-Diagnostic" class="headerlink" title="8. AX-b (BroadCoverage Diagnostic)"></a>8. <strong>AX-b (BroadCoverage Diagnostic)</strong></h5><ul>
<li><strong>任务类型</strong>：对文本片段进行语言现象的诊断评估。</li>
<li><strong>目标</strong>：分析模型在特定语言现象（如逻辑推理或世界知识推理）上的表现。</li>
<li><strong>输入和输出</strong>：与其他任务类似，具体设计用于测试模型的弱点。</li>
<li><strong>挑战</strong>：<ul>
<li>设计针对性强，旨在暴露模型局限。</li>
</ul>
</li>
</ul>
<h4 id="SuperGLUE-的评价方式"><a href="#SuperGLUE-的评价方式" class="headerlink" title="SuperGLUE 的评价方式"></a><strong>SuperGLUE 的评价方式</strong></h4><ol>
<li><strong>任务指标</strong>：<ul>
<li>每个任务有特定的评估指标（如准确率、F1 分数等）。</li>
</ul>
</li>
<li><strong>总体分数</strong>：<ul>
<li>通过所有任务的加权平均分数评估模型整体性能。</li>
</ul>
</li>
<li><strong>人类基准</strong>：<ul>
<li>超过 GLUE 基准的人类水平设置为 SuperGLUE 的参考基线。</li>
</ul>
</li>
</ol>
<h4 id="SuperGLUE-的意义"><a href="#SuperGLUE-的意义" class="headerlink" title="SuperGLUE 的意义"></a><strong>SuperGLUE 的意义</strong></h4><ul>
<li><strong>更高难度</strong>：SuperGLUE 设计了更复杂的语言理解任务，考验模型的推理能力、常识知识和语义理解。</li>
<li><strong>推动研究</strong>：提供了统一的评估框架，促进了更强大和通用的语言理解模型的开发。</li>
<li><strong>领域覆盖广</strong>：任务涉及多种语言理解能力，使模型需具备广泛的适应性。</li>
</ul>
<h4 id="模型挑战"><a href="#模型挑战" class="headerlink" title="模型挑战"></a><strong>模型挑战</strong></h4><p>虽然当前模型（如 T5、GPT-4 等）已经在部分任务上接近或超过人类水平，但在涉及常识推理、因果关系和细粒度语义分析的任务上，仍然存在改进空间。SuperGLUE 因此被视为推动 NLP 领域进步的重要基准之一。</p>
<h3 id="MMLU基准测试集"><a href="#MMLU基准测试集" class="headerlink" title="MMLU基准测试集"></a>MMLU基准测试集</h3><p><strong>MMLU (Massive Multitask Language Understanding)</strong> 是一个用于评估语言模型多任务能力的基准测试集。它专门设计用于测试大语言模型（如 GPT 系列）的 <strong>多任务学习（multitask learning）</strong> 和 <strong>通用语言理解能力（general language understanding）</strong>。MMLU 基准测试集涵盖了多种不同类型的任务，从常见的分类、推理任务到更加专业的知识领域（如医学、法律等）的应用，能够全面地评估语言模型在各种场景下的表现。</p>
<h4 id="MMLU-基准测试集的构成"><a href="#MMLU-基准测试集的构成" class="headerlink" title="MMLU 基准测试集的构成"></a><strong>MMLU 基准测试集的构成</strong></h4><p>MMLU 基准测试集包括多种类型的任务，以下是几个主要类别的任务和一些常见的子任务：</p>
<ol>
<li><p><strong>常识推理</strong>：</p>
<ul>
<li>测试语言模型在日常常识方面的理解。</li>
<li>示例任务：回答关于世界常识的问题（如历史事件、人物、科学常识等）。</li>
</ul>
</li>
<li><p><strong>数学与逻辑推理</strong>：</p>
<ul>
<li>包含算术、代数、几何等数学问题的解答。</li>
<li>示例任务：解决数学运算问题（例如基础的加减乘除，甚至更复杂的几何和代数问题）。</li>
</ul>
</li>
<li><p><strong>科学与技术理解</strong>：</p>
<ul>
<li>测试模型在物理、化学、生物学等科学领域的知识理解。</li>
<li>示例任务：解答关于物理学原理、化学反应、生物学概念等问题。</li>
</ul>
</li>
<li><p><strong>专业领域知识</strong>：</p>
<ul>
<li>涉及法律、医学、金融等专业领域的任务。</li>
<li>示例任务：法律案件分析、医学症状诊断、财务报告分析等。</li>
</ul>
</li>
<li><p><strong>语言学任务</strong>：</p>
<ul>
<li>涉及语法、语义分析和推理的任务。</li>
<li>示例任务：句法分析、语义关系提取、情感分析等。</li>
</ul>
</li>
<li><p><strong>阅读理解与推理</strong>：</p>
<ul>
<li>包括从文章或段落中提取信息并进行推理。</li>
<li>示例任务：阅读理解（给定一段文章后回答相关问题）和推理任务（基于文章进行逻辑推理）。</li>
</ul>
</li>
<li><p><strong>文化与历史</strong>：</p>
<ul>
<li>任务涉及对文化、历史、地理和人类社会的理解。</li>
<li>示例任务：回答历史事件的具体细节或文化现象的解读。</li>
</ul>
</li>
</ol>
<h4 id="MMLU的主要特点"><a href="#MMLU的主要特点" class="headerlink" title="MMLU的主要特点"></a><strong>MMLU的主要特点</strong></h4><ul>
<li><strong>多样性</strong>：MMLU 涵盖了从基础的语言理解任务到复杂的推理、专业知识领域的任务，涉及多个领域和知识层面。</li>
<li><strong>难度范围广</strong>：包含了从容易到困难不等的任务，帮助测试模型在不同难度级别下的能力。</li>
<li><strong>高质量问题</strong>：问题通常经过精心设计，涵盖了广泛的知识，确保测试的全面性和代表性。</li>
<li><strong>标准化的评估</strong>：通过统一的任务和标准化的评分体系，可以比较不同语言模型在同一基准上的表现。</li>
</ul>
<h4 id="MMLU基准测试集的评价标准"><a href="#MMLU基准测试集的评价标准" class="headerlink" title="MMLU基准测试集的评价标准"></a><strong>MMLU基准测试集的评价标准</strong></h4><p>MMLU 基准测试的性能通常使用 <strong>准确率（Accuracy）</strong> 来衡量。对于每个任务，模型给出的答案会根据是否正确进行评分，最终通过加权平均计算出各任务的综合准确率。</p>
<ul>
<li><strong>任务的加权</strong>：MMLU 通常会为不同类型的任务设定不同的权重，以确保最终的评分能够准确反映模型的综合能力。</li>
<li><strong>样本量</strong>：每个任务中的问题通常有多个样本，这些样本被设计为具有挑战性的，确保评估结果具有代表性。</li>
</ul>
<h4 id="MMLU的应用"><a href="#MMLU的应用" class="headerlink" title="MMLU的应用"></a><strong>MMLU的应用</strong></h4><p>MMLU 的设计使其成为测试大型语言模型多任务能力和理解深度的标准工具。它的应用主要体现在以下几个方面：</p>
<ol>
<li><strong>语言模型的综合能力评估</strong>：通过全面评估模型在多种任务中的表现，MMLU 可以帮助研究人员和工程师了解模型的通用语言理解能力。</li>
<li><strong>模型之间的比较</strong>：MMLU 提供了一个统一的测试标准，可以用来比较不同模型的性能，尤其是不同规模、架构的语言模型之间。</li>
<li><strong>多任务学习研究</strong>：MMLU 对于探索和优化多任务学习有重要意义。它帮助研究人员测试在多个任务上训练模型时的效果，评估模型如何从多个任务中进行泛化。</li>
</ol>
<h4 id="MMLU基准测试的例子"><a href="#MMLU基准测试的例子" class="headerlink" title="MMLU基准测试的例子"></a><strong>MMLU基准测试的例子</strong></h4><h5 id="例1：常识推理任务"><a href="#例1：常识推理任务" class="headerlink" title="例1：常识推理任务"></a><strong>例1：常识推理任务</strong></h5><p>任务：哪个国家位于北美洲？</p>
<ul>
<li>选项：A) 加拿大 B) 德国 C) 巴西 D) 中国</li>
<li>正确答案：A) 加拿大</li>
</ul>
<h5 id="例2：数学任务"><a href="#例2：数学任务" class="headerlink" title="例2：数学任务"></a><strong>例2：数学任务</strong></h5><p>任务：5x + 3 = 23, 求x的值。</p>
<ul>
<li>选项：A) 3 B) 4 C) 5 D) 2</li>
<li>正确答案：B) 4</li>
</ul>
<h5 id="例3：法律任务"><a href="#例3：法律任务" class="headerlink" title="例3：法律任务"></a><strong>例3：法律任务</strong></h5><p>任务：根据美国宪法，谁是最高法院的法官？</p>
<ul>
<li>选项：A) 总统 B) 最高法院首席法官 C) 美国国会 D) 各州州长</li>
<li>正确答案：B) 最高法院首席法官</li>
</ul>
<h5 id="例4：科学任务"><a href="#例4：科学任务" class="headerlink" title="例4：科学任务"></a><strong>例4：科学任务</strong></h5><p>任务：光合作用是如何发生的？</p>
<ul>
<li>选项：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖 B) 通过细胞呼吸 C) 通过蒸发 D) 通过化学反应</li>
<li>正确答案：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖</li>
</ul>
<h4 id="MMLU的挑战和改进"><a href="#MMLU的挑战和改进" class="headerlink" title="MMLU的挑战和改进"></a><strong>MMLU的挑战和改进</strong></h4><ol>
<li><strong>任务设计的复杂性</strong>：随着语言模型变得越来越强大，测试集的任务设计也需要不断提升，以确保能够衡量模型的深度理解和推理能力。</li>
<li><strong>多样化的测试</strong>：MMLU 还可以进一步扩展，涵盖更多样化的领域和任务，尤其是跨模态任务（如图像、视频等）的结合。</li>
<li><strong>不平衡性</strong>：不同任务的难度不均衡，可能导致一些模型在某些任务上表现突出，而在另一些任务上表现较差。</li>
</ol>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h4><p>MMLU（Massive Multitask Language Understanding）是一个综合性强、覆盖面广的基准测试集，旨在评估语言模型在各种任务上的表现。它通过涉及常识推理、数学、科学、法律等多种任务，全面衡量模型的多任务学习能力，成为衡量大规模语言模型通用能力的重要工具。</p>
</body></html>
    </div>

    
    
    
        <div class="reward-container">
  <div>Please buy me a cup of coffee if this article helps you.</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="Dr. Zhao WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Dr. Zhao Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Dr. Zhao
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/" title="Roadmap To Learn Large Language Model Of GPT Series">http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/GPT/" rel="tag"><i class="fa fa-tag"></i> GPT</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/02/Vision-Servo-Control-For-Robot-Manipulation/" rel="prev" title="Vision Servo Control For Robot Manipulation">
      <i class="fa fa-chevron-left"></i> Vision Servo Control For Robot Manipulation
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/" rel="next" title="Architecture Of Transformer And PyTorch Implementation">
      Architecture Of Transformer And PyTorch Implementation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training"><span class="nav-number">1.</span> <span class="nav-text">GPT-1: Improving Language Understanding by Generative Pre-Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-2-Language-Models-are-Unsupervised-Multitask-Learners"><span class="nav-number">2.</span> <span class="nav-text">GPT-2: Language Models are Unsupervised Multitask Learners</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-3-Language-Models-are-Few-Shot-Learners"><span class="nav-number">3.</span> <span class="nav-text">GPT-3: Language Models are Few-Shot Learners</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">3.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Approach"><span class="nav-number">3.2.</span> <span class="nav-text">Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Models-and-Architectures"><span class="nav-number">3.2.1.</span> <span class="nav-text">Models and Architectures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-Datasets"><span class="nav-number">3.2.2.</span> <span class="nav-text">Training Datasets</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training"><span class="nav-number">3.2.3.</span> <span class="nav-text">Training</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Results"><span class="nav-number">3.3.</span> <span class="nav-text">Results</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Language-Modeling-Cloze-and-Completion-Tasks"><span class="nav-number">3.3.1.</span> <span class="nav-text">Language Modeling, Cloze, and Completion Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Language-Modeling"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">Language Modeling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#LAMBDA"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">LAMBDA</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Translation"><span class="nav-number">3.3.2.</span> <span class="nav-text">Translation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Winograd-Style-Tasks"><span class="nav-number">3.3.3.</span> <span class="nav-text">Winograd-Style Tasks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Common-Sense-Reasoning"><span class="nav-number">3.3.4.</span> <span class="nav-text">Common Sense Reasoning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Comprehension"><span class="nav-number">3.3.5.</span> <span class="nav-text">Reading Comprehension</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SuperGLUE"><span class="nav-number">3.3.6.</span> <span class="nav-text">SuperGLUE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Natural-Language-Inference"><span class="nav-number">3.3.7.</span> <span class="nav-text">Natural Language Inference</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Synthetic-and-Qualitative-Tasks"><span class="nav-number">3.3.8.</span> <span class="nav-text">Synthetic and Qualitative Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Arithmetic"><span class="nav-number">3.3.8.1.</span> <span class="nav-text">Arithmetic</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Word-Scrambling-and-Manipulation-Tasks"><span class="nav-number">3.3.8.2.</span> <span class="nav-text">Word Scrambling and Manipulation Tasks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SAT-Analogies"><span class="nav-number">3.3.8.3.</span> <span class="nav-text">SAT Analogies</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#News-Article-Generation"><span class="nav-number">3.3.8.4.</span> <span class="nav-text">News Article Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Correcting-English-Grammar"><span class="nav-number">3.3.8.5.</span> <span class="nav-text">Correcting English Grammar</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Measuring-and-Preventing-Memorization-of-Benchmarks"><span class="nav-number">3.3.9.</span> <span class="nav-text">Measuring and Preventing Memorization of Benchmarks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-4-GPT-4-Technical-Report"><span class="nav-number">4.</span> <span class="nav-text">GPT-4: GPT-4 Technical Report</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">6.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BLEU"><span class="nav-number">6.1.</span> <span class="nav-text">BLEU</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">6.1.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%AD%A5%E9%AA%A4"><span class="nav-number">6.1.2.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">6.1.3.</span> <span class="nav-text">优点与局限性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BPE"><span class="nav-number">6.2.</span> <span class="nav-text">BPE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-BPE%EF%BC%9F"><span class="nav-number">6.2.1.</span> <span class="nav-text">为什么需要 BPE？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BPE-%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">6.2.2.</span> <span class="nav-text">BPE 的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4%EF%BC%9A"><span class="nav-number">6.2.2.1.</span> <span class="nav-text">算法步骤：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9A"><span class="nav-number">6.2.2.2.</span> <span class="nav-text">算法实现：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">6.2.2.3.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BPE-%E5%9C%A8-NLP-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">6.2.3.</span> <span class="nav-text">BPE 在 NLP 中的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90%E6%80%A7-1"><span class="nav-number">6.2.4.</span> <span class="nav-text">优点与局限性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">6.2.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SuperGLUE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">6.3.</span> <span class="nav-text">SuperGLUE基准测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SuperGLUE-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E6%88%90"><span class="nav-number">6.3.1.</span> <span class="nav-text">SuperGLUE 的核心组成</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Boolean-Question-BoolQ"><span class="nav-number">6.3.1.1.</span> <span class="nav-text">1. Boolean Question (BoolQ)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-CommitmentBank-CB"><span class="nav-number">6.3.1.2.</span> <span class="nav-text">2. CommitmentBank (CB)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-Choice-of-Plausible-Alternatives-COPA"><span class="nav-number">6.3.1.3.</span> <span class="nav-text">3. Choice of Plausible Alternatives (COPA)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-Multi-Sentence-Reading-Comprehension-ReCoRD"><span class="nav-number">6.3.1.4.</span> <span class="nav-text">4. Multi-Sentence Reading Comprehension (ReCoRD)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-Reading-Comprehension-with-Commonsense-Reasoning-RTE"><span class="nav-number">6.3.1.5.</span> <span class="nav-text">5. Reading Comprehension with Commonsense Reasoning (RTE)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-Winograd-Schema-Challenge-WiC"><span class="nav-number">6.3.1.6.</span> <span class="nav-text">6. Winograd Schema Challenge (WiC)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-Winograd-Schema-Challenge-WSC"><span class="nav-number">6.3.1.7.</span> <span class="nav-text">7. Winograd Schema Challenge (WSC)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-AX-b-BroadCoverage-Diagnostic"><span class="nav-number">6.3.1.8.</span> <span class="nav-text">8. AX-b (BroadCoverage Diagnostic)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SuperGLUE-%E7%9A%84%E8%AF%84%E4%BB%B7%E6%96%B9%E5%BC%8F"><span class="nav-number">6.3.2.</span> <span class="nav-text">SuperGLUE 的评价方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SuperGLUE-%E7%9A%84%E6%84%8F%E4%B9%89"><span class="nav-number">6.3.3.</span> <span class="nav-text">SuperGLUE 的意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%8C%91%E6%88%98"><span class="nav-number">6.3.4.</span> <span class="nav-text">模型挑战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MMLU%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">6.4.</span> <span class="nav-text">MMLU基准测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU-%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E6%9E%84%E6%88%90"><span class="nav-number">6.4.1.</span> <span class="nav-text">MMLU 基准测试集的构成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9"><span class="nav-number">6.4.2.</span> <span class="nav-text">MMLU的主要特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86"><span class="nav-number">6.4.3.</span> <span class="nav-text">MMLU基准测试集的评价标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">6.4.4.</span> <span class="nav-text">MMLU的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">6.4.5.</span> <span class="nav-text">MMLU基准测试的例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B1%EF%BC%9A%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E4%BB%BB%E5%8A%A1"><span class="nav-number">6.4.5.1.</span> <span class="nav-text">例1：常识推理任务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B2%EF%BC%9A%E6%95%B0%E5%AD%A6%E4%BB%BB%E5%8A%A1"><span class="nav-number">6.4.5.2.</span> <span class="nav-text">例2：数学任务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B3%EF%BC%9A%E6%B3%95%E5%BE%8B%E4%BB%BB%E5%8A%A1"><span class="nav-number">6.4.5.3.</span> <span class="nav-text">例3：法律任务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B4%EF%BC%9A%E7%A7%91%E5%AD%A6%E4%BB%BB%E5%8A%A1"><span class="nav-number">6.4.5.4.</span> <span class="nav-text">例4：科学任务</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MMLU%E7%9A%84%E6%8C%91%E6%88%98%E5%92%8C%E6%94%B9%E8%BF%9B"><span class="nav-number">6.4.6.</span> <span class="nav-text">MMLU的挑战和改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">6.4.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dr. Zhao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Dr. Zhao</p>
  <div class="site-description" itemprop="description">A technical blog for sharing articles related to Computer Vision, Robotics & Deep Learning.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YoungsonZhao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YoungsonZhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyongsheng@zju.edu.cn" title="E-Mail → mailto:zhaoyongsheng@zju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yongshengzhao" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yongshengzhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/yourname" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:swardman@126.com?add" title="Skype → skype:swardman@126.com?add" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2023003977号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dr. Zhao</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">294k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">4:27</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5de3d9a492e22c4b" async="async"></script>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.9.0/lozad.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>



  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout: 3000,
        priority: true,
        ignores: [uri => uri.includes('#'),uri => uri == 'http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/',]
      });
      });
  </script>


<script>
NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'n9rtSD8wdvwav5LXeOerMH5C-gzGzoHsz',
    appKey: 'xcMirBCoS6Ecbog1fFmXEwhu',
    placeholder: "You are welcome to leave a comment.",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
