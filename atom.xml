<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dr. Zhao&#39;s Blog</title>
  
  <subtitle>Focus on Scientific Research</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.zhaoyongsheng.com/"/>
  <updated>2025-01-05T12:01:01.000Z</updated>
  <id>http://blog.zhaoyongsheng.com/</id>
  
  <author>
    <name>Dr. Zhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Learning Notes Of Segment Anything (SAM)</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/</id>
    <published>2025-01-05T12:01:01.000Z</published>
    <updated>2025-01-05T12:01:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>由<strong>Meta</strong>开发的<a href="https://github.com/facebookresearch/segment-anything?tab=readme-ov-file">Segment Anything (SAM)</a>是图像分割领域的SOTA模型,其核心思想是<strong>Prompt Tuning</strong>，通过<strong>Prompt Tuning</strong>，SAM在无需训练的情况下，可以完成各种图像分割任务，包括但不限于<strong>语义分割</strong>、<strong>实例分割</strong>、<strong>全景分割</strong>、<strong>抠图</strong>、<strong>图像编辑</strong>等。本文将重点介绍<strong>SAM</strong>的模型结构、算法原理、训练方法以及pytorch实现。 <span id="more"></span></p><h2 id="模型结构">模型结构</h2><p><img alt="Segment Anything (SAM)模型结构" data-src="/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/sam.png"> SAM的模型结构如上图所示，首先由<strong>ViT</strong>提取图像的Embedding特征，然后。</p><ul><li>flowchart</li></ul><pre class="mermaid">flowchart LR    Start --&gt; Stop</pre><ul><li>sequence<pre class="mermaid">sequenceDiagram  Alice-&gt;&gt;John: Hello John, how are you?  John--&gt;&gt;Alice: Great!  Alice-)John: See you later!</pre></li><li>mindmap<pre class="mermaid">mindmaproot((mindmap))  Origins    Long history    Popularisation      British popular psychology author Tony Buzan  Research    On effectiveness<br>and features    On Automatic creation      Uses          Creative techniques          Strategic planning          Argument mapping  Tools    Pen and paper    Mermaid</pre></li><li>流程图<pre class="mermaid">graph TD;  A[输入图像] --&gt; B[ViT提取Embedding特征];  B --&gt; C[Prompt Tuning];  C --&gt; D[语义分割];  C --&gt; E[实例分割];  C --&gt; F[全景分割];  C --&gt; G[抠图];  C --&gt; H[图像编辑];  D --&gt; I[输出分割结果];  E --&gt; I;  F --&gt; I;  G --&gt; I;  H --&gt; I;  I --&gt; J[结束];</pre></li><li>甘特图<pre class="mermaid">gantt  title A Gantt Diagram  dateFormat YYYY-MM-DD  section Section      A task          :a1, 2014-01-01, 30d      Another task    :after a1, 20d  section Another      Task in Another :2014-01-12, 12d      another task    :24d</pre></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由&lt;strong&gt;Meta&lt;/strong&gt;开发的&lt;a href=&quot;https://github.com/facebookresearch/segment-anything?tab=readme-ov-file&quot;&gt;Segment Anything (SAM)&lt;/a&gt;是图像分割领域的SOTA模型,其核心思想是&lt;strong&gt;Prompt Tuning&lt;/strong&gt;，通过&lt;strong&gt;Prompt Tuning&lt;/strong&gt;，SAM在无需训练的情况下，可以完成各种图像分割任务，包括但不限于&lt;strong&gt;语义分割&lt;/strong&gt;、&lt;strong&gt;实例分割&lt;/strong&gt;、&lt;strong&gt;全景分割&lt;/strong&gt;、&lt;strong&gt;抠图&lt;/strong&gt;、&lt;strong&gt;图像编辑&lt;/strong&gt;等。本文将重点介绍&lt;strong&gt;SAM&lt;/strong&gt;的模型结构、算法原理、训练方法以及pytorch实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Vision-Transformer/"/>
    
      <category term="Segment Anything" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Vision-Transformer/Segment-Anything/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Segment Anything" scheme="http://blog.zhaoyongsheng.com/tags/Segment-Anything/"/>
    
  </entry>
  
  <entry>
    <title>Masked Auto Encoder (MAE) For Vision Transformer</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/</id>
    <published>2025-01-04T06:26:43.000Z</published>
    <updated>2025-01-04T06:26:43.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked Auto Encoder (MAE)</a>成功将<strong>Vision Transformer</strong>应用到了图像领域的自监督预训练中，并在多个视觉任务中取得了SOTA的表现，是视觉自监督预训练的重要突破。本文将介绍<strong>MAE</strong>的原理和实现细节，将重点介绍<strong>MAE</strong>相对于<a href="https://youngsonzhao.github.io/2024/07/05/Learning-Note-Of-Vision-Transformer/">Vision Transformer</a>和<a href="https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/">Transformer</a>的创新点。 <span id="more"></span></p><h2 id="整体结构">整体结构</h2><figure><img alt="" data-src="/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/mae.png"><figcaption>MAE Vision Transformer模型架构图</figcaption></figure><p>相对于<strong>Vision Transformer</strong>，<strong>MAE</strong>的模型架构图如上所示，主要创新点如下：</p><ul><li><ol type="1"><li>将图像的patches按照随机概率mask掉，在encoder的输入中仅保留未被mask的patches，随机mask的概率是75%。</li></ol></li><li><ol start="2" type="1"><li>使用一个轻量级的decoder来处理encoder编码的未被mask的patches，和被mask的patches，来重建原始图像。</li></ol></li><li><ol start="3" type="1"><li>encoder和decoder的大小是非对称的，由此可以提升非监督预训练的效率和效果。</li></ol></li><li><ol start="4" type="1"><li>预训练结束后，decoder被丢弃，仅保留encoder，用于下游任务。</li></ol></li><li><ol start="5" type="1"><li>MAE中可以使用global pooling来代替class token（可选的，通过参数配置）</li></ol></li><li><ol start="6" type="1"><li>MAE中的position embedding是2D sin-cos embedding，而不是<strong>Vision Transformer</strong>中的learnable position embedding。</li></ol></li></ul><h2 id="核心模块">核心模块</h2><h3 id="random-masking">Random Masking</h3><p><strong>MAE</strong>最核心的模块是随机掩码生成模块，可以以指定概率随机生成掩码。 具体步骤如下所示：</p><ul><li><ol type="1"><li>在batch和sequence维度上生成随机噪声，(batch_size, seq_len)</li></ol></li><li><ol start="2" type="1"><li>使用argsort函数对随机噪声在sequence维度进行升序排序，得到升序排序结果的索引矩阵ids_shuffle，该索引矩阵记录的是排名在此处的元素在原有随机噪声矩阵中的位置，(batch_size, seq_len)</li></ol></li><li><ol start="3" type="1"><li>继续使用argsort函数对索引矩阵进行升序排序，得到索引矩阵的索引矩阵ids_restore，该索引矩阵记录的是原有随机噪声矩阵的每个值得排名，(batch_size, seq_len)</li></ol></li><li><ol start="4" type="1"><li>根据mask_ratio参数，从ids_shuffle中保留前len_keep个元素，得到被保留的元素的索引矩阵ids_keep，(batch_size, len_keep)</li></ol></li><li><ol start="5" type="1"><li>使用gather函数，根据ids_keep的索引，从原始序列中生成masked的序列x_masked，(batch_size, len_keep, dim)</li></ol></li><li><ol start="6" type="1"><li>在sequence维度生成一个前len_keep个值是0，后seq_len-len_keep个值是1的mask矩阵，维度为(batch_size, seq_len)，然后使用gather函数和ids_restore索引，生成掩码矩阵mask，(batch_size, seq_len)，其中mask中值为0的元素表示被保留的元素，值为1的元素表示被mask的元素</li></ol></li><li><ol start="7" type="1"><li>返回被mask保留的序列x_masked，掩码矩阵mask，以及表明每个元素排名的ids_restore索引矩阵，其中</li></ol><ul><li>x_masked用于encoder编码器的输入</li><li>ids_restore在decoder中使用，用于将mask_token向量填充到被mask的序列Token中，融合encoder输出的序列Token特征，生成原始序列特征。</li><li>mask掩码矩阵在计算loss时使用，由于被mask掉的token对应的mask值是1,因此在计算loss时，仅计算被mask掉的token的loss. <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))</span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)</span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="encoder">Encoder</h3><p>Encoder就是使用ViT对x_masked进行编码，得到编码后的特征序列. 值的注意的是，在encoder中保留了class token。</p><h3 id="decoder">Decoder</h3><p>Decoder并不是将encoder输出的编码特征与输出序列进行cross attention，而是利用ids_restore索引矩阵将encoder输出的编码特征恢复至原有序列特征位置，被mask掉的序列特征统一用一个可学习的mask_token向量填充，然后进行decoder编码最终恢复原始图像。</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf&quot;&gt;Masked Auto Encoder (MAE)&lt;/a&gt;成功将&lt;strong&gt;Vision Transformer&lt;/strong&gt;应用到了图像领域的自监督预训练中，并在多个视觉任务中取得了SOTA的表现，是视觉自监督预训练的重要突破。本文将介绍&lt;strong&gt;MAE&lt;/strong&gt;的原理和实现细节，将重点介绍&lt;strong&gt;MAE&lt;/strong&gt;相对于&lt;a href=&quot;https://youngsonzhao.github.io/2024/07/05/Learning-Note-Of-Vision-Transformer/&quot;&gt;Vision Transformer&lt;/a&gt;和&lt;a href=&quot;https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/&quot;&gt;Transformer&lt;/a&gt;的创新点。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Vision-Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Masked Auto Encoder" scheme="http://blog.zhaoyongsheng.com/tags/Masked-Auto-Encoder/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes Of Pytorch</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/03/Learning-Notes-Of-Pytorch/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/03/Learning-Notes-Of-Pytorch/</id>
    <published>2025-01-03T14:13:14.000Z</published>
    <updated>2025-01-03T14:13:14.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Pytorch</strong>是开源的深度学习框架，由Facebook的Deep Learning Team开发，2016年发布。本文主要记录<strong>Pytorch</strong>学习过程中的一些知识点。 <span id="more"></span></p><h2 id="tensor维度操作">Tensor维度操作</h2><h3 id="flatten">flatten</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.flatten(<span class="built_in">input</span>, start_dim=<span class="number">0</span>, end_dim=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p>将输入张量展平，从<code>start_dim</code>到<code>end_dim</code>的维度展平。 参数说明：</p><ul><li>input：输入的张量。</li><li>start_dim：开始展平的维度，默认为 0。</li><li>end_dim：结束展平的维度，默认为 -1，表示最后一个维度。</li></ul><h3 id="unsqueeze">unsqueeze</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim)</span><br></pre></td></tr></tbody></table></figure><p>在指定维度上增加一个维度，维度大小为1。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim：要增加的维度。</li></ul><h3 id="squeeze">squeeze</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><p>删除张量中所有大小为1的维度。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim：要删除的维度，默认为 None，表示删除所有大小为1的维度。</li></ul><h3 id="transpose">transpose</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1)</span><br></pre></td></tr></tbody></table></figure><p>交换张量的两个维度。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim0：要交换的第一个维度。</li><li>dim1：要交换的第二个维度。</li></ul><h3 id="permute">permute</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.permute(<span class="built_in">input</span>, dims)</span><br></pre></td></tr></tbody></table></figure><p>重新排列张量的多个维度。 参数说明：</p><ul><li>input：输入的张量。</li><li>dims：新的维度顺序。</li></ul><h3 id="einsum">einsum</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.einsum(<span class="string">'nhwpqc-&gt;nchpwq'</span>, x)</span><br></pre></td></tr></tbody></table></figure><p>通过爱因斯坦求和约定计算张量的各种操作。</p><h2 id="tensor排序操作">Tensor排序操作</h2><h3 id="argmax">argmax</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>返回张量中指定维度上最大值的索引。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>keepdim：是否保持维度，默认为 False。</li></ul><h3 id="topk">topk</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.topk(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>返回张量中指定维度上k个最大值的索引和值。 参数说明：</p><ul><li>input：输入的张量。</li><li>k：返回的最大值的数量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>largest：是否返回最大值，默认为 True。</li></ul><h3 id="sort">sort</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>对张量进行排序，返回排序后的张量和索引。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>descending：是否降序排序，默认为 False。</li></ul><h3 id="argsort">argsort</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argsort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>对张量进行排序，返回排序后的索引。 参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>descending：是否降序排序，默认为 False。</li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;是开源的深度学习框架，由Facebook的Deep Learning Team开发，2016年发布。本文主要记录&lt;strong&gt;Pytorch&lt;/strong&gt;学习过程中的一些知识点。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Visualization Of Transformer&#39;s Attention Score</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/</id>
    <published>2025-01-02T06:10:38.000Z</published>
    <updated>2025-01-02T06:10:38.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/">Transformer</a>模型在自然语言处理、视觉检测、分割和识别等任务中都得到了广泛应用，其核心思想是通过自注意力机制来捕捉学习输入序列中的嵌入特征，多头注意力模块中的注意力分数（Attention Score）是自注意力机制中的重要操作，可以很好的体现出输入序列各Token的关联关系。本文将介绍如何使用<a href="https://altair-viz.github.io/user_guide/saving_charts.html">Altair</a>对于Attention Score进行可视化，以方便对模型进行分析。 <span id="more"></span></p><h3 id="工具包安装">工具包安装</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install vl-convert-python <span class="comment"># Altair保存数据依赖该包</span></span><br><span class="line">pip install altair</span><br></pre></td></tr></tbody></table></figure><h3 id="可视化">可视化</h3><p>Attention Score可视化步骤包括： 1. 载入数据集和模型参数 2. 模型进行前向推理，以获取Attention Score 3. 使用Altair对Attention Score进行可视化并将结果保存为png文件</p><h4 id="载入数据集和模型参数">载入数据集和模型参数</h4><p>步骤主要有四步： 1. 载入key-value格式的config配置文件，以获取模型参数和路径 2. 利用config配置信息载入数据集和tokenizer 3. 利用config配置信息创建模型，并从训练好的checkpoint中载入模型参数 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_config, get_weight_file_path</span><br><span class="line">config = get_config()</span><br><span class="line">display(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> get_dataset</span><br><span class="line">train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> get_model</span><br><span class="line">model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_weight_file_path</span><br><span class="line">model_filename = get_weight_file_path(config, <span class="string">'22'</span>)</span><br><span class="line"><span class="comment"># model_filename = get_weight_file_path(config, config['preload'])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Preloading model from <span class="subst">{model_filename}</span>'</span>)</span><br><span class="line">state = torch.load(model_filename, weights_only=<span class="literal">True</span>)</span><br><span class="line">model.load_state_dict(state[<span class="string">'model_state_dict'</span>])</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="模型前向推理">模型前向推理</h4><ul><li><p>载入验证数据集的一个batch数据 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_next_val_batch</span>(<span class="params">val_dataloader, tokenizer_src, tokenizer_tgt, device</span>):</span><br><span class="line">    <span class="string">"""Load the next batch from the validation set</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        val_dataloader: validation dataloader</span></span><br><span class="line"><span class="string">        tokenizer_src: source tokenizer</span></span><br><span class="line"><span class="string">        tokenizer_tgt: target tokenizer</span></span><br><span class="line"><span class="string">        device: cuda or cpu</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        batch: next batch from the validation set</span></span><br><span class="line"><span class="string">        encoder_input_tokens: source sentence tokens</span></span><br><span class="line"><span class="string">        decoder_input_tokens: target sentence tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Load a sample batch from the validation set</span></span><br><span class="line">    batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(val_dataloader))</span><br><span class="line">    encoder_input = batch[<span class="string">"encoder_input"</span>].to(device)</span><br><span class="line">    encoder_mask = batch[<span class="string">"encoder_mask"</span>].to(device)</span><br><span class="line">    decoder_input = batch[<span class="string">"decoder_input"</span>].to(device)</span><br><span class="line">    decoder_mask = batch[<span class="string">"decoder_mask"</span>].to(device)</span><br><span class="line"></span><br><span class="line">    encoder_input_tokens = [tokenizer_src.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> encoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line">    decoder_input_tokens = [tokenizer_tgt.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> decoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># check that the batch size is 1</span></span><br><span class="line">    <span class="keyword">assert</span> encoder_input.size(</span><br><span class="line">        <span class="number">0</span>) == <span class="number">1</span>, <span class="string">"Batch size must be 1 for validation"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> batch, encoder_input_tokens, decoder_input_tokens</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>模型前向推理 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device</span>):</span><br><span class="line">    <span class="string">"""Greedy decode the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model: the model to decode</span></span><br><span class="line"><span class="string">        src: the source sequence</span></span><br><span class="line"><span class="string">        src_mask: the source mask</span></span><br><span class="line"><span class="string">        tokenizer_src: source tokenizer</span></span><br><span class="line"><span class="string">        tokenizer_tgt: target tokenizer</span></span><br><span class="line"><span class="string">        max_len: the maximum length of the input sequence</span></span><br><span class="line"><span class="string">        device: cuda or cpu</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sos_idx = tokenizer_tgt.token_to_id(<span class="string">'[SOS]'</span>)</span><br><span class="line">    eos_idx = tokenizer_tgt.token_to_id(<span class="string">'[EOS]'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Precompute the encoder output and reuse it for every token we get from the decoder</span></span><br><span class="line">    encoder_output = model.encode(src, src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the decoder input with the sos token</span></span><br><span class="line">    decoder_input = torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(sos_idx).type_as(src).to(device)</span><br><span class="line">    <span class="comment"># Predict the next token until reaching the max_len or the eos token</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> decoder_input.size(<span class="number">1</span>) == max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># Build mask for the target (decoder input)</span></span><br><span class="line">        decoder_mask = causal_mask(decoder_input.size(<span class="number">1</span>)).type_as(src).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the decoder and get the output</span></span><br><span class="line">        decoder_output = model.decode(decoder_input, encoder_output, src_mask, decoder_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the next token</span></span><br><span class="line">        prob = model.project(decoder_output[:, -<span class="number">1</span>]) <span class="comment"># (1, vocab_tgt_len)</span></span><br><span class="line">        <span class="comment"># Select the next token with the maximum probability (greedy search)</span></span><br><span class="line">        _, next_token = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.cat((decoder_input, torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(next_token.item()).type_as(src).to(device)), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the next token is the eos token</span></span><br><span class="line">        <span class="keyword">if</span> next_token.item() == eos_idx:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decoder_input.squeeze(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>Attention Score可视化 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">"%.3d %s"</span> % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">                <span class="string">"%.3d %s"</span> % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        columns=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_map</span>(<span class="params">attn_type: <span class="built_in">str</span>, layer: <span class="built_in">int</span>, head: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="keyword">if</span> attn_type == <span class="string">"encoder"</span>:</span><br><span class="line">        attn = model.encoder.encoder_blocks[layer].multi_head_attention.score</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">"decoder"</span>:</span><br><span class="line">        attn = model.decoder.decoder_blocks[layer].self_attention_block.score</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">"encoder-decoder"</span>:</span><br><span class="line">        attn = model.decoder.decoder_blocks[layer].cross_attention_block.score</span><br><span class="line">    <span class="keyword">return</span> attn[<span class="number">0</span>, head].data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn_type, layer, head, row_tokens, col_tokens, max_sentence_len</span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        get_attn_map(attn_type, layer, head),</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">"col_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">"row_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            color=<span class="string">"value"</span>,</span><br><span class="line">            tooltip=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#.title(f"Layer {layer} Head {head}")</span></span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>, title=<span class="string">f"Layer <span class="subst">{layer}</span> Head <span class="subst">{head}</span>"</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_all_attention_maps</span>(<span class="params">attn_type: <span class="built_in">str</span>, layers: <span class="built_in">list</span>[<span class="built_in">int</span>], heads: <span class="built_in">list</span>[<span class="built_in">int</span>], row_tokens: <span class="built_in">list</span>, col_tokens, max_sentence_len: <span class="built_in">int</span></span>):</span><br><span class="line">    charts = []</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        rowCharts = []</span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))</span><br><span class="line">        charts.append(alt.hconcat(*rowCharts))</span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(*charts)</span><br></pre></td></tr></tbody></table></figure><p></p></li></ul><p>整个流程代码如下所是： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Using device: <span class="subst">{device}</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> load_next_val_batch</span><br><span class="line">batch, encoder_input_tokens, decoder_input_tokens = load_next_val_batch(val_dataloader, tokenizer_src, tokenizer_tgt, device)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> greedy_decode</span><br><span class="line">model_out = greedy_decode(model, batch[<span class="string">'encoder_input'</span>].to(device), batch[<span class="string">'encoder_mask'</span>].to(device), tokenizer_src, tokenizer_tgt, config[<span class="string">'src_seq_len'</span>], device)</span><br><span class="line">display(model_out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">layers = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">heads = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">sentence_len = encoder_input_tokens.index(<span class="string">"[PAD]"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder Self-Attention</span></span><br><span class="line"><span class="built_in">map</span> = get_all_attention_maps(<span class="string">"encoder"</span>, layers, heads, encoder_input_tokens, encoder_input_tokens, <span class="built_in">min</span>(<span class="number">20</span>, sentence_len))</span><br><span class="line"><span class="built_in">map</span>.save(<span class="string">'chart.png'</span>, ppi=<span class="number">200</span>)</span><br></pre></td></tr></tbody></table></figure> <img alt="Encoder Attention Score可视化图" data-src="/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/encoder_attention_score.png"><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/&quot;&gt;Transformer&lt;/a&gt;模型在自然语言处理、视觉检测、分割和识别等任务中都得到了广泛应用，其核心思想是通过自注意力机制来捕捉学习输入序列中的嵌入特征，多头注意力模块中的注意力分数（Attention Score）是自注意力机制中的重要操作，可以很好的体现出输入序列各Token的关联关系。本文将介绍如何使用&lt;a href=&quot;https://altair-viz.github.io/user_guide/saving_charts.html&quot;&gt;Altair&lt;/a&gt;对于Attention Score进行可视化，以方便对模型进行分析。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="Attention" scheme="http://blog.zhaoyongsheng.com/tags/Attention/"/>
    
      <category term="Visualization" scheme="http://blog.zhaoyongsheng.com/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>Visualization and Analysis Of Parameters And Architectures Of Models</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/</id>
    <published>2024-12-31T03:30:09.000Z</published>
    <updated>2024-12-31T03:30:09.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要介绍模型参数量的分析方法和模型结构的可视化方法。本文主要有两部分内容，第一部分是使用<a href="https://github.com/TylerYep/torchinfo">torchinfo</a>分析模型参数量，第二部分是使用<a href="https://github.com/lanpa/torch-summary">torchview</a>工具包对模型结构和模块进行可视化。 <span id="more"></span></p><h2 id="parameter-and-memory-analysis"><strong>Parameter and Memory Analysis</strong></h2><p>模型参数分析参考文章：<a href="https://medium.com/the-owl/how-to-get-model-summary-in-pytorch-57db7824d1e3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImFiODYxNGZmNjI4OTNiYWRjZTVhYTc5YTc3MDNiNTk2NjY1ZDI0NzgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMTA0MjY2MjgwMDUyOTgwMTQyMDAiLCJlbWFpbCI6InlvbmdzaGVuZy56aGFvLmNzY0BnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNzM1NTc0MDgyLCJuYW1lIjoiWW91bmdzb24gWmhhbyIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NLUWltYTZjZ054WnFOZW1majF1LXdXRW5Cd09wZWlMRERFenJ3Y01aNTNSY1JocXM0PXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IllvdW5nc29uIiwiZmFtaWx5X25hbWUiOiJaaGFvIiwiaWF0IjoxNzM1NTc0MzgyLCJleHAiOjE3MzU1Nzc5ODIsImp0aSI6ImU2OTRmMzEyZDAwZTA3Yjg3NWU1NzBmMDIxZTlkY2YyMWEwOTQ0MjIifQ.jB6C-8q7ZWwyWSwEa0a3Y4smtKiCtXT7zSkJQBqzNARU5zvDn_w90Funus3UgKcYEFh4g80KwKv3uEiDlnicvcQO93-LqOihbz2n8ljKuKpOyQpuTnaGQ4YayxmoLRwOHC4gzTpZkVadrDnc1BiBoyoXiFNKCE54wJIToD5Dj7qJFBC43djXHG6MlGnyt4TRJEu2ovbnpvef8LqryEXnpv1KVmlFdak_gsrnD4NSsQCQJd3Z_BJBls1nRt8Ph0QEi2SzZjnu7HbNBWS9cQxYTNllR4HP_umrd8GHBQWzQjPay0X0MXC9-j0jCV78Da_NtPJtWqv4T-88b16XZx1Ydw">How to get Model Summary in PyTorch</a> ### <strong>torchinfo</strong> <a href="https://github.com/TylerYep/torchinfo">torchinfo</a>是一个用于分析PyTorch模型参数的工具包，可以方便地查看模型的参数量、计算量、内存占用等信息。</p><ul><li><p>torchinfo的安装方法如下： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchinfo</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>torchinfo的使用方法如下： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line">model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># print model summary information</span></span><br><span class="line">summary(model, input_data=(torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>)).to(device)), device=device.<span class="built_in">type</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>torchinfo的模型summary信息(以Transformer)如下： </p><figure class="highlight nestedtext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">=========================================================================================================</span><br><span class="line">Layer (type:depth-idx)                                  Output Shape              Param #</span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Transformer                                             [16, 256, 22466]          --</span></span><br><span class="line"><span class="attribute">├─InputEmbedding</span><span class="punctuation">:</span> <span class="string">1-1                                   [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Embedding</span><span class="punctuation">:</span> <span class="string">2-1                                   [16, 256, 512]            8,038,912</span></span><br><span class="line"><span class="attribute">├─PositionalEncoding</span><span class="punctuation">:</span> <span class="string">1-2                               [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Dropout</span><span class="punctuation">:</span> <span class="string">2-2                                     [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">├─Encoder</span><span class="punctuation">:</span> <span class="string">1-3                                          [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─ModuleList</span><span class="punctuation">:</span> <span class="string">2-3                                  --                        --</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-1                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-2                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-3                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-4                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-5                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-6                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    └─LayerNormalization</span><span class="punctuation">:</span> <span class="string">2-4                          [16, 256, 512]            2</span></span><br><span class="line"><span class="attribute">├─InputEmbedding</span><span class="punctuation">:</span> <span class="string">1-4                                   [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Embedding</span><span class="punctuation">:</span> <span class="string">2-5                                   [16, 256, 512]            11,502,592</span></span><br><span class="line"><span class="attribute">├─PositionalEncoding</span><span class="punctuation">:</span> <span class="string">1-5                               [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Dropout</span><span class="punctuation">:</span> <span class="string">2-6                                     [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">├─Decoder</span><span class="punctuation">:</span> <span class="string">1-6                                          [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─ModuleList</span><span class="punctuation">:</span> <span class="string">2-7                                  --                        --</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-7                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-8                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-9                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-10                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-11                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-12                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    └─LayerNormalization</span><span class="punctuation">:</span> <span class="string">2-8                          [16, 256, 512]            2</span></span><br><span class="line"><span class="attribute">├─ProjectionLayer</span><span class="punctuation">:</span> <span class="string">1-7                                  [16, 256, 22466]          --</span></span><br><span class="line"><span class="attribute">│    └─Linear</span><span class="punctuation">:</span> <span class="string">2-9                                      [16, 256, 22466]          11,525,058</span></span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Total params</span><span class="punctuation">:</span> <span class="string">75,174,402</span></span><br><span class="line"><span class="attribute">Trainable params</span><span class="punctuation">:</span> <span class="string">75,174,402</span></span><br><span class="line"><span class="attribute">Non-trainable params</span><span class="punctuation">:</span> <span class="string">0</span></span><br><span class="line"><span class="attribute">Total mult-adds (G)</span><span class="punctuation">:</span> <span class="string">1.20</span></span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Input size (MB)</span><span class="punctuation">:</span> <span class="string">8.49</span></span><br><span class="line"><span class="attribute">Forward/backward pass size (MB)</span><span class="punctuation">:</span> <span class="string">3521.18</span></span><br><span class="line"><span class="attribute">Params size (MB)</span><span class="punctuation">:</span> <span class="string">300.70</span></span><br><span class="line"><span class="attribute">Estimated Total Size (MB)</span><span class="punctuation">:</span> <span class="string">3830.37</span></span><br><span class="line">=========================================================================================================</span><br></pre></td></tr></tbody></table></figure><p></p></li></ul><h3 id="parameter-analysis-step-by-step"><strong>Parameter Analysis Step by Step</strong></h3><p><strong>Transformer</strong>模型的参数如下所示： </p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">src_vocab_size</span>: <span class="number">15701</span></span><br><span class="line"><span class="attribute">tgt_vocab_size</span>: <span class="number">22466</span></span><br><span class="line"><span class="attribute">src_seq_len</span>: <span class="number">256</span></span><br><span class="line"><span class="attribute">tgt_seq_len</span>: <span class="number">256</span></span><br><span class="line"><span class="attribute">embed_size</span>: <span class="number">512</span></span><br><span class="line"><span class="attribute">hidden_size</span>: <span class="number">2048</span></span><br><span class="line"><span class="attribute">num_heads</span>: <span class="number">8</span></span><br><span class="line"><span class="attribute">num_encoder_layers</span>: <span class="number">6</span></span><br><span class="line"><span class="attribute">num_decoder_layers</span>: <span class="number">6</span></span><br><span class="line"><span class="attribute">dropout</span>: <span class="number">0</span>.<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure> <strong>Transformer</strong>模型参数量大小跟<strong>src_vocab_size</strong>、<strong>tgt_vocab_size</strong>、<strong>embed_size</strong>、<strong>hidden_size</strong>、<strong>num_encoder_layers</strong>、<strong>num_decoder_layers</strong>有关。<p></p><h4 id="inputembedding"><strong>InputEmbedding</strong></h4><p><strong>InputEmbedding</strong>本质上是一个词表嵌入向量矩阵，用Lookup Table实现，其大小取决于词表中的词数和每个词的嵌入向量维度。</p><ul><li><p>对于源序列，其<strong>InputEmbedding</strong>的参数量大小为： <span class="math display">\[ P_{ie}^{src} = src\_vocab\_size \times embed\_size = 15701 \times 512 = 8038912\]</span> 其中<span class="math inline">\(P_{ie}^{src}\)</span>为源序列的<strong>InputEmbedding</strong>参数量大小。</p></li><li><p>对于目标序列，其<strong>InputEmbedding</strong>的参数量大小为： <span class="math display">\[ P_{ie}^{tgt} = tgt\_vocab\_size \times embed\_size = 22466 \times 512 = 11502592\]</span> 其中<span class="math inline">\(P_{ie}^{tgt}\)</span>为目标序列的<strong>InputEmbedding</strong>参数量大小。</p></li></ul><h4 id="positionalencoding"><strong>PositionalEncoding</strong></h4><p><strong>PositionalEncoding</strong>是由不同频率的正弦和余弦函数生成的绝对位置编码，是一个唯一确定的位置编码表，不存在可训练的参数，因此其参数量为0。</p><h4 id="encoder"><strong>Encoder</strong></h4><p><strong>Encoder</strong>由6个<strong>EncoderBlock</strong>和1个<strong>LayerNormalization</strong>组成，因此其参数量大小为： <span class="math display">\[P_{enc} = 6 \times P_{encb} + P_{ln} = 6 \times 3150340 + 2 = 18902042\]</span> 其中<span class="math inline">\(P_{enc}\)</span>为<strong>Encoder</strong>的参数量大小，<span class="math inline">\(P_{encb}\)</span>为<strong>EncoderBlock</strong>的参数量大小，<span class="math inline">\(P_{ln}\)</span>为<strong>LayerNormalization</strong>的参数量大小。</p><h5 id="encoderblock"><strong>EncoderBlock</strong></h5><p><strong>EncoderBlock</strong>由一个<strong>MultiHeadAttentionBlock</strong>、一个<strong>FeedForwardBlock</strong>和两个<strong>ResidualConnection</strong>组成，因此其参数量大小为： <span class="math display">\[P_{encb} = P_{mhab} + P_{ffb} + 2 \times P_{rc} = 1050624 + 2099712 + 2 \times 2 = 3150340\]</span> 其中<span class="math inline">\(P_{encb}\)</span>为<strong>EncoderBlock</strong>的参数量大小，<span class="math inline">\(P_{mhab}\)</span>为<strong>MultiHeadAttentionBlock</strong>的参数量大小，<span class="math inline">\(P_{ffb}\)</span>为<strong>FeedForwardBlock</strong>的参数量大小，<span class="math inline">\(P_{rc}\)</span>为<strong>ResidualConnection</strong>的参数量大小。</p><h6 id="residualconnection"><strong>ResidualConnection</strong></h6><p><strong>ResidualConnection</strong>模块包含一个<strong>LayerNormalization</strong>和一个<strong>Add</strong>操作，因此其参数量s大小为： <span class="math display">\[P_{rc} = P_{ln} = 2\]</span> 其中<span class="math inline">\(P_{rc}\)</span>为<strong>ResidualConnection</strong>的参数量大小，<span class="math inline">\(P_{ln}\)</span>为<strong>LayerNormalization</strong>的参数量大小。</p><h6 id="multiheadattentionblock"><strong>MultiHeadAttentionBlock</strong></h6><p><strong>MultiHeadAttentionBlock</strong>由四个<strong>Linear</strong>模块组成，因此其参数量大小为： <span class="math display">\[P_{mhab} = 4 \times P_{linear} = 4 \times (embed\_size \times embed\_size + embed\_size) = 4 \times (512 \times 512 + 512 \times 512 + 512) = 1050624\]</span> 其中<span class="math inline">\(P_{linear}\)</span>为<strong>MultiHeadAttentionBlock</strong>模块中的线性映射层，共有4个，<span class="math inline">\(embed\_size\)</span>为<strong>Linear</strong>模块的输入和输出维度。</p><h6 id="feedforwardblock"><strong>FeedForwardBlock</strong></h6><p><strong>FeedForwardBlock</strong>由两个<strong>Linear</strong>、一个<strong>ReLU</strong>和一个<strong>Dropout</strong>组成，其中<strong>ReLU</strong>和<strong>Dropout</strong>模块不包含可训练的参数，因此其参数量大小为： <span class="math display">\[P_{ffb} = embed\_size \times hidden\_size + hidden\_size + hidden\_size \times embed\_size + embed\_size = 512 \times 2048 + 2048 + 2048 \times 512 + 512 = 2099712\]</span> 其中<span class="math inline">\(P_{ffb}\)</span>为<strong>FeedForwardBlock</strong>的参数量大小，<span class="math inline">\(embed\_size\)</span>为<strong>Linear</strong>模块的输入和输出维度，<span class="math inline">\(hidden\_size\)</span>为<strong>FeedForwardBlock</strong>的隐藏层维度。</p><h4 id="decoder"><strong>Decoder</strong></h4><p><strong>Decoder</strong>由6个<strong>DecoderBlock</strong>和1个<strong>LayerNormalization</strong>组成，因此其参数量大小为： <span class="math display">\[P_{dec} = 6 \times P_{decb} + P_{ln} = 6 \times 4200966 + 2 = 25205798\]</span> 其中<span class="math inline">\(P_{dec}\)</span>为<strong>Decoder</strong>的参数量大小，<span class="math inline">\(P_{decb}\)</span>为<strong>DecoderBlock</strong>的参数量大小，<span class="math inline">\(P_{ln}\)</span>为<strong>LayerNormalization</strong>的参数量大小。</p><h5 id="decoderblock"><strong>DecoderBlock</strong></h5><p><strong>DecoderBlock</strong>由两个<strong>MultiHeadAttentionBlock</strong>、一个<strong>FeedForwardBlock</strong>和三个<strong>ResidualConnection</strong>组成，因此其参数量大小为： <span class="math display">\[P_{decb} = P_{smhab} + P_{cmhab} + P_{ffb} + 3 \times P_{rc} = 1050624 + 1050624 + 2099712 + 3 \times 2 = 4200966\]</span> 其中<span class="math inline">\(P_{decb}\)</span>为<strong>DecoderBlock</strong>的参数量大小，<span class="math inline">\(P_{smhab}\)</span>为<strong>SelfMultiHeadAttentionBlock</strong>的参数量大小，<span class="math inline">\(P_{cmhab}\)</span>为<strong>CrossMultiHeadAttentionBlock</strong>的参数量大小，<span class="math inline">\(P_{ffb}\)</span>为<strong>FeedForwardBlock</strong>的参数量大小，<span class="math inline">\(P_{rc}\)</span>为<strong>ResidualConnection</strong>的参数量大小。</p><h4 id="projectionlayer"><strong>ProjectionLayer</strong></h4><p><strong>ProjectionLayer</strong>由一个<strong>Linear</strong>和一个<strong>Dropout</strong>组成，因此其参数量大小为： <span class="math display">\[P_{pl} = embedding\_size \times tgt\_vocab\_size + tgt\_vocab\_size = 512 \times 22466 + 22466 = 11525058\]</span></p><h4 id="total-parameters"><strong>Total Parameters</strong></h4><p><strong>Transformer</strong>模型的总参数量为： <span class="math display">\[P_{total} = P_{ie}^{src} + P_{ie}^{tgt} + P_{enc} + P_{dec} + P_{pl} = 8038912 + 11502592 + 18902042 + 25205798 + 11525058 = 75174402\]</span></p><h3 id="memory-analysis"><strong>Memory Analysis</strong></h3><p><strong>Transformer</strong>模型的参数都是Float32格式的，一个参数占用4字节，因此参数量占用内存大小为： <span class="math display">\[M_{total} = P_{total} * 4 / 1000 / 1000 = 75174402 * 4 / 1000 / 1000 = 300.70MB\]</span></p><h4 id="中间激活值">中间激活值</h4><p>中间激活值是为了计算梯度而需要保留的中间结果，具体计算方法和步骤可以参照<a href="https://mingchao.wang/4KTgtnFc/">这里</a>。</p><h2 id="architecture-visualization"><strong>Architecture Visualization</strong></h2><p><a href="https://github.com/mert-kurttutan/torchview">torchview</a>是pytorch模型结构常用的可视化工具，可以分层查看模型结构和模型的输入输出。 ### <strong>安装</strong> <strong>torchview</strong>是基于<strong>graphviz</strong>实现的，因此需要先安装<strong>graphviz</strong>，然后安装<strong>torchview</strong>。 </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install graphviz</span><br><span class="line">pip install graphviz</span><br><span class="line">pip install torchview</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="使用"><strong>使用</strong></h3><p><strong>torchview</strong>的使用步骤很简单，首先定义模型(步骤见上面章节)，然后定义模型的输入数据，最后调用<strong>torchview</strong>函数即可。 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchview <span class="keyword">import</span> draw_graph</span><br><span class="line">model_graph = draw_graph(model, input_data=(torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>)).to(device)), depth=<span class="number">2</span>, device=device.<span class="built_in">type</span>, expand_nested=<span class="literal">True</span>, save_graph=<span class="literal">True</span>, filename=<span class="string">'transformer'</span>)</span><br><span class="line">model_graph.visual_graph</span><br></pre></td></tr></tbody></table></figure> <img alt="Transformer模型结构（depth=2, expand_nested=True）" data-src="/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/transformer.png"><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍模型参数量的分析方法和模型结构的可视化方法。本文主要有两部分内容，第一部分是使用&lt;a href=&quot;https://github.com/TylerYep/torchinfo&quot;&gt;torchinfo&lt;/a&gt;分析模型参数量，第二部分是使用&lt;a href=&quot;https://github.com/lanpa/torch-summary&quot;&gt;torchview&lt;/a&gt;工具包对模型结构和模块进行可视化。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Model" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Model/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Model/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Model" scheme="http://blog.zhaoyongsheng.com/tags/Model/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Architecture Of Transformer And PyTorch Implementation</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/</id>
    <published>2024-12-23T15:38:41.000Z</published>
    <updated>2024-12-23T15:38:41.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文详细介绍<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf">Attention Is All You Need</a>论文中Transformer模型架构、各个模块的数学原理、输入输出、参数规模，以及PyTorch实现。 <span id="more"></span></p><h2 id="模型整体架构">模型整体架构</h2><p><img alt="Transformer模型结构图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/transformer-architecture.png"> 原始的Transformer模型是<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>的经典结构，<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>分别由<span class="math inline">\(N_e\)</span>个<strong>Encoder Block</strong>和<span class="math inline">\(N_d\)</span>个<strong>Decoder Block</strong>残差堆叠组成。编码器的输入是源序列，尺寸是(batch_size, src_seq_len)，输出是Encoder Embedding，尺寸是(batch_size, src_seq_len, embed_size)，解码器的输入是目标序列，尺寸是(batch_size, tgt_seq_len)和Encoder Embedding，输出是Decoder Embedding，尺寸是(batch_size, seq_len, embed_size)。</p><ul><li>参考连接<ul><li><a href="https://xiaosheng.blog/2022/06/28/use-pytorch-to-implement-transformer">使用 Pytorch 一步一步实现 Transformer Encoder</a></li><li><a href="https://mingchao.wang/crmAf7MS/">Transformer</a></li><li><a href="https://github.com/hkproj/pytorch-transformer/tree/main">pytorch-transformer</a></li><li><a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference</a></li><li><a href="https://www.zhihu.com/question/347678607">如何理解Transformer论文中的positional encoding，和三角函数有什么关系？</a></li></ul></li></ul><h2 id="各模块数学原理与代码实现">各模块数学原理与代码实现</h2><h3 id="input-embedding"><strong>Input Embedding</strong></h3><p><strong>Input Embedding</strong>（输入嵌入）是将离散Token序列转换为连续向量序列的过程。Transformer模型的输入是文本字符串，首先利用<a href="">Tokenizer</a>将文本转换为Token序列，然后利用<a href="">nn.Embedding</a>将每一个Token保存为一个embed_size维的Embedding向量。<strong>nn.Embedding</strong>本质是一个Lookup Table，Key是Token，Value是Embedding向量，关键参数有两个，vocab_size表示词表中的Token数量, embed_size表示Embedding向量的维度。</p><p>Input Embedding是(vocab_size, embed_size)的参数矩阵，在模型训练时会同步更新该参数矩阵。如果想要冻结Input Embedding的参数矩阵，可以设置embedding.weight.requires_grad = False。Input Embedding的实现代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InputEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Embedding layer for input tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size: <span class="built_in">int</span>, embed_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the embedding layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vocab_size: size of the vocabulary</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(InputEmbedding, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.embedding.weight.data.uniform_(-<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the embedding layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            embedded tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.embedding(x) * math.sqrt(self.embed_size)</span><br></pre></td></tr></tbody></table></figure><h3 id="positional-encoding"><strong>Positional Encoding</strong></h3><p><strong>Positional Encoding</strong>（位置编码）是Transformer模型中用于给每个Token添加位置信息的机制。由于Transformer模型中没有循环神经网络（RNN）中的序列位置顺序信息，因此需要通过位置编码来提供序列的顺序信息，在论文中使用了正弦和余弦函数来编码位置信息。 在 Transformer 模型中，<strong>Positional Encoding (位置编码)</strong> 用于为输入序列引入位置信息，以弥补自注意力机制（Self-Attention）缺少序列顺序感知能力的不足。它的设计基于正弦和余弦函数的数学原理，使得不同位置的编码具有可解析的相对位置信息。</p><h4 id="数学公式"><strong>数学公式</strong></h4><p>给定一个位置 <span class="math inline">\(pos\)</span> 和一个维度 <span class="math inline">\(i\)</span>（嵌入向量的第 <span class="math inline">\(i\)</span> 个维度），位置编码的值由以下公式定义： <span class="math display">\[PE(pos, k) = \begin{cases} \sin\left(\frac{1}{10000^{\frac{2i}{embed\_size}}} * pos\right)&amp; \text{if } k = 2i \\\cos\left(\frac{1}{10000^{\frac{2i}{embed\_size}}} * pos\right)&amp; \text{if } k = 2i+1\end{cases}\]</span> 其中 * <span class="math inline">\(pos\)</span> 是Token在序列中的位置（从 0 开始，最大值是seq_len - 1）。 * <span class="math inline">\(k\)</span> 是Embedding的维度索引（从 0 开始，最大值是embed_size - 1）。 * <span class="math inline">\(embed\_size\)</span> 是Embedding的维度（embed_size）。 <img alt="正弦位置编码示意图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/sine_frequencies_plot.png"></p><h4 id="位置编码可视化"><strong>位置编码可视化</strong></h4><p>不同序列长度和嵌入特征维度下的位置编码可视化如下： <img alt="位置编码可视化（embed_size=512, seq_len=256）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap.png"> <img alt="位置编码可视化（embed_size=256, seq_len=128）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap_1.png"> <img alt="位置编码可视化（embed_size=128, seq_len=64）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap_2.png"></p><ol type="1"><li><strong>频率范围：</strong><ul><li>分母中的 <span class="math inline">\(10000^{\frac{2i}{embed\_size}}\)</span> 利用指数函数设计了一个频率缩放机制，使得高维度对应的频率更低，低维度对应的频率更高。</li><li>这种设计让每个维度的编码值具有不同的频率特性，确保编码中包含丰富的位置信息。</li><li>对于每个维度 <span class="math inline">\(i\)</span>，通过不同的频率编码（由 <span class="math inline">\(10000^{\frac{2i}{embed\_size}}\)</span> 控制）生成一个独特的信号，使得高维信息捕获细粒度的相对位置信息。</li><li>嵌入特征的维度（embed_size）越高，位置编码的粒度越细，信息丰富度越大。</li></ul></li><li><strong>正弦与余弦函数：</strong><ul><li>使用正弦和余弦为奇数维度和偶数维度生成不同的变化模式。</li><li>正弦和余弦函数可以保证位置编码的值在[-1, 1]范围内。</li></ul></li></ol><h4 id="核心性质"><strong>核心性质</strong></h4><ol type="1"><li><strong>绝对与相对位置：</strong><ul><li>位置编码是对所有位置<span class="math inline">\((pos, k)\)</span>的绝对编码，且编码唯一（由嵌入特征维度决定）。</li><li>位置编码的设计确保了任意两个位置之间的相对位置信息可以被简单计算： <span class="math display">\[PE(pos_1) - PE(pos_2)\]</span> 这一性质帮助模型直接捕获相对位置。</li></ul></li><li><strong>序列长度无关：</strong><ul><li>由于正弦函数和余弦函数是周期性函数，其频率系数由嵌入特征的维度（embed_size）和嵌入特征的索引（<span class="math inline">\(2i\)</span>）决定，因此位置编码的值与序列长度无关，位置编码可以为任意长度的序列编码，且相同序列索引（<span class="math inline">\(pos\)</span>）的位置编码相同。</li><li></li></ul></li><li><strong>无需训练：</strong><ul><li>位置编码的值是通过公式计算的，无需学习参数。这减少了模型参数量，同时保持了良好的泛化能力。</li></ul></li></ol><h3 id="改进与替代方法"><strong>改进与替代方法</strong></h3><p>尽管正弦-余弦位置编码有诸多优点，但在后续研究中，也提出了许多替代方法：</p><ol type="1"><li><p><strong>可学习的位置编码：</strong><br>通过将位置编码视为可学习的参数矩阵，使模型在特定任务上更具适应性。</p></li><li><p><strong>相对位置编码（Relative Positional Encoding）：</strong><br>提取位置之间的相对关系，如 Transformer-XL 和 T5 中的改进版本。</p></li><li><p><strong>旋转位置编码（RoPE, Rotary Positional Embedding）：</strong><br>在 Transformers 中通过旋转操作高效捕获相对位置信息。</p></li></ol><h4 id="代码实现"><strong>代码实现</strong></h4><p>为了提升计算效率，将位置编码中的频率系数做如下优化： <span class="math display">\[\begin{align*}\frac{1}{10000^{\frac{2i}{embed\_size}}}  &amp;= \exp(\log(\frac{1}{10000^{\frac{2i}{embed\_size}}})) \\  &amp;= \exp(\log({10000^{\frac{2i}{embed\_size}}}^{-1})) \\  &amp;= \exp(-1 * \log({10000^{\frac{2i}{embed\_size}}})) \\  &amp;= \exp(-1 * {\frac{2i}{embed\_size}} * \log(10000)) \\  &amp;= \exp(-{\frac{\log(10000)}{embed\_size}} * {2i})\end{align*}\]</span> </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Positional encoding layer for input tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the positional encoding layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            max_len: maximum length of the input sequence</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create positional encoding, a matrix of shape (seq_len, embed_size)</span></span><br><span class="line">        pe = torch.zeros(seq_len, embed_size)</span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, seq_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># (seq_len, 1), unsqueeze: add a new dimension</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, embed_size, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / embed_size)) <span class="comment"># (embed_size/2,)</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * div_term)</span><br><span class="line">        <span class="comment"># self.pe = self.pe.unsqueeze(0).transpose(0, 1) # (seq_len, 1, embed_size)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># (1, seq_len, embed_size), 1 for batch size</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># register buffer, so it doesn't need to be trained but can be saved</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the positional encoding layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            positional encoded tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># x = x + self.pe[:, :x.size(1), :].requires_grad(False)</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>), :].detach()</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="layer-normalization"><strong>Layer Normalization</strong></h3><p><strong>Layer Normalization</strong>（层归一化）是一种常用于深度学习模型的正则化技术，特别是在序列建模和自然语言处理任务中。它旨在通过标准化每一层的激活值来提高模型的训练稳定性和收敛速度。</p><h4 id="基本原理"><strong>基本原理</strong></h4><p>在神经网络中，每一层的激活值可能因输入数据的变化而幅度不一，这种变化被称为<strong>内部分布漂移（Internal Covariate Shift）</strong>。Layer Normalization 通过对每一层的神经元输出进行标准化，减少这种漂移的影响。</p><p>Layer Normalization 的标准化是针对<strong>层的维度</strong>（即神经元的集合）进行的，而不是像 Batch Normalization 针对<strong>样本的维度</strong>。具体而言，对于每个输入样本，Layer Normalization 会对其当前层的激活值进行归一化处理。</p><h4 id="数学表达"><strong>数学表达</strong></h4><p>假设某层的输入是一个向量 <span class="math inline">\(\mathbf{h} = [h_1, h_2, ..., h_d]\)</span>，其中 <span class="math inline">\(d\)</span> 是该层的神经元数。Layer Normalization 的计算过程如下：</p><ol type="1"><li><p><strong>计算均值和方差</strong>： <span class="math display">\[\mu = \frac{1}{d} \sum_{i=1}^d h_i, \quad \sigma^2 = \frac{1}{d} \sum_{i=1}^d (h_i - \mu)^2\]</span></p></li><li><p><strong>标准化</strong>： <span class="math display">\[\hat{h}_i = \frac{h_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\]</span> 其中，<span class="math inline">\(\epsilon\)</span> 是一个小常数，用于防止分母为零。</p></li><li><p><strong>添加可学习参数</strong>（仿射变换）： <span class="math display">\[h_i^{\text{norm}} = \gamma \hat{h}_i + \beta\]</span> 其中，<span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 是可学习参数，用于恢复网络的表达能力。</p></li></ol><h4 id="与-batch-normalization-的对比"><strong>与 Batch Normalization 的对比</strong></h4><table><thead><tr class="header"><th>特性</th><th>Layer Normalization</th><th>Batch Normalization</th></tr></thead><tbody><tr class="odd"><td><strong>归一化维度</strong></td><td>每层神经元（样本内部）</td><td>样本维度（批次内的样本间）</td></tr><tr class="even"><td><strong>依赖批大小</strong></td><td>不依赖</td><td>依赖批大小</td></tr><tr class="odd"><td><strong>适用场景</strong></td><td>RNN、Transformer 等序列模型</td><td>CNN 等需要大批量训练的模型</td></tr><tr class="even"><td><strong>计算开销</strong></td><td>较小</td><td>可能较大</td></tr></tbody></table><h4 id="优点"><strong>优点</strong></h4><ol type="1"><li><strong>适用于小批量或单样本训练</strong>：因为不依赖批次统计信息，Layer Normalization 能很好地适用于小批量甚至单样本训练。</li><li><strong>改善收敛性</strong>：通过减少激活值的变化，模型更容易收敛。</li><li><strong>在序列任务中的优势</strong>：特别适合 Recurrent Neural Network（RNN）和 Transformer，因为它不会破坏时间步的依赖关系。</li></ol><h4 id="应用场景"><strong>应用场景</strong></h4><ul><li>Transformer 中广泛使用（例如 BERT 和 GPT 模型）。</li><li>在序列建模（如语言模型、时间序列预测）和小批量训练任务中效果显著。</li></ul><p>通过 Layer Normalization，深度学习模型可以更快地学习、避免梯度爆炸或消失，并增强泛化能力。</p><h4 id="代码实现-1"><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps: <span class="built_in">float</span> = <span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the layer normalization layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            eps: epsilon value for numerical stability</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).__init__()</span><br><span class="line">        self.eps = eps <span class="comment"># epsilon value for numerical stability</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>)) <span class="comment"># scale parameter, learnable parameter</span></span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>)) <span class="comment"># shift parameter, learnable parameter</span></span><br><span class="line">        <span class="comment"># self.beta = nn.Parameter(torch.ones(embed_size))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the layer normalization layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            normalized tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br></pre></td></tr></tbody></table></figure><h3 id="feed-forward-block"><strong>Feed Forward Block</strong></h3><p><strong>Feed Forward Block</strong> 是 Transformer 架构中的一个重要模块，主要用于将序列特征进行非线性变换，以提升模型的表达能力。它由两个全连接层组成，第一个层用于处理输入序列特征，第二个层用于处理第一个层的输出。</p><p>主要参数有两个： * embed_size: 输入序列特征的维度 * hidden_size: 全连接层的隐藏层维度</p><h4 id="数学原理"><strong>数学原理</strong></h4><ol type="1"><li><p><strong>第一层全连接层</strong>：将输入序列特征 <span class="math inline">\(X\)</span> 通过线性变换映射到一个更高维度的空间，得到 <span class="math inline">\(Y\)</span>。 <span class="math display">\[Y = W_1X + b_1\]</span> 其中<span class="math inline">\(X \in \mathbb{R}^{seq\_len \times embed\_size}\)</span>为输入序列特征，<span class="math inline">\(Y \in \mathbb{R}^{seq\_len \times hidden\_size}\)</span>为第一层全连接层的输出序列特征。<span class="math inline">\(W_1\)</span>和<span class="math inline">\(b_1\)</span>是可学习的参数，<span class="math inline">\(W_1 \in \mathbb{R}^{hidden\_size \times embed\_size}\)</span>, <span class="math inline">\(b_1 \in \mathbb{R}^{hidden\_size}\)</span>。</p></li><li><p><strong>激活函数</strong>：在第一层全连接层之后，通常会添加一个非线性激活函数，如 ReLU 或 GELU，以引入非线性变换。</p></li><li><p><strong>Dropout</strong>：为了防止过拟合，通常会在第二层全连接层之前添加 Dropout。</p></li><li><p><strong>第二层全连接层</strong>：将第一层全连接层的输出<span class="math inline">\(Y\)</span>通过线性变换映射回原始空间，得到 <span class="math inline">\(Z\)</span>。 <span class="math display">\[Z = W_2Y + b_2\]</span> 其中<span class="math inline">\(Y \in \mathbb{R}^{seq\_len \times hidden\_size}\)</span>为第一个全连接层的输出特征，<span class="math inline">\(Z \in \mathbb{R}^{seq\_len \times embed\_size}\)</span>为第二层全连接层的输出序列特征。<span class="math inline">\(W_2\)</span>和<span class="math inline">\(b_2\)</span>是可学习的参数，<span class="math inline">\(W_2 \in \mathbb{R}^{embed\_size \times hidden\_size}\)</span>, <span class="math inline">\(b_2 \in \mathbb{R}^{embed\_size}\)</span>。</p></li></ol><h4 id="代码实现-2"><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, hidden_size: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the feedforward layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            hidden_size: size of the hidden layer</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForwardBlock, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(embed_size, hidden_size) <span class="comment"># (embed_size, hidden_size), linear layer</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, embed_size) <span class="comment"># (hidden_size, embed_size), linear layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the feedforward layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.linear1(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.relu(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.dropout(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.linear2(x) <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure><h3 id="multi-head-attention-block"><strong>Multi Head Attention Block</strong></h3><p><strong>Multi Head Attention Block</strong> 是 Transformer 架构中的核心模块，主要用于处理序列特征之间的交互关系，提取序列编码特征。它由三个主要部分组成：<strong>Query, Key, Value</strong>，以及多头注意力机制。</p><p>主要参数有三个： * embed_size: 输入序列特征的维度 * num_heads: 多头注意力机制的heads数量 * dropout: Dropout层的概率，用于防止过拟合，在<strong>Query</strong>和<strong>Key</strong>相乘之后使用</p><h4 id="数学原理-1"><strong>数学原理</strong></h4><p><img alt="多头注意力机制网络架构图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/multi_head_self_attention.png"> 如上图所是，多头注意力机制由<strong>Scaled Dot-Product Attention</strong>和<strong>Multi-Head Attention</strong>两部分组成。 <strong>Scaled Dot-Product Attention</strong> 是多头注意力机制基础部分，其公式如下： <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{head\_size}})V\]</span> 其中<span class="math inline">\(Q, K, V \in \mathbb{R}^{batch\_size \times num\_heads \times seq\_len \times head\_size}\)</span>分别表示Query, Key, Value序列特征，<span class="math inline">\(head\_size\)</span>表示序列特征头维度，<span class="math inline">\(\sqrt{head\_size}\)</span>是为了防止<span class="math inline">\(QK^T\)</span>过大导致的梯度消失问题。</p><p>在计算<span class="math inline">\(\frac{QK^T}{\sqrt{head\_size}}\)</span>之后，需要进行三步操作： 1. 需要使用<strong>mask</strong>对得到的注意力权重进行掩码操作，防止模型关注到无效的位置。 2. 使用<strong>softmax</strong>对注意力权重进行归一化，得到最终的注意力权重矩阵。 3. 将注意力权重矩阵进行<strong>dropout</strong>操作，防止过拟合。</p><p><strong>Multi-Head Attention</strong> 是多头注意力机制的最终实现，是在<strong>Scaled Dot-Product Attention</strong>的基础上进行了线性映射，以提高多头注意力模块的表达能力和拟合能力。 对输入序列特征<span class="math inline">\(Q, K, V\)</span>进行线性映射，得到新的序列特征，公式如下： <span class="math display">\[\begin{align*}Q^{'} &amp;= W_{q}Q + b_{q} \\K^{'} &amp;= W_{k}K + b_{k} \\V^{'} &amp;= W_{v}V + b_{v} \end{align*}\]</span></p><p>对输出序列特征<span class="math inline">\(O\)</span>进行线性映射，得到最终的输出序列特征，公式如下： <span class="math display">\[O^{'} = W_{o}O + b_{o} \]</span></p><h4 id="代码实现-3"><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Multi-head attention block.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, num_heads: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the multi-head attention layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            num_heads: number of attention heads</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttentionBlock, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> embed_size % num_heads == <span class="number">0</span>, <span class="string">"Embedding size must be divisible by number of heads"</span></span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_size = embed_size // num_heads</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define the linear layers</span></span><br><span class="line">        self.query = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for query seq</span></span><br><span class="line">        self.key = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for key seq</span></span><br><span class="line">        self.value = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for value seq</span></span><br><span class="line">        </span><br><span class="line">        self.out = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for output</span></span><br><span class="line">    <span class="comment"># @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout: nn.Dropout = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Compute the attention scores and apply the softmax function. key and value are the same shape and key_seq_len = value_seq_len.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: input tensor of shape (batch_size, head_nums, query_seq_len, head_size)</span></span><br><span class="line"><span class="string">            key: input tensor of shape (batch_size, head_nums, key_seq_len, head_size)</span></span><br><span class="line"><span class="string">            value: input tensor of shape (batch_size, head_nums, value_seq_len, head_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, key_seq_len) or (batch_size, 1, query_seq_len, key_seq_len)</span></span><br><span class="line"><span class="string">            dropout: optional dropout layer</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            embedding: attention embedding tensor of shape (batch_size, num_heads, seq_len, head_size)</span></span><br><span class="line"><span class="string">            score: attention scores tensor of shape (batch_size, num_heads, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># compute the attention scores, QK^T/(sqrt(head_size)</span></span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, head_size) x (batch_size, num_heads, key_seq_len, head_size) --&gt; (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line">        score = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(query.size(-<span class="number">1</span>)) <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply mask if provided</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># print(type(mask))</span></span><br><span class="line">            <span class="comment"># print(mask.shape)</span></span><br><span class="line">            score = score.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># apply softmax function</span></span><br><span class="line">        score = torch.softmax(score, dim=-<span class="number">1</span>) <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply dropout</span></span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            score = dropout(score)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute the weighted sum of values as embedding</span></span><br><span class="line">        <span class="comment"># key_seq_len = value_seq_len</span></span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len) x (batch_size, num_heads, value_seq_len, head_size) --&gt; (batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        embedding = torch.matmul(score, value) <span class="comment">#(batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        <span class="keyword">return</span> embedding, score</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Forward pass of the multi-head attention layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            key: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            value: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, seq_len) or (batch_size, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Linear transformations for query, key, and value</span></span><br><span class="line">        query = self.query(query) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line">        key = self.key(key) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line">        value = self.value(value) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the embedding into multiple heads</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, num_heads, head_size) --&gt; (batch_size, num_heads, seq_len, head_size)</span></span><br><span class="line">        <span class="comment"># view: reshape tensor without changing its data</span></span><br><span class="line">        <span class="comment"># transpose: swap the positions of two axes in an array</span></span><br><span class="line">        batch_size = query.shape[<span class="number">0</span>]</span><br><span class="line">        query = query.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        key = key.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, key_seq_len, head_size)</span></span><br><span class="line">        value = value.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, value_seq_len, head_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the attention embedding and score</span></span><br><span class="line">        embedding, self.score = self.attention(query, key, value, mask, self.dropout)</span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, head_size) --&gt; (batch_size, query_seq_len, num_heads, head_size) --&gt; (batch_size, query_seq_len, embed_size)</span></span><br><span class="line">        embedding = embedding.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.embed_size) </span><br><span class="line">        <span class="keyword">return</span> self.out(embedding) <span class="comment"># (batch_size, query_seq_len, embed_size) --&gt; (batch_size, query_seq_len, embed_size)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="residual-connection"><strong>Residual Connection</strong></h3><p>从Transformer模型结构图中可以看出，每一个<strong>Encoder Block</strong>和<strong>Decoder Block</strong>都包含<strong>Residual Connection</strong>模块，它将输入与经过子层（Sublayer）的输出相加，然后进行Layer Normalization。这样做的好处是，可以防止梯度消失和梯度爆炸，同时也可以让模型更容易训练。</p><h4 id="数学原理-2">数学原理</h4><p><strong>Residual Connection</strong>的数学原理很简单，就是将输入与经过Dropout操作之后的子层（Sublayer）输出相加，然后进行Layer Normalization。具体公式如下： <span class="math display">\[\text{Residual Connection}(x, \text{Sublayer}(x)) = x + \text{Dropout}(\text{Sublayer}(\text{Layer Normalization}(x)))\]</span></p><ul><li>Layer Normalization：在进行子层操作之前首先进行Layer Normalization操作，用于对输入进行归一化，使得输入具有均值为0和标准差为1的分布，从而提高模型的训练效果。</li><li>Sublayer：表示子层，可以是任何神经网络层，如Multi-Head Attention、Feed Forward Network等</li><li>Dropout：子层操作之后需要进行Dropout操作，用于防止过拟合</li></ul><h4 id="代码实现-4">代码实现</h4><figure class="highlight julia"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class ResidualConnection(nn.<span class="built_in">Module</span>):</span><br><span class="line">    def __init__(self, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""Initialize the residual connection layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResidualConnection, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, sublayer: nn.<span class="built_in">Module</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies a sublayer followed by a residual connection and layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            sublayer: nn.Module representing the sublayer to be applied</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># return self.norm(x + self.dropout(sublayer(x)))</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></tbody></table></figure><h3 id="encoder-block"><strong>Encoder Block</strong></h3><p><strong>Encoder Block</strong>是Transformer模型的核心组成部分，它由三个子模块组成：<strong>Multi-Head Attention</strong>，<strong>Feed Forward Block</strong>和<strong>Residual Connection</strong>。</p><h4 id="数学原理-3">数学原理</h4><p><strong>Encoder Block</strong>的数学原理如下： <span class="math display">\[\text{Encoder Block}(x) = \begin{cases} \text{Residual Connection}(x, \text{Multi-Head Attention}(x, x, x))&amp; step1 \\\text{Residual Connection}(x, \text{Feed Forward Block}(x))&amp; step2\end{cases}\]</span></p><ul><li>Multi-Head Attention：用于对输入进行自注意力机制，提取输入中的关键信息。</li><li>Feed Forward Network：用于对输入进行非线性变换，增加模型的表达能力。</li></ul><h4 id="代码实现-5">代码实现</h4><figure class="highlight ceylon"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> EncoderBlock(nn.Module):</span><br><span class="line">    def <span class="number">__</span>init<span class="number">__</span>(self, embed<span class="number">_</span>size: int, hidden<span class="number">_</span>size: int, num<span class="number">_</span>heads: int, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""Initialize the encoder block layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            hidden_size: hidden size</span></span><br><span class="line"><span class="string">            num_heads: number of heads</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">super</span>(EncoderBlock, self).<span class="number">__</span>init<span class="number">__</span>()</span><br><span class="line">        self.multi<span class="number">_</span>head<span class="number">_</span>attention = MultiHeadAttentionBlock(embed<span class="number">_</span>size, num<span class="number">_</span>heads, dropout)</span><br><span class="line">        self.feed<span class="number">_f</span>orward = FeedForwardBlock(embed<span class="number">_</span>size, hidden<span class="number">_</span>size, dropout)</span><br><span class="line">        self.residual<span class="number">_</span>connections = nn.ModuleList([ResidualConnection(dropout) <span class="keyword">for</span> <span class="number">_</span> <span class="keyword">in</span> range(<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, src<span class="number">_m</span>ask: torch.Tensor = None) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies encode block layer followed by a feed-forward layer and a residual connection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            src_mask: optional mask tensor of shape (batch_size, 1, 1, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.residual<span class="number">_</span>connections[<span class="number">0</span>](x, lambda x: self.multi<span class="number">_</span>head<span class="number">_</span>attention(x, x, x, src<span class="number">_m</span>ask))</span><br><span class="line">        x = self.residual<span class="number">_</span>connections[<span class="number">1</span>](x, self.feed<span class="number">_f</span>orward)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure><h3 id="encoder"><strong>Encoder</strong></h3><p><strong>Encoder</strong>是Transformer模型中的编码器部分，它由多个<strong>Encoder Block</strong>串联组成。</p><h4 id="数学原理-4">数学原理</h4><p><strong>Encoder</strong>的数学原理如下： <span class="math display">\[\text{Encoder}(x) = \text{Layer Normalization}(\text{Encoder Block}(\text{Encoder Block}(...(\text{Encoder Block}(x))...)))\]</span></p><ul><li>Encoder Block：编码器串联多个Encoder Block提取序列特征，原始Transformer模型中使用了6个Encoder Block。</li><li>Layer Normalization：对Encoder Block输出进行归一化，以解决特征迁移问题。</li></ul><h4 id="代码实现-6">代码实现</h4><figure class="highlight ceylon"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Encoder(nn.Module):</span><br><span class="line">    def <span class="number">__</span>init<span class="number">__</span>(self, embed<span class="number">_</span>size: int, hidden<span class="number">_</span>size: int, num<span class="number">_</span>heads: int, num<span class="number">_</span>layers: int, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the encoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            hidden_size: hidden size</span></span><br><span class="line"><span class="string">            num_heads: number of heads</span></span><br><span class="line"><span class="string">            num_layers: number of encoder blocks</span></span><br><span class="line"><span class="string">            dropout: dropout rate, defaults to 0.1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">super</span>(Encoder, self).<span class="number">__</span>init<span class="number">__</span>()</span><br><span class="line">        self.encoder<span class="number">_</span>blocks = nn.ModuleList([EncoderBlock(embed<span class="number">_</span>size, hidden<span class="number">_</span>size, num<span class="number">_</span>heads, dropout) <span class="keyword">for</span> <span class="number">_</span> <span class="keyword">in</span> range(num<span class="number">_</span>layers)])</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies encode layer followed by a stack of encoder blocks and a layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> encoder<span class="number">_</span>block <span class="keyword">in</span> self.encoder<span class="number">_</span>blocks:</span><br><span class="line">            x = encoder<span class="number">_</span>block(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></tbody></table></figure><h3 id="decoder-block"><strong>Decoder Block</strong></h3><p><strong>Decoder Block</strong>是Transformer模型的核心组成部分，它由三个子模块组成：<strong>Multi-Head Attention</strong>，<strong>Feed Forward Block</strong>和<strong>Residual Connection</strong>，其中<strong>Multi-Head Attention</strong>模块又根据输入的序列特征不同分成两个实例模块，一个是<strong>self-attention</strong>模块，一个是<strong>cross-attention</strong>模块。</p><p><strong>self-attention</strong>模块用于处理<strong>Decoder Block</strong>的输入序列（实际上为模型的输出序列），而<strong>cross-attention</strong>模块用于处理<strong>Encoder</strong>输出的序列特征和<strong>self-attention</strong>模块处理后的序列特征。</p><h4 id="数学原理-5">数学原理</h4><p><strong>Decoder Block</strong>的数学原理如下： <span class="math display">\[\text{Decoder Block}(x) = \begin{cases}\text{Residual Connection}(x, \text{Multi-Head Attention}(x, x, x, tgt\_mask)) &amp; step1 \\\text{Residual Connection}(x, \text{Multi-Head Attention}(x, enc\_output, enc\_output, src\_mask)) &amp; step2 \\\text{Residual Connection}(x, \text{Feed Forward Block}(x) &amp; step3\end{cases}\]</span></p><ul><li>Multi-Head Attention：多头注意力机制，用于提取序列特征。</li><li>Feed Forward Network：用于对输入进行非线性变换，增加模型的表达能力。</li><li>Residual Connection：残差连接，用于实现梯度 vanishing 和 exploding 的问题。</li></ul><h4 id="代码实现-7">代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">DecoderBlock</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">embed_size</span>: <span class="title">int</span>, <span class="title">hidden_size</span>: <span class="title">int</span>, <span class="title">num_heads</span>: <span class="title">int</span>, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        super(<span class="type">DecoderBlock</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        self.self_attention_block = <span class="type">MultiHeadAttentionBlock</span>(<span class="title">embed_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.cross_attention_block = <span class="type">MultiHeadAttentionBlock</span>(<span class="title">embed_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.feed_forward = <span class="type">FeedForwardBlock</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.residual_connections = nn.<span class="type">ModuleList</span>([<span class="type">ResidualConnection(dropout)</span> <span class="title">for</span> <span class="title">_</span> <span class="title">in</span> <span class="title">range</span>(3)])</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">enc_output</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Applies</span> the decoder block.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            x: input tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            enc_output: encoder output tensor of shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            src_mask: source mask tensor of shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask: target mask tensor of shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            output tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        x = self.residual_connections[0](<span class="title">x</span>, <span class="title">lambda</span> <span class="title">x</span>: <span class="title">self</span>.<span class="title">self_attention_block</span>(<span class="title">x</span>, <span class="title">x</span>, <span class="title">x</span>, <span class="title">tgt_mask</span>))</span></span><br><span class="line"><span class="class">        x = self.residual_connections[1](<span class="title">x</span>, <span class="title">lambda</span> <span class="title">x</span>: <span class="title">self</span>.<span class="title">cross_attention_block</span>(<span class="title">x</span>, <span class="title">enc_output</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>))</span></span><br><span class="line"><span class="class">        x = self.residual_connections[2](<span class="title">x</span>, <span class="title">self</span>.<span class="title">feed_forward</span>)</span></span><br><span class="line"><span class="class">        return x</span></span><br></pre></td></tr></tbody></table></figure><h3 id="decoder"><strong>Decoder</strong></h3><p><strong>Decoder</strong>是Transformer模型的解码器部分，它由多个<strong>Decoder Block</strong>串联组成。</p><h4 id="数学原理-6">数学原理</h4><p><strong>Decoder</strong>的数学原理如下： <span class="math display">\[\text{Decoder}(x) = \text{Layer Normalization}(\text{Decoder Block}(\text{Decoder Block}(...(\text{Decoder Block}(x))...)))\]</span></p><ul><li>Decoder Block：解码器串联多个Decoder Block提取序列特征，原始Transformer模型中使用了6个Decoder Block。</li><li>Layer Normalization：对Decoder Block输出进行归一化，以解决特征迁移问题。</li></ul><h4 id="代码实现-8">代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Decoder</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">embed_size</span>: <span class="title">int</span>, <span class="title">hidden_size</span>: <span class="title">int</span>, <span class="title">num_heads</span>: <span class="title">int</span>, <span class="title">num_layers</span>: <span class="title">int</span>, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        super(<span class="type">Decoder</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        self.decoder_blocks = nn.<span class="type">ModuleList</span>([<span class="type">DecoderBlock</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>) for _ in range(<span class="title">num_layers</span>)])</span></span><br><span class="line"><span class="class">        self.norm = <span class="type">LayerNormalization</span>()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">enc_output</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Applies</span> the decoder by a stack of decoder blocks and a layer normalization.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            x: input tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            enc_output: encoder output tensor of shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>) as the input of decoder</span></span><br><span class="line"><span class="class">            src_mask: source mask tensor of shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask: target mask tensor of shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            output tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        for decoder_block in self.decoder_blocks:</span></span><br><span class="line"><span class="class">            x = decoder_block(<span class="title">x</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>, <span class="title">tgt_mask</span>)</span></span><br><span class="line"><span class="class">        return self.norm(<span class="title">x</span>)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="projection-layer"><strong>Projection Layer</strong></h3><p><strong>Projection Layer</strong>是Transformer模型的输出层，作用是解码器的输出序列特征映射到词汇表大小，以生成最终的预测结果。 参数有两个： * vocab_size：词汇表大小。 * embed_size：嵌入向量的维度。</p><h4 id="数学原理-7">数学原理</h4><p><strong>Projection Layer</strong>的数学原理如下： <span class="math display">\[\text{Projection Layer}(x) = \text{log_softmax}(\text{Linear}(x))\]</span></p><h4 id="代码实现-9">代码实现</h4><figure class="highlight julia"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class ProjectionLayer(nn.<span class="built_in">Module</span>):</span><br><span class="line">    def __init__(self, embed_size: int, vocab_size: int):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the projection layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            vocab_size: vocabulary size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ProjectionLayer, self).__init__()</span><br><span class="line">        <span class="comment"># assert embed_size == vocab_size, "Embedding size and vocabulary size must be equal"</span></span><br><span class="line">        self.projection = nn.Linear(embed_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies the projection layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, vocab_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_size) -&gt; (batch_size, seq_len, vocab_size)</span></span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(self.projection(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="transformer"><strong>Transformer</strong></h3><p><strong>Transformer</strong>的模型可以由上述模块组成，它由<strong>Input Embedding</strong>、<strong>Positional Encoding</strong>、<strong>Encoder</strong>、<strong>Decoder</strong>和<strong>Projection Layer</strong>组成。</p><h4 id="代码实现-10">代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Transformer</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">src_vocab_size</span>: <span class="title">int</span>, <span class="title">tgt_vocab_size</span>: <span class="title">int</span>, <span class="title">src_seq_len</span>: <span class="title">int</span>, <span class="title">tgt_seq_len</span>: <span class="title">int</span>, <span class="title">embed_size</span>: <span class="title">int</span> = 512, <span class="title">hidden_size</span>: <span class="title">int</span> = 2048, <span class="title">num_heads</span>: <span class="title">int</span> = 8, <span class="title">num_encoder_layers</span>: <span class="title">int</span> = 6, <span class="title">num_decoder_layers</span>: <span class="title">int</span>=6, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Initializes</span> the transformer model. <span class="type">The</span> model consists of an encoder and a decoder.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            src_vocab_size: source vocabulary size</span></span><br><span class="line"><span class="class">            tgt_vocab_size: target vocabulary size</span></span><br><span class="line"><span class="class">            src_seq_len: source sequence length</span></span><br><span class="line"><span class="class">            tgt_seq_len: target sequence length</span></span><br><span class="line"><span class="class">            embed_size: embedding size</span></span><br><span class="line"><span class="class">            hidden_size: hidden size</span></span><br><span class="line"><span class="class">            num_heads: number of attention heads</span></span><br><span class="line"><span class="class">            num_encoder_layers: number of encoder layers</span></span><br><span class="line"><span class="class">            num_decoder_layers: number of decoder layers</span></span><br><span class="line"><span class="class">            dropout: dropout rate</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        super(<span class="type">Transformer</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> input embedding layer</span></span><br><span class="line"><span class="class">        self.src_embedding = <span class="type">InputEmbedding</span>(<span class="title">src_vocab_size</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        self.tgt_embedding = <span class="type">InputEmbedding</span>(<span class="title">tgt_vocab_size</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> positional encoding layers</span></span><br><span class="line"><span class="class">        self.src_pos_encoding = <span class="type">PositionalEncoding</span>(<span class="title">embed_size</span>, <span class="title">src_seq_len</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.tgt_pos_encoding = <span class="type">PositionalEncoding</span>(<span class="title">embed_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> encoder and decoder</span></span><br><span class="line"><span class="class">        self.encoder = <span class="type">Encoder</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">num_encoder_layers</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.decoder = <span class="type">Decoder</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">num_decoder_layers</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.projection = <span class="type">ProjectionLayer</span>(<span class="title">embed_size</span>, <span class="title">tgt_vocab_size</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Initialize</span> the parameters</span></span><br><span class="line"><span class="class">        for p in self.parameters():</span></span><br><span class="line"><span class="class">            if p.dim() &gt; 1:</span></span><br><span class="line"><span class="class">                nn.init.xavier_uniform_(<span class="title">p</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">src</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """<span class="type">Performs</span> a forward pass through the transformer model.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            src (<span class="title">torch</span>.<span class="type">Tensor</span>): source input, shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt (<span class="title">torch</span>.<span class="type">Tensor</span>): target input, shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">            src_mask (<span class="title">torch</span>.<span class="type">Tensor</span>): source mask, shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask (<span class="title">torch</span>.<span class="type">Tensor</span>): target mask, shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            torch.<span class="type">Tensor</span>: output of the transformer model, shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">tgt_vocab_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        # <span class="type">Encoder</span></span></span><br><span class="line"><span class="class">        # (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>) -&gt; (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        src = self.src_embedding(<span class="title">src</span>)</span></span><br><span class="line"><span class="class">        # (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>) -&gt; (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        src = self.src_pos_encoding(<span class="title">src</span>)</span></span><br><span class="line"><span class="class">        enc_output = self.encoder(<span class="title">src</span>, <span class="title">src_mask</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Decoder</span></span></span><br><span class="line"><span class="class">        tgt = self.tgt_embedding(<span class="title">tgt</span>)</span></span><br><span class="line"><span class="class">        tgt = self.tgt_pos_encoding(<span class="title">tgt</span>)</span></span><br><span class="line"><span class="class">        output = self.decoder(<span class="title">tgt</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>, <span class="title">tgt_mask</span>)</span></span><br><span class="line"><span class="class">        # projection</span></span><br><span class="line"><span class="class">        return self.projection(<span class="title">output</span>)</span></span><br></pre></td></tr></tbody></table></figure><h2 id="训练">训练</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Epoch</span> <span class="number">0</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">0</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=5.7891]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">1</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">1</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=4.3471]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">2</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">2</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=4.2629]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">3</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">3</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.50it/s, loss=5.1802]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">4</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">4</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:01&lt;00:00,  5.50it/s, loss=4.5534]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">5</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">5</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:01&lt;00:00,  5.50it/s, loss=3.7544]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">6</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">6</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=2.6769]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">7</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">7</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:59&lt;00:00,  5.51it/s, loss=3.1740]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">8</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">8</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:59&lt;00:00,  5.52it/s, loss=1.5529]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">9</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">9</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=1.5789]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">10</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">10</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=2.2088]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">11</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">11</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=1.8507]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">12</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">12</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:58&lt;00:00,  5.53it/s, loss=1.6555]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">13</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">13</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=1.7904]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">14</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">14</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=2.1260]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">15</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">15</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.6503]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">16</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">16</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.1801]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">17</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">17</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.6803]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">18</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">18</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.0962]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">19</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">19</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=1.8696]</span></span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文详细介绍&lt;a href=&quot;https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf&quot;&gt;Attention Is All You Need&lt;/a&gt;论文中Transformer模型架构、各个模块的数学原理、输入输出、参数规模，以及PyTorch实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Roadmap To Learn Large Language Model Of GPT Series</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/</id>
    <published>2024-12-21T08:33:09.000Z</published>
    <updated>2024-12-21T08:33:09.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要是OpenAI的GPT系列论文的阅读笔记。</p><span id="more"></span><h2 id="gpt-1-improving-language-understanding-by-generative-pre-training">GPT-1: Improving Language Understanding by Generative Pre-Training</h2><p>GPT-1的论文在2018年6月发布，GPT-1的模型参数量达到了1.17亿，训练数据量达到了40GB，训练成本达到了12.5万美元。 ## GPT-2: Language Models are Unsupervised Multitask Learners GPT-2的论文在2018年11月发布，GPT-2的模型参数量达到了1.5亿，训练数据量达到了1.5TB，训练成本达到了100万美元。 ## GPT-3: Language Models are Few-Shot Learners GPT-3的论文在2021年12月发布，GPT-3的模型参数量达到了1750亿，训练数据量达到了45TB，训练成本达到了1200万美元。GPT-3主要解决的是语言模型中的少样本学习问题。 ### Introduction * In-Context Learning In-Context Learning (ICL)是将预训练模型的文本输入作为一种任务具象化的形式，将自然语言指令或者示例作为条件，然后预测后面输出的文本。 * Zero-shot VS. One-shot VS. Few-shot Learning 取决于模型推理时输入的示例数量。</p><p><img alt="In-Context示例数量、模型大小与模型准确度的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/In-Context-Examples-Accuracy.png"> 从上图可以的出结论： 1. 模型参数量越大，模型的准确度越高。 2. 不需要梯度训练和微调，仅仅提供更多的In-Context示例，也能显著提升模型的准确度。 3. 模型参数量越大，In-Context示例数量的边际收益越大。 * 模型参数量 GPT-3最大模型的参数量是175B，还有125M和13B的小模型。 ### Approach GPT-3的预训练方法包括模型、数据和训练与GPT-2的预训练方法一致，仅仅是提升了模型和数据的规模和复杂度，并延长了训练时间。有以下四个点需要重点讨论： * 1. Fine-tuning Fine-tuning的好处是通过监督样本微调，可以显著提升在指定基准任务上的表现。 Fine-tuning的缺点是针对每一个任务，都需要准备数量足够多的监督样本，会降低模型在监督数据分布之外的泛化能力。 GPT-3更关注任务无关的性能表现，因此没有使用Fine-tuning。 * 2. Few-Shot Learning Few-Shot Learning指的是在模型推理阶段提供几个示例作为条件，并没有用这几个示例做微调和训练。 一个完整的示例包含了上下文和期望输出。 示例的数量从10到100不等，取决于GPT-3模型的上下文长度。 Few-Shot Learning的优点是显著降低了对于任务相关数据的依赖，并避免了模型受窄分布数据影响而泛化性降低的问题。 * 3. One-Shot Learning One-Shot Learning与Few-Shot Learning类似，在模型推理时只提供一个示例。它更接近于人类完成语言任务的方式。 * 4. Zero-Shot Learning Zero-Shot Learning指的是在模型推理时没有提供任何示例作为条件，仅仅提供语言指令描述任务。</p><figure><img alt="" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Few-One-Zero-Shot.png"><figcaption>Few-Shot VS. One-Shot VS. Zero-Shot VS. Fine-tuning</figcaption></figure><h4 id="models-and-architectures">Models and Architectures</h4><p>GPT-3的模型架构与GPT-2的模型架构类似，包括参数初始化方法、预正则化方法、可逆的Token化方法，不同的是Transformer中的Attention模式。GPT-3的Attention模式与Sparse Transformer类似。 为了验证不同模型大小对性能的影响，GPT-3总共训练了3个参数量级8个模型，具体如下图所是： <img alt="GPT-3模型参数量" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/GPT-3-Model-Parameters.png"> GPT-3使用的Context Window为2048。 <span class="math display">\[embed\_size = n_{heads} \times d_{head}\]</span> 前向神经网络的参数量是Transformer特征向量的4倍，<span class="math inline">\(d_{ff} = 4 \times embed\_size\)</span>。 #### Training Datasets 用于训练GPT-3的数据集是Common Crawl,共计包含1TB字数的文本数据。为了保障数据集的质量做了如下处理： * 1. 根据与高质量语料的相似度进行过滤 * 2. 在文件层面进行了模糊去重 * 3. 添加了一些高质量的参考语料，包括扩展版本的WebText数据集、两个互联网书库（Books1和Book2）以及英文版的维基百科。 Common Crawl数据集涵盖了2016-2019年的Web数据，过滤前的文本数据约45TB，过滤后有570GB，约有4000亿个Tokens。 在训练的时候，质量越高的数据被采样的概率越高，共计用了3000亿个Tokens进行训练（Common Crawl数据集的采样率小于1）。 #### Training 大参数量的模型需要使用大的Batch Size和小的Learning Rate进行训练。 GPT-3是使用V100集群进行分布式训练的，使用了混合模型并行化的方式来节省内存，包括矩阵相乘并行化和层间并行化。</p><h3 id="results">Results</h3><p>当模型结构、训练数据和训练方法高效时，模型的性能表现与算力成正相关。 #### Language Modeling, Cloze, and Completion Tasks ##### Language Modeling GPT-3仅仅使用Zero-shot Learning进行语言建模,就在PTB数据集上拿到了SOTA。 ##### LAMBDA 该数据集预测一段话最后一个字，用此用于测试长范围的语言模型性能。 该数据集的测试分析表明，随着模型参数量的增加，模型的Few-Shot性能会显著提升。 <img alt="模型性能与参数量的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Performance-wrt-Model-Size.png"></p><h4 id="translation">Translation</h4><p>GPT-2由于模型能力的限制，只在英文数据集上进行了训练。GPT-3的训练数据集有93%的英文数据，有7%的其他语言数据，包括法语、德语和罗马语。</p><h4 id="winograd-style-tasks">Winograd-Style Tasks</h4><p>Winograd-Style Tasks是NLP中的经典任务，主要是根据上下文的语义决定代词的含义。</p><h4 id="common-sense-reasoning">Common Sense Reasoning</h4><ul><li>PhysicalQA：物理知识推理</li><li>ARC：3-9年级科学考试中的多选题</li><li>OpenBookQA：阅读理解</li></ul><h4 id="reading-comprehension">Reading Comprehension</h4><h4 id="superglue">SuperGLUE</h4><h4 id="natural-language-inference">Natural Language Inference</h4><h4 id="synthetic-and-qualitative-tasks">Synthetic and Qualitative Tasks</h4><h5 id="arithmetic">Arithmetic</h5><h5 id="word-scrambling-and-manipulation-tasks">Word Scrambling and Manipulation Tasks</h5><ul><li>Cycle letters in word</li><li>Anagrams of all but first and last character</li><li>Anagrams of all but first and last two characters</li><li>Random insertion in word</li><li>Reversed words ##### SAT Analogies ##### News Article Generation 通过测试人类是否能区分文章是人写的还是大模型写的来评估大模型在这方面的能力。175B的GPT-3生成的文章参与测试的80个人判断准确率仅52%，意味着GPT-3生成的文章已经基本达到人类水平。 ##### Correcting English Grammar</li></ul><h4 id="measuring-and-preventing-memorization-of-benchmarks">Measuring and Preventing Memorization of Benchmarks</h4><p>由于训练模型的数据来自互联网，因此基准测试集中的测试数据很可能也包含在模型训练数据中。</p><h2 id="gpt-4-gpt-4-technical-report">GPT-4: GPT-4 Technical Report</h2><p>GPT-4是一个多模态大模型，可以同时处理文本和图像，并生成文本。 GPT-4是使用RLHF（Reinforcement Learning from Human Feedback）进行微调的。 GPT-4的贡献在于OpenAI开发了深度学习栈基础设置和最优化算法，让不同尺度的模型表现变得可以预测。基于这项技术，OpenAI可以以1/1000的算力训练和评估模型。 GPT-4训练的数据集来自2021年9月之前的数据。 Rule based reward models(RBRM)是GPT-4模型Zero-shot的分类器。这个模型在RLHF微调阶段提供额外的信号，来校准GPT-4的输出行为。 RBRM的输入包含三部分：1. Prompt；2. 模型的输出；3. 人类定义的评估规则集合。 ## 数据集 * <a href="https://commoncrawl.org/get-started">Common Crawl</a> - Over 250 billion pages spanning 17 years. - Free and open corpus since 2007. - Cited in over 10,000 research papers. - 3–5 billion new pages added each month. * <a href="https://paperswithcode.com/dataset/webtext">WebText</a> - WebText is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. The authors scraped all outbound links from Reddit which received at least 3 karma. - WebText contains the text subset of these 45 million links. It consists of over 8 million documents for a total of 40 GB of text. - All Wikipedia documents were removed from WebText since it is a common data source for other datasets. * <a href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a> - The release of a beta version of Open WebText – an open source effort to reproduce OpenAI’s WebText dataset. - <a href="https://huggingface.co/datasets/Skylion007/openwebtext">Download</a> * <a href="https://catalog.ldc.upenn.edu/LDC99T42">Penn TreeBank(PTB)</a> - The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC95T7) and Treebank-3 (LDC99T42) releases of PTB. Treebank-2 includes the raw text for each story. Three "map" files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER. * <a href="https://zenodo.org/records/2630551#.YFJVaWT7S_w">LAMBDA</a> - We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text. * <a href="https://rowanzellers.com/hellaswag/">HellaSwag</a> - Pick the best ending to the context. - We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. * <a href="https://huggingface.co/datasets/Salesforce/wikitext">WikiText-103</a> - The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. - Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. - The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies. * <a href="https://www.gutenberg.org/">Books1</a> - OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。 * <a href="https://www.gutenberg.org/">Books2</a> - OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。 * <a href="https://huggingface.co/datasets/LSDSem/story_cloze">StoryCloze</a> - This dataset proposes a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. - It contains a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. - This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. * <a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a> - To help spur development in open-domain question answering, we have created the Natural Questions (NQ) corpus, along with a challenge website based on this data. - The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. - The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets. * <a href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">WebQuestions</a> - This dataset consists of 6,642 question/answer pairs. - The questions are supposed to be answerable by Freebase, a large knowledge graph. - The questions are mostly centered around a single named entity. - The questions are popular ones asked on the web (at least in 2013). * <a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a> - TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. - TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. * <a href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">Winograd Schema Challenge</a> - The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. * <a href="https://winogrande.allenai.org/">Winograde</a> - WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. * <a href="">PhysicalQA</a> - PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP. * <a href="https://github.com/google-research-datasets/arc">ARC</a> * <a href="https://stanfordnlp.github.io/coqa/">CoQA</a> * <a href="https://rajpurkar.github.io/SQuAD-explorer/">QuAC</a> * <a href="https://github.com/google-research-datasets/drop">DROP</a> * <a href="">RACE</a> * <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/multinews.jsonl">MultiNews</a> * <a href="https://openbookqa.org/">OpenBookQA</a> * <a href="https://github.com/tliuhuan/COPA">COPA</a> * <a href="https://github.com/allenai/record">ReCoRD</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/boolq.jsonl">BoolQ</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/multiRC.jsonl">MultiRC</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/rte.jsonl">RTE</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/anli.jsonl">ANLI</a> * <a href="https://huggingface.co/datasets/webarchive/resolve/main/triviaqa.jsonl">TruthfulQA</a></p><h2 id="附录">附录</h2><h3 id="bleu">BLEU</h3><p>BLEU（Bilingual Evaluation Understudy）是自然语言处理 (NLP) 中一种常用的评估指标，主要用于衡量机器翻译系统生成文本的质量，由IBM的研究人员Kenney和Manning在2002年提出。BLEU通过计算生成文本与参考文本之间的相似度来评估翻译的准确性和流畅性。，来评估机器翻译的质量。</p><p>以下是 BLEU 的核心思想及其计算方式：</p><h4 id="核心思想">核心思想</h4><ol type="1"><li><p><strong>N-gram 匹配</strong>： BLEU 通过计算生成文本与参考文本之间的 n-gram 匹配情况来衡量质量。n-gram 是文本中连续 n 个词的序列（例如，"the cat" 是一个 2-gram）。BLEU 会分别计算 1-gram、2-gram、3-gram 等的匹配情况。</p></li><li><p><strong>精确率 (Precision)</strong>： BLEU 计算生成文本中 n-gram 与参考文本中 n-gram 的匹配比例。例如，如果生成文本包含 10 个 2-gram，而其中 6 个在参考文本中出现，则 2-gram 的精确率为 60%。</p></li><li><p><strong>惩罚机制 (Brevity Penalty)</strong>： 为了避免生成系统通过过于简短的输出（例如，重复参考中的高频词）获得高分，BLEU 引入了长度惩罚。如果生成文本的长度明显短于参考文本，BLEU 分数会降低。</p></li></ol><h4 id="计算步骤">计算步骤</h4><ol type="1"><li><p><strong>计算 n-gram 的精确率</strong>： <span class="math display">\[P_n = \frac{\text{生成文本中与参考文本匹配的 n-gram 数}}{\text{生成文本中的 n-gram 总数}}\]</span></p></li><li><p><strong>几何平均精确率</strong>： 对不同 n-gram（如 1-gram 到 4-gram）的精确率取几何平均： <span class="math display">\[\text{Precision}_{\text{geometric mean}} = \left( \prod_{n=1}^{N} P_n \right)^{1/N}\]</span></p></li><li><p><strong>长度惩罚 (Brevity Penalty, BP)</strong>： 如果生成文本长度 ( c ) 小于参考文本长度 ( r )，引入惩罚因子： <span class="math display">\[BP = \begin{cases} 1 &amp; \text{如果 } c &gt; r \\ e^{1 - r/c} &amp; \text{如果 } c \leq r \end{cases}\]</span></p></li><li><p><strong>最终 BLEU 分数</strong>： 将几何平均精确率和惩罚因子相结合： <span class="math display">\[BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log P_n \right)\]</span> 其中 ( w_n ) 是 n-gram 的权重（通常均分，如 ( w_n = 1/N )）。</p></li></ol><h4 id="优点与局限性">优点与局限性</h4><p><strong>优点</strong>： - 自动化评估，计算快速。 - 与参考翻译的直接比较，适用于多种语言。</p><p><strong>局限性</strong>： 1. <strong>忽略语义</strong>：BLEU 只关心 n-gram 的表面匹配，不考虑语义。 2. <strong>依赖参考翻译</strong>：如果参考翻译质量不好，BLEU 分数也会受到影响。 3. <strong>长文本适用性弱</strong>：对段落或整篇文章的翻译，BLEU 可能无法全面反映质量。</p><h3 id="bpe">BPE</h3><p>BPE（Byte Pair Encoding）是一种基于数据压缩技术的<strong>子词分割算法</strong>，广泛应用于自然语言处理（NLP）任务中，尤其是神经机器翻译和预训练语言模型。其核心思想是通过递归地合并频率最高的字符对或子词对，逐步构建适应语料分布的子词单元，从而在减少词表大小和处理未登录词（OOV）问题之间取得平衡。</p><h4 id="为什么需要-bpe">为什么需要 BPE？</h4><p>传统的分词方法（如按词分割或按字符分割）存在以下问题： 1. <strong>按词分割</strong>： - 词表会非常大，难以处理。 - 对低频词表现不好，容易导致 OOV（Out-Of-Vocabulary，未登录词）。 2. <strong>按字符分割</strong>： - 词表很小，但上下文关系的学习难度大。 - 生成的序列太长，影响模型效率。</p><p><strong>BPE</strong> 通过将高频的字符序列合并为子词单元，既降低了词表大小，又能捕捉常见词的整体信息，解决了上述问题。</p><h4 id="bpe-的基本原理">BPE 的基本原理</h4><p>BPE 的核心思想是<strong>基于频率的合并</strong>：从初始字符开始，将出现频率最高的字符对逐步合并为新的子词单元，直到达到预定义的分词单元数量或其他停止条件。</p><h5 id="算法步骤">算法步骤：</h5><ol type="1"><li><strong>初始化</strong>：<ul><li>将训练语料中的每个单词分解为字符序列，并在每个单词的末尾添加特殊标记（如 <code>▁</code> 表示词的开头）。</li><li>例如：<code>hello world</code> → <code>h e l l o ▁ w o r l d</code></li></ul></li><li><strong>统计字符对的频率</strong>：<ul><li>计算语料中每对相邻字符的频率。</li><li>例如：<code>h e l l o</code> 中，<code>he</code>、<code>el</code>、<code>ll</code>、<code>lo</code> 是字符对，统计其出现次数。</li></ul></li><li><strong>合并频率最高的字符对</strong>：<ul><li>找到频率最高的字符对，将其合并为新的子词单元。</li><li>例如：如果 <code>ll</code> 是频率最高的对，则合并为 <code>ll</code>，更新序列为：<code>h e ll o</code>.</li></ul></li><li><strong>重复步骤 2 和 3</strong>：<ul><li>每次合并后重新统计频率，直到达到预定义的子词单元数量。</li></ul></li><li><strong>生成子词词表</strong>：<ul><li>记录所有生成的子词单元，作为最终的分词词表。</li></ul></li></ol><h5 id="算法实现">算法实现：</h5><p>BPE 的核心可以表示为以下伪代码：</p><figure class="highlight livecodeserver"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">输入：语料 D，目标子词表大小 V</span><br><span class="line">初始化：将 D 分解为字符序列 S</span><br><span class="line"><span class="keyword">while</span> |S| &lt; V:</span><br><span class="line">    统计 S 中所有相邻字符对的频率</span><br><span class="line">    找到频率最高的字符对 <span class="keyword">a</span>, b</span><br><span class="line">    将 <span class="keyword">a</span>, b 合并为新的子词单元 ab</span><br><span class="line">    更新 S 中的所有出现</span><br><span class="line">输出：最终的子词词表 S</span><br></pre></td></tr></tbody></table></figure><h5 id="示例">示例</h5><p>假设我们对以下单词应用 BPE：<code>low</code>, <code>lowest</code>, <code>new</code>, <code>newer</code>.</p><ol type="1"><li><p><strong>初始状态</strong>： </p><figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l o w</span><br><span class="line">l o w e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p><strong>统计字符对频率</strong>： </p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">l</span> o: <span class="number">2</span>, o w: <span class="number">2</span>, w e: <span class="number">2</span>, e s: <span class="number">1</span>, s t: <span class="number">1</span>, n e: <span class="number">2</span>, e w: <span class="number">2</span>, w e: <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p><strong>合并频率最高的对</strong>： 假设 <code>o w</code> 频率最高，合并为 <code>ow</code>： </p><figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l ow</span><br><span class="line">l ow e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p><strong>重复以上步骤</strong>：</p><ul><li>合并后重新统计频率并合并，直到达到预定义的子词单元数量。</li></ul></li></ol><h4 id="bpe-在-nlp-中的应用">BPE 在 NLP 中的应用</h4><ol type="1"><li><strong>机器翻译</strong>：如 Transformer 中，BPE 被用作标准分词方法，以减少 OOV 现象并提高模型的泛化能力。</li><li><strong>预训练模型</strong>：如 GPT 和 BERT，BPE 用于构建子词词表，兼顾词表大小与覆盖率。</li><li><strong>多语言任务</strong>：BPE 在多语言模型中尤其有效，能够生成语言无关的统一词表。</li></ol><h4 id="优点与局限性-1">优点与局限性</h4><p><strong>优点</strong>： 1. <strong>词表大小灵活</strong>：通过限制词表大小 ( V )，可以在效率与表达能力之间取得平衡。 2. <strong>处理未登录词（OOV）</strong>：将罕见词分解为子词单元，从而减少 OOV 的发生。 3. <strong>语言无关性</strong>：BPE 是基于频率的统计方法，不依赖具体语言规则。</p><p><strong>局限性</strong>： 1. <strong>固定词表</strong>：训练好的子词词表对新领域的适应性有限。 2. <strong>上下文独立性</strong>：合并规则是基于全局频率的，而不考虑上下文。 3. <strong>生成效率</strong>：在解码过程中，可能导致生成序列较长，影响模型推断速度。</p><h4 id="总结">总结</h4><p>BPE 是一种简单而高效的分词算法，能够生成兼具词语信息和灵活性的子词单元，在 NLP 中广泛应用，是现代语言模型的关键预处理步骤之一。</p><h3 id="superglue基准测试集">SuperGLUE基准测试集</h3><p>SuperGLUE（<strong>A Stickier Benchmark for General-Purpose Language Understanding Systems</strong>）是自然语言处理（NLP）领域中的一个基准测试集，用于评估模型在广泛的语言理解任务上的能力。它是对 GLUE（General Language Understanding Evaluation）的改进版，专门设计来挑战基于预训练语言模型（如 BERT、GPT）在多种语言理解任务上的性能，涵盖更复杂的推理和语义理解问题。</p><h4 id="superglue-的核心组成"><strong>SuperGLUE 的核心组成</strong></h4><p>SuperGLUE 包含 8 个不同的任务，每个任务对应一种语言理解的核心能力。以下是每个任务的详细说明：</p><h5 id="boolean-question-boolq">1. <strong>Boolean Question (BoolQ)</strong></h5><ul><li><strong>任务类型</strong>：二分类任务（Yes/No）。</li><li><strong>目标</strong>：根据给定的段落和问题，判断答案是“是”还是“否”。</li><li><strong>输入</strong>：一个段落（context）和一个自然语言问题（question）。</li><li><strong>输出</strong>：布尔值（Yes/No）。</li><li><strong>数据来源</strong>：从真实用户查询生成的问题，例如维基百科。</li><li><strong>挑战</strong>：<ul><li>问题的答案需要通过复杂推理和跨句子语义整合来确定。</li></ul></li></ul><hr><h5 id="commitmentbank-cb">2. <strong>CommitmentBank (CB)</strong></h5><ul><li><strong>任务类型</strong>：三分类任务（Entailment / Neutral / Contradiction）。</li><li><strong>目标</strong>：判断一个假设（hypothesis）是否从给定的文本段落（premise）中可以：<ol type="1"><li>推出（Entailment），</li><li>不确定（Neutral），</li><li>或矛盾（Contradiction）。</li></ol></li><li><strong>输入</strong>：一个前提句（premise）和一个假设句（hypothesis）。</li><li><strong>输出</strong>：三分类标签。</li><li><strong>数据来源</strong>：对语言学研究中的嵌套从句进行标注。</li><li><strong>挑战</strong>：<ul><li>涉及复杂的语义推理和句法关系。</li></ul></li></ul><hr><h5 id="choice-of-plausible-alternatives-copa">3. <strong>Choice of Plausible Alternatives (COPA)</strong></h5><ul><li><strong>任务类型</strong>：因果推理（二选一）。</li><li><strong>目标</strong>：判断给定的句子是否是因果链中的原因或结果，并从两个备选答案中选择一个。</li><li><strong>输入</strong>：一个句子和两个备选答案。</li><li><strong>输出</strong>：选择一个更可能的答案（1 或 2）。</li><li><strong>数据来源</strong>：手动创建的因果推理数据。</li><li><strong>挑战</strong>：<ul><li>涉及因果关系的推理。</li><li>候选答案具有一定的迷惑性。</li></ul></li></ul><hr><h5 id="multi-sentence-reading-comprehension-record">4. <strong>Multi-Sentence Reading Comprehension (ReCoRD)</strong></h5><ul><li><strong>任务类型</strong>：填空问题。</li><li><strong>目标</strong>：通过阅读长段落，选择填充问题空白的正确词语或短语。</li><li><strong>输入</strong>：一个长段落和包含空白的句子。</li><li><strong>输出</strong>：正确的词语或短语（从段落中选取）。</li><li><strong>数据来源</strong>：新闻语料库。</li><li><strong>挑战</strong>：<ul><li>涉及跨句子关系的理解。</li><li>候选答案可能具有语义相似性，增加了任务难度。</li></ul></li></ul><hr><h5 id="reading-comprehension-with-commonsense-reasoning-rte">5. <strong>Reading Comprehension with Commonsense Reasoning (RTE)</strong></h5><ul><li><strong>任务类型</strong>：二分类任务（Entailment / Not Entailment）。</li><li><strong>目标</strong>：判断假设（hypothesis）是否可以从前提句（premise）中推出。</li><li><strong>输入</strong>：前提句和假设句。</li><li><strong>输出</strong>：二分类标签（Entailment / Not Entailment）。</li><li><strong>数据来源</strong>：多个现有的自然语言推理数据集。</li><li><strong>挑战</strong>：<ul><li>推理涉及隐含的背景知识和常识。</li></ul></li></ul><hr><h5 id="winograd-schema-challenge-wic">6. <strong>Winograd Schema Challenge (WiC)</strong></h5><ul><li><strong>任务类型</strong>：词义消歧。</li><li><strong>目标</strong>：判断某个词在两个句子中的含义是否相同。</li><li><strong>输入</strong>：两个句子和目标词。</li><li><strong>输出</strong>：布尔值（Yes/No）。</li><li><strong>数据来源</strong>：词义标注和上下文信息。</li><li><strong>挑战</strong>：<ul><li>涉及对多义词的准确语义判断。</li><li>考验模型对上下文细微差别的理解。</li></ul></li></ul><hr><h5 id="winograd-schema-challenge-wsc">7. <strong>Winograd Schema Challenge (WSC)</strong></h5><ul><li><strong>任务类型</strong>：共指消解。</li><li><strong>目标</strong>：判断代词或名词短语的指代对象。</li><li><strong>输入</strong>：一个句子，包含一个指代目标和两个候选对象。</li><li><strong>输出</strong>：正确的指代对象。</li><li><strong>数据来源</strong>：手工创建的共指消解数据。</li><li><strong>挑战</strong>：<ul><li>考察模型对代词解析和语义常识的理解。</li></ul></li></ul><hr><h5 id="ax-b-broadcoverage-diagnostic">8. <strong>AX-b (BroadCoverage Diagnostic)</strong></h5><ul><li><strong>任务类型</strong>：对文本片段进行语言现象的诊断评估。</li><li><strong>目标</strong>：分析模型在特定语言现象（如逻辑推理或世界知识推理）上的表现。</li><li><strong>输入和输出</strong>：与其他任务类似，具体设计用于测试模型的弱点。</li><li><strong>挑战</strong>：<ul><li>设计针对性强，旨在暴露模型局限。</li></ul></li></ul><h4 id="superglue-的评价方式"><strong>SuperGLUE 的评价方式</strong></h4><ol type="1"><li><strong>任务指标</strong>：<ul><li>每个任务有特定的评估指标（如准确率、F1 分数等）。</li></ul></li><li><strong>总体分数</strong>：<ul><li>通过所有任务的加权平均分数评估模型整体性能。</li></ul></li><li><strong>人类基准</strong>：<ul><li>超过 GLUE 基准的人类水平设置为 SuperGLUE 的参考基线。</li></ul></li></ol><h4 id="superglue-的意义"><strong>SuperGLUE 的意义</strong></h4><ul><li><strong>更高难度</strong>：SuperGLUE 设计了更复杂的语言理解任务，考验模型的推理能力、常识知识和语义理解。</li><li><strong>推动研究</strong>：提供了统一的评估框架，促进了更强大和通用的语言理解模型的开发。</li><li><strong>领域覆盖广</strong>：任务涉及多种语言理解能力，使模型需具备广泛的适应性。</li></ul><h4 id="模型挑战"><strong>模型挑战</strong></h4><p>虽然当前模型（如 T5、GPT-4 等）已经在部分任务上接近或超过人类水平，但在涉及常识推理、因果关系和细粒度语义分析的任务上，仍然存在改进空间。SuperGLUE 因此被视为推动 NLP 领域进步的重要基准之一。</p><h3 id="mmlu基准测试集">MMLU基准测试集</h3><p><strong>MMLU (Massive Multitask Language Understanding)</strong> 是一个用于评估语言模型多任务能力的基准测试集。它专门设计用于测试大语言模型（如 GPT 系列）的 <strong>多任务学习（multitask learning）</strong> 和 <strong>通用语言理解能力（general language understanding）</strong>。MMLU 基准测试集涵盖了多种不同类型的任务，从常见的分类、推理任务到更加专业的知识领域（如医学、法律等）的应用，能够全面地评估语言模型在各种场景下的表现。</p><h4 id="mmlu-基准测试集的构成"><strong>MMLU 基准测试集的构成</strong></h4><p>MMLU 基准测试集包括多种类型的任务，以下是几个主要类别的任务和一些常见的子任务：</p><ol type="1"><li><strong>常识推理</strong>：<ul><li>测试语言模型在日常常识方面的理解。</li><li>示例任务：回答关于世界常识的问题（如历史事件、人物、科学常识等）。</li></ul></li><li><strong>数学与逻辑推理</strong>：<ul><li>包含算术、代数、几何等数学问题的解答。</li><li>示例任务：解决数学运算问题（例如基础的加减乘除，甚至更复杂的几何和代数问题）。</li></ul></li><li><strong>科学与技术理解</strong>：<ul><li>测试模型在物理、化学、生物学等科学领域的知识理解。</li><li>示例任务：解答关于物理学原理、化学反应、生物学概念等问题。</li></ul></li><li><strong>专业领域知识</strong>：<ul><li>涉及法律、医学、金融等专业领域的任务。</li><li>示例任务：法律案件分析、医学症状诊断、财务报告分析等。</li></ul></li><li><strong>语言学任务</strong>：<ul><li>涉及语法、语义分析和推理的任务。</li><li>示例任务：句法分析、语义关系提取、情感分析等。</li></ul></li><li><strong>阅读理解与推理</strong>：<ul><li>包括从文章或段落中提取信息并进行推理。</li><li>示例任务：阅读理解（给定一段文章后回答相关问题）和推理任务（基于文章进行逻辑推理）。</li></ul></li><li><strong>文化与历史</strong>：<ul><li>任务涉及对文化、历史、地理和人类社会的理解。</li><li>示例任务：回答历史事件的具体细节或文化现象的解读。</li></ul></li></ol><h4 id="mmlu的主要特点"><strong>MMLU的主要特点</strong></h4><ul><li><strong>多样性</strong>：MMLU 涵盖了从基础的语言理解任务到复杂的推理、专业知识领域的任务，涉及多个领域和知识层面。</li><li><strong>难度范围广</strong>：包含了从容易到困难不等的任务，帮助测试模型在不同难度级别下的能力。</li><li><strong>高质量问题</strong>：问题通常经过精心设计，涵盖了广泛的知识，确保测试的全面性和代表性。</li><li><strong>标准化的评估</strong>：通过统一的任务和标准化的评分体系，可以比较不同语言模型在同一基准上的表现。</li></ul><h4 id="mmlu基准测试集的评价标准"><strong>MMLU基准测试集的评价标准</strong></h4><p>MMLU 基准测试的性能通常使用 <strong>准确率（Accuracy）</strong> 来衡量。对于每个任务，模型给出的答案会根据是否正确进行评分，最终通过加权平均计算出各任务的综合准确率。</p><ul><li><strong>任务的加权</strong>：MMLU 通常会为不同类型的任务设定不同的权重，以确保最终的评分能够准确反映模型的综合能力。</li><li><strong>样本量</strong>：每个任务中的问题通常有多个样本，这些样本被设计为具有挑战性的，确保评估结果具有代表性。</li></ul><h4 id="mmlu的应用"><strong>MMLU的应用</strong></h4><p>MMLU 的设计使其成为测试大型语言模型多任务能力和理解深度的标准工具。它的应用主要体现在以下几个方面：</p><ol type="1"><li><strong>语言模型的综合能力评估</strong>：通过全面评估模型在多种任务中的表现，MMLU 可以帮助研究人员和工程师了解模型的通用语言理解能力。</li><li><strong>模型之间的比较</strong>：MMLU 提供了一个统一的测试标准，可以用来比较不同模型的性能，尤其是不同规模、架构的语言模型之间。</li><li><strong>多任务学习研究</strong>：MMLU 对于探索和优化多任务学习有重要意义。它帮助研究人员测试在多个任务上训练模型时的效果，评估模型如何从多个任务中进行泛化。</li></ol><h4 id="mmlu基准测试的例子"><strong>MMLU基准测试的例子</strong></h4><h5 id="例1常识推理任务"><strong>例1：常识推理任务</strong></h5><p>任务：哪个国家位于北美洲？ - 选项：A) 加拿大 B) 德国 C) 巴西 D) 中国 - 正确答案：A) 加拿大</p><h5 id="例2数学任务"><strong>例2：数学任务</strong></h5><p>任务：5x + 3 = 23, 求x的值。 - 选项：A) 3 B) 4 C) 5 D) 2 - 正确答案：B) 4</p><h5 id="例3法律任务"><strong>例3：法律任务</strong></h5><p>任务：根据美国宪法，谁是最高法院的法官？ - 选项：A) 总统 B) 最高法院首席法官 C) 美国国会 D) 各州州长 - 正确答案：B) 最高法院首席法官</p><h5 id="例4科学任务"><strong>例4：科学任务</strong></h5><p>任务：光合作用是如何发生的？ - 选项：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖 B) 通过细胞呼吸 C) 通过蒸发 D) 通过化学反应 - 正确答案：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖</p><h4 id="mmlu的挑战和改进"><strong>MMLU的挑战和改进</strong></h4><ol type="1"><li><strong>任务设计的复杂性</strong>：随着语言模型变得越来越强大，测试集的任务设计也需要不断提升，以确保能够衡量模型的深度理解和推理能力。</li><li><strong>多样化的测试</strong>：MMLU 还可以进一步扩展，涵盖更多样化的领域和任务，尤其是跨模态任务（如图像、视频等）的结合。</li><li><strong>不平衡性</strong>：不同任务的难度不均衡，可能导致一些模型在某些任务上表现突出，而在另一些任务上表现较差。</li></ol><h4 id="总结-1"><strong>总结</strong></h4><p>MMLU（Massive Multitask Language Understanding）是一个综合性强、覆盖面广的基准测试集，旨在评估语言模型在各种任务上的表现。它通过涉及常识推理、数学、科学、法律等多种任务，全面衡量模型的多任务学习能力，成为衡量大规模语言模型通用能力的重要工具。</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是OpenAI的GPT系列论文的阅读笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Deep-Learning/Papers/"/>
    
    
      <category term="GPT" scheme="http://blog.zhaoyongsheng.com/tags/GPT/"/>
    
      <category term="LLM" scheme="http://blog.zhaoyongsheng.com/tags/LLM/"/>
    
      <category term="NLP" scheme="http://blog.zhaoyongsheng.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Vision Servo Control For Robot Manipulation</title>
    <link href="http://blog.zhaoyongsheng.com/2024/10/02/Vision-Servo-Control-For-Robot-Manipulation/"/>
    <id>http://blog.zhaoyongsheng.com/2024/10/02/Vision-Servo-Control-For-Robot-Manipulation/</id>
    <published>2024-10-02T02:14:50.000Z</published>
    <updated>2024-10-02T02:14:50.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>视觉伺服是多自由度机械臂灵巧作业领域经典运动控制算法之一，常用于对精细作业和范化作业，例如利用视觉伺服实现对接线面板和USB接口的插拔。视觉伺服算法的核心思想是利用视觉反馈信息，对机械臂末端执行器进行实时位置和姿态调整，以实现精确的运动控制。视觉伺服通常分为两类，一类是基于图像特征的视觉伺服（IBVS），另一类是基于位置姿态的视觉伺服（PBVS）。本文将介绍这两种视觉伺服算法的基本原理和实现方法。 <span id="more"></span> ## 背景理论 视觉伺服本质上是一个在坐标转换（平移、旋转）运动空间的非线性优化问题，因此会用到<strong>Gauss-Newton</strong>非线性优化算法和<strong>Jacobian矩阵</strong>。 ### 非线性优化问题 将最优化问题的目标函数表示为<span class="math inline">\(f(x)\)</span>，其中<span class="math inline">\(f(x)\)</span>二阶连续可微，那么最优化的问题可以转化： <span class="math display">\[{argmin}_{x}f(x)\]</span> 即求解<span class="math inline">\(f(x)\)</span>的最小值以及对应的<span class="math inline">\(x^{*}\)</span>。 复杂的非线性最优化问题，<span class="math inline">\(f(x)\)</span>一般为高维非线性方程组，<span class="math inline">\(x\)</span>为多维向量，即<span class="math inline">\(x \in \mathbb{R}^n\)</span>，<span class="math inline">\(f(x) \in \mathbb{R}^m\)</span>，其中<span class="math inline">\(m \geq n\)</span>。</p><p>对于机械臂视觉伺服运动控制问题，上述非线性优化问题可以表示为： <span class="math display">\[x = \begin{bmatrix}t &amp; r \end{bmatrix}\]</span> 其中<span class="math inline">\(t\)</span>表示坐标转换空间中的平移向量，维度为<span class="math inline">\([3x1]\)</span>，<span class="math inline">\(r\)</span>表示坐标转换空间中的旋转向量，维度为<span class="math inline">\([3x1]\)</span>。 <span class="math display">\[f(x) = x_{target} - x_{current}\]</span> 或者 <span class="math display">\[f(x) = x_{target}^{-1} x_{current}\]</span> 公式选择取决于参考坐标系的选择，已经视觉伺服方式的选择，具体请见<strong>视觉伺服</strong>章节。</p><h3 id="jacobian矩阵">Jacobian矩阵</h3><p>对于复杂的非线性目标函数，在数学上直接求解解析解比较复杂，一般通过一级泰勒公式展开进行线性化近似，即： <span class="math display">\[f(x+\Delta x) \approx f(x) + J(x)\Delta x\]</span> 其中，<span class="math inline">\(J(x)\)</span>为<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x\)</span>处偏导数<span class="math inline">\(\frac{\partial f(x)}{\partial x}\)</span>，称为Jacobian矩阵，<span class="math inline">\(\Delta x\)</span>为增量。</p><p>假设<span class="math inline">\(f(x) \in \mathbb{R}^m\)</span>，<span class="math inline">\(x \in \mathbb{R}^n\)</span>，则雅可比矩阵表示的是<span class="math inline">\(\mathbb{R}^n\)</span>空间到<span class="math inline">\(\mathbb{R}^m\)</span>空间的线性映射，是一个<span class="math inline">\([m×n]\)</span>的矩阵，换句话讲其重要意义在于它表现了一个多变数向量函数的最佳线性逼近，公式为： <span class="math display">\[J(x) = \frac{\partial f(x)}{\partial x} = \begin{bmatrix}\frac{\partial f(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f(x)}{\partial x_{n}}\end{bmatrix} = \begin{bmatrix}\frac{\partial f_{1}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{1}(x)}{\partial x_{n}} \\\vdots &amp; \ddots &amp; \vdots \\\frac{\partial f_{m}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{m}(x)}{\partial x_{n}} \\\end{bmatrix}\]</span> 一级泰勒公式展开线性逼近公式可以描述为矩阵形式： <span class="math display">\[\begin{bmatrix}f_{1}(x + \Delta x) \\ \vdots \\ f_{m}(x + \Delta x)\end{bmatrix} = \begin{bmatrix}f_{1}(x) \\ \vdots \\ f_{m}(x)\end{bmatrix} + \begin{bmatrix}\frac{\partial f_{1}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{1}(x)}{\partial x_{n}} \\\vdots &amp; \ddots &amp; \vdots \\\frac{\partial f_{m}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{m}(x)}{\partial x_{n}} \\\end{bmatrix}\begin{bmatrix}\Delta x_{1} \\ \vdots \\ \Delta x_{n}\end{bmatrix} \]</span></p><p>如果 <span class="math inline">\(m = n\)</span>，那么雅可比矩阵是一个<span class="math inline">\([nxn]\)</span>方阵。于是我们可以取它的行列式，称为雅可比行列式。 * 参考链接</p><p><a href="https://zh.wikipedia.org/zh-hans/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5">Jacobian Matrix</a></p><p><a href="https://blog.csdn.net/gwplovekimi/article/details/104977255">雅各比矩阵和机械臂关节坐标转换</a></p><h3 id="gauss-newton非线性优化">Gauss-Newton非线性优化</h3><p>对于大部分非线性优化问题，一般很难直接得到解析解。在数学上可以使用<strong>Gauss-Newton</strong>算法通过循环迭代地方式获取非线性方程组的数值解，其基本思想是从给定的初始值<span class="math inline">\(x^{0}\)</span>开始，沿着梯度下降的方向，循环迭代直至目标函数达到或者接近最优值。</p><p>基于一级泰勒公式展开线性逼近方程，可以将最优化问题转换为关于<span class="math inline">\(\Delta x\)</span>的线性最小二乘问题： <span class="math display">\[argmin_{x}\frac{1}{2} \Vert f(x) + J(x)\Delta x \Vert^2\]</span> 其中，<span class="math inline">\(J(x)\)</span>为<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x\)</span>处偏导数<span class="math inline">\(\frac{\partial f(x)}{\partial x}\)</span>，称为Jacobian矩阵，<span class="math inline">\(\Delta x\)</span>为增量。</p><p>注意：<strong>Gauss-Newton</strong>算法必须要转换成最小二乘问题进行求解。</p><p>求取上述公式相对于<span class="math inline">\(\Delta x\)</span>的导数，并令导数等于0，即可得到最优解。推导过程如下所示：</p><p>已知公式：<span class="math inline">\(\Vert X \Vert^2 = X^TX\)</span>，其中<span class="math inline">\(X\)</span>为矩阵。</p><p>可以将最小二乘函数转化为： <span class="math display">\[\begin{align}\frac{1}{2} \Vert f(x) + J(x)\Delta x \Vert^2 &amp;= (f(x) + J(x)\Delta x)^T(f(x) + J(x)\Delta x) \\&amp;= f(x)^Tf(x) + \Delta x^TJ(x)^Tf(x) + f(x)^TJ(x)\Delta x + \Delta x^TJ(x)^TJ(x)\Delta x\end{align}\]</span> 已知矩阵求导公式<span class="math inline">\(\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x} = a\)</span>，<span class="math inline">\(\frac{}{}\)</span> $$</p><p>$$ 迭代公式为： * 参考链接</p><p><a href="https://zhuanlan.zhihu.com/p/482540286?utm_id=0">Bundle Adjustment 重投影误差模型及相应雅克比公式推导</a></p><p><a href="https://www.cnblogs.com/bingjianing/p/9093054.html">Jacobian矩阵、Hessian矩阵和Newton's method</a></p><p><a href="https://loopvoid.github.io/2018/04/28/Jacobian%E7%9F%A9%E9%98%B5%E4%B8%8EHessian%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/">Jacobian矩阵与Hessian矩阵与最小二乘</a></p><p><a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导</a> ## 视觉伺服 ### 基于位置的视觉伺服（PBVS） ### 基于图像的视觉伺服（IBVS）</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;视觉伺服是多自由度机械臂灵巧作业领域经典运动控制算法之一，常用于对精细作业和范化作业，例如利用视觉伺服实现对接线面板和USB接口的插拔。视觉伺服算法的核心思想是利用视觉反馈信息，对机械臂末端执行器进行实时位置和姿态调整，以实现精确的运动控制。视觉伺服通常分为两类，一类是基于图像特征的视觉伺服（IBVS），另一类是基于位置姿态的视觉伺服（PBVS）。本文将介绍这两种视觉伺服算法的基本原理和实现方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Algorithms" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Algorithms/"/>
    
      <category term="Robotics" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Algorithms/Robotics/"/>
    
    
      <category term="Vision Servo" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Servo/"/>
    
      <category term="PBVS" scheme="http://blog.zhaoyongsheng.com/tags/PBVS/"/>
    
      <category term="IBVS" scheme="http://blog.zhaoyongsheng.com/tags/IBVS/"/>
    
  </entry>
  
  <entry>
    <title>Usage Of Testing Tool Locust</title>
    <link href="http://blog.zhaoyongsheng.com/2024/08/22/Usage-Of-Testing-Tool-Locust/"/>
    <id>http://blog.zhaoyongsheng.com/2024/08/22/Usage-Of-Testing-Tool-Locust/</id>
    <published>2024-08-22T02:06:02.000Z</published>
    <updated>2024-08-22T02:06:02.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>完成后端服务开发后，在业务上线之前，通常需要进行压力测试，以评估目前的服务器算力在不同QPS请求下的负载承受能力，以此做系统后端可靠性分析。<strong>Locust</strong>是基于Python接口的开源压力测试工具，本文将介绍如何使用该工具对后端服务器进行压力测试。 <span id="more"></span> ### 安装 可以直接使用<strong>pip</strong>包管理工具进行安装，如下所示： </p><figure class="highlight cmake"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> locust</span><br></pre></td></tr></tbody></table></figure> 需要将<strong>Locust</strong>安装目录添加到<strong>PATH</strong>中，这样可以直接在<strong>terminal</strong>中使用<strong>locust</strong>指令。 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.zshrc</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">PATH</span>=<span class="string">"/home/parallels/.local/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></tbody></table></figure> 安装完成后，可以通过如下指令查看并确认安装是否成功： <figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust -V</span></span><br></pre></td></tr></tbody></table></figure> 如果看到以下信息，则表明安装成功。 <figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust</span> <span class="number">2</span>.<span class="number">31</span>.<span class="number">3</span> from /home/parallels/.local/lib/python3.<span class="number">10</span>/site-packages/locust (Python <span class="number">3</span>.<span class="number">10</span>.<span class="number">12</span>, OpenSSL <span class="number">3</span>.<span class="number">0</span>.<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure> ### 压测用例 * HttpUser 首先创建一个<strong>locustfile.py</strong>文件，并实现一个继承<strong>HttpUser</strong>的自定义测试类，<strong>Locust</strong>程序在同目录启动后会默认执行该测试用例。 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> response</span><br><span class="line"><span class="keyword">from</span> locust <span class="keyword">import</span> HttpUser, task, between, constant</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ServoApiUser</span>(<span class="title class_ inherited__">HttpUser</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    User class that does requests to the locust web server running on localhost,</span></span><br><span class="line"><span class="string">    using the fast HTTP client</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    host = <span class="string">"https://www.yourwebsite.com"</span></span><br><span class="line">    wait_time = between(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># wait_time = constant(1)</span></span><br><span class="line">    <span class="comment"># some things you can configure on FastHttpUser</span></span><br><span class="line">    <span class="comment"># connection_timeout = 60.0</span></span><br><span class="line">    <span class="comment"># insecure = True</span></span><br><span class="line">    <span class="comment"># max_redirects = 5</span></span><br><span class="line">    <span class="comment"># max_retries = 1</span></span><br><span class="line">    <span class="comment"># network_timeout = 60.0</span></span><br><span class="line">    <span class="comment"># proxy_host = my-proxy.com</span></span><br><span class="line">    <span class="comment"># proxy_port = 8080</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_start</span>(<span class="params">self</span>):</span><br><span class="line">        self.client.headers = {</span><br><span class="line">            <span class="string">'Accept'</span>: <span class="string">'application/json'</span>,</span><br><span class="line">            <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>,</span><br><span class="line">            <span class="string">'Authorization'</span>: <span class="string">'TOKEN xxxxxxxxx'</span>,</span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'csrftoken=xxxxxxx'</span></span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line"><span class="meta">    @task</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_card_info</span>(<span class="params">self</span>):</span><br><span class="line">        url = <span class="string">"/api/card_info/?stage=pregnancy&amp;level=user"</span></span><br><span class="line">        response = self.client.get(url=url, name=<span class="string">'get_card_info'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"status code: {}"</span>.<span class="built_in">format</span>(response.status_code))</span><br><span class="line">        <span class="comment"># print("data: {}".format(response.json()))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># @task</span></span><br><span class="line">    <span class="comment"># def stats(self):</span></span><br><span class="line">    <span class="comment">#     self.client.get("/stats/requests")</span></span><br></pre></td></tr></tbody></table></figure> <strong>host</strong>参数可以设置测试服务器的地址。 <strong>wait_time</strong>参数可以设置每个用户实例完成一个task之后，执行下一个task需要的等待时间，<strong>constant</strong>为固定等待时间，<strong>between</strong>为随机等待时间。该参数可以模拟不同用户随机访问的情况。 <strong>on_start</strong>函数会在测试任务启动时运行一次，通常用户配置一些频繁使用但是测试过程中保持不变的参数变量，例如header。 <strong>task</strong>decorator用于定义一个具体的测试任务，通常为一个<strong>api</strong>接口的<strong>POST</strong>、<strong>GET</strong>、<strong>DELETE、</strong>等request请求任务，用于用户自定义测试任务。 ### 压测 在<strong>locustfile.py</strong>同目录的<strong>terminal</strong>输入以下指令，即可启动压测。 <figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust</span></span><br></pre></td></tr></tbody></table></figure> 在浏览器中访问<strong>http://0.0.0.0:8089</strong>即可开始压测。 <strong>Number of Users</strong>用于配制压测的最高用户数量，配合<strong>wait_time</strong>即可估算出最高QPS <span class="math display">\[qps = \frac{UV}{wait\_time}\]</span> <strong>Ramp Up</strong>用户每秒钟用户增长的数量，用于测试访问用户不断增长情况下，服务器的性能表现。<p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;完成后端服务开发后，在业务上线之前，通常需要进行压力测试，以评估目前的服务器算力在不同QPS请求下的负载承受能力，以此做系统后端可靠性分析。&lt;strong&gt;Locust&lt;/strong&gt;是基于Python接口的开源压力测试工具，本文将介绍如何使用该工具对后端服务器进行压力测试。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Testing Tool" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Testing-Tool/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Testing Tool" scheme="http://blog.zhaoyongsheng.com/tags/Testing-Tool/"/>
    
  </entry>
  
  <entry>
    <title>Learning Note Of Vision Transformer</title>
    <link href="http://blog.zhaoyongsheng.com/2024/07/05/Learning-Note-Of-Vision-Transformer/"/>
    <id>http://blog.zhaoyongsheng.com/2024/07/05/Learning-Note-Of-Vision-Transformer/</id>
    <published>2024-07-04T16:18:01.000Z</published>
    <updated>2024-07-04T16:18:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Vision Transformer (ViT)</strong>是Transformer模型应用到视觉领域的开山之作,在分类识别领域取得了SOTA表现，在<a href="https://github.com/Alibaba-MIIL/ImageNet21K">ImageNet-21K</a>上训练的模型作为骨干基座，被广泛应用在了各个视觉任务中。本文主要介绍<strong>ViT</strong>的模型结构，<a href="https://github.com/jeonsworld/ViT-pytorch?tab=readme-ov-file">pytorch代码实现</a>和训练方法。</p><span id="more"></span><h2 id="vit整体结构">ViT整体结构</h2><p><img alt="Vision Transformer模型架构图" data-src="/2024/07/05/Learning-Note-Of-Vision-Transformer/vision_transformer.png"> <strong>Vision Transformer</strong>是在<strong>Transformer</strong>基础之上构建的，关键是如何将二维图像转换为<strong>Transformer</strong>模型可以处理的序列特征。<strong>ViT</strong>首先将图像用固定大小的patch切分，每一个patch作为一个Token，学习一组Embedding特征，图片的多个patch按照自上而下、从左到右的顺序排列，构成序列。然后后在序列最后拼接一个可以学习的类别Token，用于分类任务。最后在序列Embedding特征中增加位置编码，以保留图片patch的位置信息。</p><h2 id="vit核心模块">ViT核心模块</h2><h3 id="embedding">Embedding</h3><p>该模块是<strong>ViT</strong>模型的核心idea，主要是将二维图像转换为<strong>Transformer</strong>模型可以处理的序列特征。</p><ul><li><p>超参数 -- patch_size，决定了将图像切分为patch的大小 图像序列特征的序列长度，即patch的数量，由输入图像的尺寸<span class="math inline">\([width, height]\)</span>和patch的尺寸共同决定： <span class="math display">\[seq\_len = patch\_num = \frac{width}{patch\_size} * \frac{height}{patch\_size}\]</span></p></li><li><p>超参数 -- hidden_size，决定了embedding特征的长度</p></li><li><p>超参数 -- dropout_rate，决定了embedding特征dropout的比率</p></li></ul><p>切分的patch图像通过二维卷积操作提取Embedding特征，拼接上类目编码之后再加上位置编码。 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Construct the embeddings from patch, position embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, img_size, in_channels=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.hybrid = <span class="literal">None</span></span><br><span class="line">        img_size = _pair(img_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> config.patches.get(<span class="string">"grid"</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            grid_size = config.patches[<span class="string">"grid"</span>]</span><br><span class="line">            patch_size = (img_size[<span class="number">0</span>] // <span class="number">16</span> // grid_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // <span class="number">16</span> // grid_size[<span class="number">1</span>])</span><br><span class="line">            n_patches = (img_size[<span class="number">0</span>] // <span class="number">16</span>) * (img_size[<span class="number">1</span>] // <span class="number">16</span>)</span><br><span class="line">            self.hybrid = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            patch_size = _pair(config.patches[<span class="string">"size"</span>])</span><br><span class="line">            n_patches = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>]) * (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">            self.hybrid = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.hybrid:</span><br><span class="line">            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,</span><br><span class="line">                                         width_factor=config.resnet.width_factor)</span><br><span class="line">            in_channels = self.hybrid_model.width * <span class="number">16</span></span><br><span class="line">        self.patch_embeddings = Conv2d(in_channels=in_channels,</span><br><span class="line">                                       out_channels=config.hidden_size,</span><br><span class="line">                                       kernel_size=patch_size,</span><br><span class="line">                                       stride=patch_size)</span><br><span class="line">        self.position_embeddings = nn.Parameter(torch.zeros(<span class="number">1</span>, n_patches+<span class="number">1</span>, config.hidden_size))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, config.hidden_size))</span><br><span class="line"></span><br><span class="line">        self.dropout = Dropout(config.transformer[<span class="string">"dropout_rate"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># input x shape is [b,c,h,w]</span></span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># cls_token shape: [1, 1, emb_len] ==&gt; [b, 1, emb_len]</span></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.hybrid:</span><br><span class="line">            x = self.hybrid_model(x)</span><br><span class="line">        <span class="comment"># x shape: [b,c,h,w] ==&gt; [b, emb_len, h//patch_size, w//patch_size]</span></span><br><span class="line">        x = self.patch_embeddings(x)</span><br><span class="line">        <span class="comment"># x shape: [b, emb_len, h_p, w_p] ==&gt; [b, emb_len, h_p * w_p] ==&gt; [b, emb_len, n_patches]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, emb_len, n_patches] ==&gt; [b, n_patches, emb_len]</span></span><br><span class="line">        x = x.transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, n_patches, emb_len] ==&gt; [b, n_patches+1, emb_len]</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, n_patches+1, emb_len] ==&gt; [b, n_patches+1, emb_len], add is auto broadcasting</span></span><br><span class="line">        embeddings = x + self.position_embeddings</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="attention">Attention</h3><p><strong>Self-Attention</strong>模块是<strong>Transformer</strong>的核心基础模块。</p><ul><li>超参数 -- num_heads，多头注意力机制的头数量</li><li>超参数 -- hidden_size，决定了输入的embedding特征长度</li></ul><p><strong>hidden_size</strong>和<strong>num_heads</strong>共同决定了多头注意力机制每一头的特征长度。 <span class="math display">\[attention\_head\_size = \frac{hidden\_size}{num\_heads}\]</span> 一般需要确保上述公式是整除的。</p><ul><li>超参数 -- attention_dropout_rate，决定了attention_score和output的dropout比率 <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, vis</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.vis = vis</span><br><span class="line">        self.num_attention_heads = config.transformer[<span class="string">"num_heads"</span>]</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / self.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.out = Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.attn_dropout = Dropout(config.transformer[<span class="string">"attention_dropout_rate"</span>])</span><br><span class="line">        self.proj_dropout = Dropout(config.transformer[<span class="string">"attention_dropout_rate"</span>])</span><br><span class="line"></span><br><span class="line">        self.softmax = Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x size: [b, seq, all_head_len], new_x_shape: [b, seq, num_heads, head_len]</span></span><br><span class="line">        new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        <span class="comment"># reshape: [b, seq, all_head_len] ==&gt; [b, seq, num_heads, head_len]</span></span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="comment"># x: [b, seq, num_heads, head_len] ==&gt; [b, hum_heads, seq, head_len]</span></span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        <span class="comment"># input: [b, seq, emb_len] ==&gt; [b, seq, all_head_len]</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">        mixed_key_layer = self.key(hidden_states)</span><br><span class="line">        mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose_for_scores: 切分为多头</span></span><br><span class="line">        <span class="comment"># input: [b, seq, all_head_len] ==&gt; [b, num_heads, seq, head_len]</span></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, head_len] * [b, num_heads, head_len, seq] ==&gt; [b, num_heads, seq, seq]</span></span><br><span class="line">        <span class="comment"># 本质上是在num_heads个通道上，计算一个[seq, seq]维度的协方差矩阵，表征序列特征之间的相似度</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># scaling factor: sqrt(head_len)</span></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="comment"># 在最后维度softmax</span></span><br><span class="line">        attention_probs = self.softmax(attention_scores)</span><br><span class="line">        <span class="comment"># 返回可视化的attention_score [b, num_heads, seq, seq]</span></span><br><span class="line">        weights = attention_probs <span class="keyword">if</span> self.vis <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        attention_probs = self.attn_dropout(attention_probs)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, seq] * [b, num_heads, seq, head_len] ==&gt; [b, num_heads, seq, head_len]</span></span><br><span class="line">        <span class="comment"># 本质上是用序列相似度加权平均序列特征</span></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, head_len] ==&gt; [b, seq, num_heads, head_len]</span></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># [b, seq, num_heads, head_len] ==&gt; [b, seq, all_head_size]</span></span><br><span class="line">        <span class="comment"># 合并多头，形成attention特征</span></span><br><span class="line">        new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line">        <span class="comment"># 线性变换</span></span><br><span class="line">        attention_output = self.out(context_layer)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        attention_output = self.proj_dropout(attention_output)</span><br><span class="line">        <span class="keyword">return</span> attention_output, weights</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="mlp">Mlp</h3><p>该模块为两层全联接层，本质上是将Attention输出的特征映射到更高维度（第一层）之后再映射回统一维度（第二层）。</p><ul><li>超参数 -- hidden_state，定义了输入输出的特征维度</li><li>超参数 --</li></ul><h3 id="transformer">Transformer</h3><h2 id="模型训练">模型训练</h2><h3 id="apex">Apex</h3><p><a href="https://nvidia.github.io/apex/index.html">Apex</a>是NVIDIA开源的一个用于混合精度训练的库，可以通过数据并行化等方式加速训练速度，降低显存占用，提升模型精度。 #### 安装 </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/apex</span><br><span class="line"><span class="built_in">cd</span> apex</span><br><span class="line"><span class="comment"># if pip &gt;= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... </span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings <span class="string">"--build-option=--cpp_ext"</span> --config-settings <span class="string">"--build-option=--cuda_ext"</span> ./</span><br><span class="line"><span class="comment"># otherwise</span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=<span class="string">"--cpp_ext"</span> --global-option=<span class="string">"--cuda_ext"</span> ./</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="使用">使用</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br></pre></td></tr></tbody></table></figure><h3 id="mixed-precision">Mixed Precision</h3><h4 id="使用-1">使用</h4></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt;是Transformer模型应用到视觉领域的开山之作,在分类识别领域取得了SOTA表现，在&lt;a href=&quot;https://github.com/Alibaba-MIIL/ImageNet21K&quot;&gt;ImageNet-21K&lt;/a&gt;上训练的模型作为骨干基座，被广泛应用在了各个视觉任务中。本文主要介绍&lt;strong&gt;ViT&lt;/strong&gt;的模型结构，&lt;a href=&quot;https://github.com/jeonsworld/ViT-pytorch?tab=readme-ov-file&quot;&gt;pytorch代码实现&lt;/a&gt;和训练方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Vision-Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Training A Model Using Pytorch</title>
    <link href="http://blog.zhaoyongsheng.com/2024/07/01/Training-A-Model-Using-Pytorch/"/>
    <id>http://blog.zhaoyongsheng.com/2024/07/01/Training-A-Model-Using-Pytorch/</id>
    <published>2024-07-01T01:26:39.000Z</published>
    <updated>2024-07-01T01:26:39.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Pytorch</strong>是目前主流的深度学习框架，本文主要介绍使用<strong>Pytorch</strong>训练模型的主要流程，包括超参数定义、训练数据准备、配置GPU、加载模型、加在模型参数、训练日志打印、模型参数保存等步骤。 <span id="more"></span> ## 主流程 训练模型的主流程一般定义一个<strong>main</strong>函数作为函数执行的入口函数，具体如下所示： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="定义训练模型的超参数">定义训练模型的超参数</h3><p>通过<strong>argparser</strong>定义模型训练的超参数，具体如下所示： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"><span class="comment"># Required parameters</span></span><br><span class="line">parser.add_argument(<span class="string">"--name"</span>, required=<span class="literal">True</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Name of this run. Used for monitoring."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--dataset"</span>, choices=[<span class="string">"cifar10"</span>, <span class="string">"cifar100"</span>], default=<span class="string">"cifar10"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Which downstream task."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--model_type"</span>, choices=[<span class="string">"ViT-B_16"</span>, <span class="string">"ViT-B_32"</span>, <span class="string">"ViT-L_16"</span>,</span><br><span class="line">                                                <span class="string">"ViT-L_32"</span>, <span class="string">"ViT-H_14"</span>, <span class="string">"R50-ViT-B_16"</span>],</span><br><span class="line">                    default=<span class="string">"ViT-B_16"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Which variant to use."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pretrained_dir"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">"checkpoint/ViT-B_16.npz"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Where to search for pretrained ViT models."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--output_dir"</span>, default=<span class="string">"output"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"The output directory where checkpoints will be written."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--img_size"</span>, default=<span class="number">224</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Resolution size"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--train_batch_size"</span>, default=<span class="number">512</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total batch size for training."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--eval_batch_size"</span>, default=<span class="number">64</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total batch size for eval."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--eval_every"</span>, default=<span class="number">100</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Run prediction on validation set every so many steps."</span></span><br><span class="line">                            <span class="string">"Will always run one evaluation at the end of training."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--learning_rate"</span>, default=<span class="number">3e-2</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"The initial learning rate for SGD."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--weight_decay"</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Weight deay if we apply some."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--num_steps"</span>, default=<span class="number">10000</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total number of training epochs to perform."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--decay_type"</span>, choices=[<span class="string">"cosine"</span>, <span class="string">"linear"</span>], default=<span class="string">"cosine"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"How to decay the learning rate."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--warmup_steps"</span>, default=<span class="number">500</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Step of training to perform learning rate warmup for."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--max_grad_norm"</span>, default=<span class="number">1.0</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Max gradient norm."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--local_rank"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=-<span class="number">1</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"local_rank for distributed training on gpus"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--seed'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"random seed for initialization"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--gradient_accumulation_steps'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Number of updates steps to accumulate before performing a backward/update pass."</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--fp16'</span>, action=<span class="string">'store_true'</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Whether to use 16-bit float precision instead of 32-bit"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--fp16_opt_level'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">'O2'</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."</span></span><br><span class="line">                            <span class="string">"See details at https://nvidia.github.io/apex/amp.html"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--loss_scale'</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n"</span></span><br><span class="line">                            <span class="string">"0 (default value): dynamic loss scaling.\n"</span></span><br><span class="line">                            <span class="string">"Positive power of 2: static loss scaling value.\n"</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="配置训练模型的gpu显卡">配置训练模型的GPU显卡</h3><p>支持多卡和单卡训练，配置方法略有差异。</p><h4 id="单卡训练">单卡训练</h4><p>单卡训练，默认的local_rank=-1 </p><figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch<span class="selector-class">.device</span>(<span class="string">"cuda"</span> <span class="keyword">if</span> torch<span class="selector-class">.cuda</span><span class="selector-class">.is_available</span>() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">args<span class="selector-class">.n_gpu</span> = torch<span class="selector-class">.cuda</span><span class="selector-class">.device_count</span>()</span><br><span class="line">args<span class="selector-class">.device</span> = device</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="多卡训练">多卡训练</h4><p>多卡训练的实现方式有多种，推荐使用<strong>分布式数据并行（Distributed Data Parallel）</strong>方式，既可以用于单机多卡训练，又可以用于多机多卡训练。由多进程实现，可以从以下方面理解： 1. 从一开始就会启动多个进程(进程数等于GPU数)，每个进程独享一个GPU，每个进程都会独立地执行代码。这意味着每个进程都独立地初始化模型、训练，当然，在每次迭代过程中会通过进程间通信共享梯度，整合梯度，然后独立地更新参数。</p><ol start="2" type="1"><li><p>每个进程都会初始化一份训练数据集，当然它们会使用数据集中的不同记录做训练，这相当于同样的模型喂进去不同的数据做训练，也就是所谓的数据并行。这是通过torch.utils.data.distributed.DistributedSampler函数实现的，不过逻辑上也不难想到，只要做一下数据partition，不同进程拿到不同的partition就可以了。</p></li><li><p>进程通过local_rank变量来标识自己，local_rank为0的为master，其他是slave。local_rank表示的是当前的进程在当前节点的编号，因为我们设置了2个进程，因此进程的编号就是0和1。在使用启动命令时，torch.distributed.launch工具会默认地根据nproc_per_node传入local_rank参数。</p></li></ol><ul><li><p><a href="https://blog.csdn.net/Mr_health/article/details/122822483">pytorch多GPU训练的两种模式</a></p></li><li><p><a href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">pytorch-multi-gpu-training</a></p></li><li><p>通过<strong>argparser</strong>定义local_rank参数 </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pars<span class="number">*e</span>r.add_argument(<span class="string">"--local_rank"</span>, <span class="attribute">type</span>=int, <span class="attribute">default</span>=-1,</span><br><span class="line">                    <span class="attribute">help</span>=<span class="string">"local_rank for distributed training on gpus"</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>命令行注入local_rank参数 </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch <span class="attribute">--nproc_per_node</span>=4 <span class="attribute">--nnodes</span>=1 train.py</span><br></pre></td></tr></tbody></table></figure> 上述命令表示启动一个训练节点，每个节点4张GPU卡。<strong>torch.distributed.launch</strong>就以命令行参数的方式将args.local_rank变量注入到每个进程中，每个进程得到的变量值都不相同。比如使用 4 个GPU的话，则 4 个进程获得的args.local_rank值分别为0、1、2、3。<p></p></li><li><p>调用GPU显卡 </p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.set<span class="constructor">_device(<span class="params">args</span>.<span class="params">local_rank</span>)</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>, args.local_rank)</span><br><span class="line">torch.distributed.init<span class="constructor">_process_group(<span class="params">backend</span>='<span class="params">nccl</span>', <span class="params">timeout</span>=<span class="params">timedelta</span>(<span class="params">minutes</span>=60)</span>)</span><br><span class="line">args.n_gpu = <span class="number">1</span></span><br><span class="line">args.device = device</span><br></pre></td></tr></tbody></table></figure><p></p></li></ul><h3 id="设置随机数种子">设置随机数种子</h3><p>随机数影响了模型初始化，进而影响模型的训练效果，为了保证多次训练的结果一致性，通常将随机数种子设置为固定值。 </p><figure class="highlight maxima"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def set_seed(<span class="built_in">args</span>):</span><br><span class="line">    <span class="built_in">random</span>.seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    <span class="built_in">np</span>.<span class="built_in">random</span>.seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    torch.manual_seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">args</span>.n_gpu &gt; <span class="number">0</span>:</span><br><span class="line">        torch.cuda.manual_seed_all(<span class="built_in">args</span>.seed)</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="配置模型">配置模型</h3><p>同一个模型结构，一般会预留一些超参数，用于生成模型参数量的模型。</p><h4 id="配置模型参数">配置模型参数</h4><p>推荐使用<strong>ml_collections</strong>实现模型参数配置，具体如下所示： </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ml_collections</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_config</span>():</span><br><span class="line">    <span class="string">"""Returns a minimal configuration for testing."""</span></span><br><span class="line">    config = ml_collections.ConfigDict()</span><br><span class="line">    config.patches = ml_collections.ConfigDict({<span class="string">'size'</span>: (<span class="number">16</span>, <span class="number">16</span>)})</span><br><span class="line">    config.hidden_size = <span class="number">1</span></span><br><span class="line">    config.transformer = ml_collections.ConfigDict()</span><br><span class="line">    config.transformer.mlp_dim = <span class="number">1</span></span><br><span class="line">    config.transformer.num_heads = <span class="number">1</span></span><br><span class="line">    config.transformer.num_layers = <span class="number">1</span></span><br><span class="line">    config.transformer.attention_dropout_rate = <span class="number">0.0</span></span><br><span class="line">    config.transformer.dropout_rate = <span class="number">0.1</span></span><br><span class="line">    config.classifier = <span class="string">'token'</span></span><br><span class="line">    config.representation_size = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> config</span><br></pre></td></tr></tbody></table></figure><p></p><p>上述代码可以生成如下所示的配置。 </p><figure class="highlight nestedtext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">classifier</span><span class="punctuation">:</span> <span class="string">token</span></span><br><span class="line"><span class="attribute">hidden_size</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line"><span class="attribute">patches</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">size</span><span class="punctuation">:</span> <span class="string">!!python/tuple</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">16</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">16</span></span><br><span class="line"><span class="attribute">representation_size</span><span class="punctuation">:</span> <span class="string">null</span></span><br><span class="line"><span class="attribute">transformer</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">attention_dropout_rate</span><span class="punctuation">:</span> <span class="string">0.0</span></span><br><span class="line">  <span class="attribute">dropout_rate</span><span class="punctuation">:</span> <span class="string">0.1</span></span><br><span class="line">  <span class="attribute">mlp_dim</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line">  <span class="attribute">num_heads</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line">  <span class="attribute">num_layers</span><span class="punctuation">:</span> <span class="string">1</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>模型的多个配置，可以使用一个dict记录。 </p><figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CONFIGS = {</span><br><span class="line">    <span class="string">'ViT-B_16'</span>: configs<span class="selector-class">.get_b16_config</span>(),</span><br><span class="line">    <span class="string">'ViT-B_32'</span>: configs<span class="selector-class">.get_b32_config</span>(),</span><br><span class="line">    <span class="string">'ViT-L_16'</span>: configs<span class="selector-class">.get_l16_config</span>(),</span><br><span class="line">    <span class="string">'ViT-L_32'</span>: configs<span class="selector-class">.get_l32_config</span>(),</span><br><span class="line">    <span class="string">'ViT-H_14'</span>: configs<span class="selector-class">.get_h14_config</span>(),</span><br><span class="line">    <span class="string">'R50-ViT-B_16'</span>: configs<span class="selector-class">.get_r50_b16_config</span>(),</span><br><span class="line">    <span class="string">'testing'</span>: configs<span class="selector-class">.get_testing</span>(),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="生成并加载模型">生成并加载模型</h4><p>首先，将配置传入模型构造函数中，生成模型；其次，模型可以载入之前保存好的参数；最后，将模型加载到device中。 </p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="constructor">VisionTransformer(<span class="params">config</span>, <span class="params">args</span>.<span class="params">img_size</span>, <span class="params">zero_head</span>=True, <span class="params">num_classes</span>=<span class="params">num_classes</span>)</span></span><br><span class="line">model.load<span class="constructor">_from(<span class="params">np</span>.<span class="params">load</span>(<span class="params">args</span>.<span class="params">pretrained_dir</span>)</span>)</span><br><span class="line">model.<span class="keyword">to</span>(args.device)</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="统计模型参数量">统计模型参数量</h4><p>模型参数一般指的是有梯度的参数。 </p><figure class="highlight stan"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def count_parameters(<span class="title">model</span>):</span><br><span class="line">    params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> <span class="title">model</span>.<span class="title">parameters</span>() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">    <span class="keyword">return</span> params/<span class="number">1000000</span> <span class="comment"># in million（M）</span></span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="训练模型">训练模型</h3><p>模型训练一般由一个<strong>train</strong>函数实现。 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">args, model</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="保存训练过程">保存训练过程</h4><ul><li><p>通过<strong>SummaryWriter</strong>保存训练过程数据日志，并可以通过<strong>TensorBoard</strong>查看。 一般由单卡或者master卡保存训练日志，多卡训练的slave节点不保存日志。 </p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">in</span> <span class="literal">[-<span class="number">1</span>, <span class="number">0</span>]</span>:</span><br><span class="line">    os.makedirs(args.output_dir, exist_ok=True)</span><br><span class="line">    writer = <span class="constructor">SummaryWriter(<span class="params">log_dir</span>=<span class="params">os</span>.<span class="params">path</span>.<span class="params">join</span>(<span class="string">"logs"</span>, <span class="params">args</span>.<span class="params">name</span>)</span>)</span><br><span class="line"></span><br><span class="line"># train loop</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">in</span> <span class="literal">[-<span class="number">1</span>, <span class="number">0</span>]</span>:</span><br><span class="line">        writer.close<span class="literal">()</span></span><br></pre></td></tr></tbody></table></figure><p></p></li><li><p>在terminal中打印日志 推荐使用<strong>logging</strong>包实现，具体如下所示： </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import<span class="built_in"> logging</span></span><br><span class="line"><span class="built_in"></span>logger = logging.getLogger(__name__)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Total optimization steps = %d"</span>, args.num_steps)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Instantaneous batch size per GPU = %d"</span>, args.train_batch_size)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Total train batch size (w. parallel, distributed &amp; accumulation) = %d"</span>,</span><br><span class="line">                args.train_batch_size * args.gradient_accumulation_steps * (</span><br><span class="line">                    torch.distributed.get_world_size() <span class="keyword">if</span> args.local_rank != -1 <span class="keyword">else</span> 1))</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Gradient Accumulation steps = %d"</span>, args.gradient_accumulation_steps)</span><br></pre></td></tr></tbody></table></figure><p></p></li></ul><h4 id="准备数据集">准备数据集</h4><p>通过一个<strong>data_loader</strong>函数实现，返回<strong>train_loader</strong>和<strong>test_loader</strong>，具体参照！(数据集准备)[] </p><figure class="highlight autohotkey"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">train_loader,</span> test_loader = dat<span class="built_in">a_loader</span>(args)</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="准备优化器和训练计划">准备优化器和训练计划</h4><p>需要使用<strong>apex</strong>库。 </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apex import amp</span><br><span class="line"><span class="keyword">from</span> apex.parallel import DistributedDataParallel as DDP</span><br><span class="line"><span class="comment"># Prepare optimizer and scheduler</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),</span><br><span class="line">                            <span class="attribute">lr</span>=args.learning_rate,</span><br><span class="line">                            <span class="attribute">momentum</span>=0.9,</span><br><span class="line">                            <span class="attribute">weight_decay</span>=args.weight_decay)</span><br><span class="line">t_total = args.num_steps</span><br><span class="line"><span class="keyword">if</span> args.decay_type == <span class="string">"cosine"</span>:</span><br><span class="line">   <span class="built_in"> scheduler </span>= WarmupCosineSchedule(optimizer, <span class="attribute">warmup_steps</span>=args.warmup_steps, <span class="attribute">t_total</span>=t_total)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="built_in"> scheduler </span>= WarmupLinearSchedule(optimizer, <span class="attribute">warmup_steps</span>=args.warmup_steps, <span class="attribute">t_total</span>=t_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.fp16:</span><br><span class="line">    model, optimizer = amp.initialize(<span class="attribute">models</span>=model,</span><br><span class="line">                                        <span class="attribute">optimizers</span>=optimizer,</span><br><span class="line">                                        <span class="attribute">opt_level</span>=args.fp16_opt_level)</span><br><span class="line">    amp._amp_state.loss_scalers[0]._loss_scale = 2*<span class="number">*20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Distributed training</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank != -1:</span><br><span class="line">    model = DDP(model, <span class="attribute">message_size</span>=250000000, <span class="attribute">gradient_predivide_factor</span>=get_world_size())</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="开始训练">开始训练</h4><ul><li>valid 在训练过程中一般会每隔一定时间用测试集评估一下模型的训练结果，过程如下所示： <figure class="highlight scss"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">valid</span>(args, model, writer, test_loader, global_step):</span><br><span class="line">    # Validation!</span><br><span class="line">    eval_losses = <span class="built_in">AverageMeter</span>()</span><br><span class="line"></span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"***** Running Validation *****"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"  Num steps = %d"</span>, <span class="built_in">len</span>(test_loader))</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"  Batch size = %d"</span>, args.eval_batch_size)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    all_preds, all_label = [], []</span><br><span class="line">    epoch_iterator = <span class="built_in">tqdm</span>(test_loader,</span><br><span class="line">                          desc=<span class="string">"Validating... (loss=X.X)"</span>,</span><br><span class="line">                          bar_format=<span class="string">"{l_bar}{r_bar}"</span>,</span><br><span class="line">                          dynamic_ncols=True,</span><br><span class="line">                          disable=args.local_rank not in [-<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    loss_fct = torch.nn.<span class="built_in">CrossEntropyLoss</span>()</span><br><span class="line">    for step, batch in <span class="built_in">enumerate</span>(epoch_iterator):</span><br><span class="line">        batch = <span class="built_in">tuple</span>(t.<span class="built_in">to</span>(args.device) for t in batch)</span><br><span class="line">        x, y = batch</span><br><span class="line">        with torch.<span class="built_in">no_grad</span>():</span><br><span class="line">            logits = <span class="built_in">model</span>(x)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            eval_loss = <span class="built_in">loss_fct</span>(logits, y)</span><br><span class="line">            eval_losses.<span class="built_in">update</span>(eval_loss.<span class="built_in">item</span>())</span><br><span class="line"></span><br><span class="line">            preds = torch.<span class="built_in">argmax</span>(logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        if <span class="built_in">len</span>(all_preds) == <span class="number">0</span>:</span><br><span class="line">            all_preds.<span class="built_in">append</span>(preds.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>())</span><br><span class="line">            all_label.<span class="built_in">append</span>(y.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>())</span><br><span class="line">        else:</span><br><span class="line">            all_preds[<span class="number">0</span>] = np.<span class="built_in">append</span>(</span><br><span class="line">                all_preds[<span class="number">0</span>], preds.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>(), axis=<span class="number">0</span></span><br><span class="line">            )</span><br><span class="line">            all_label[<span class="number">0</span>] = np.<span class="built_in">append</span>(</span><br><span class="line">                all_label[<span class="number">0</span>], y.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>(), axis=<span class="number">0</span></span><br><span class="line">            )</span><br><span class="line">        epoch_iterator.<span class="built_in">set_description</span>(<span class="string">"Validating... (loss=%2.5f)"</span> % eval_losses.val)</span><br><span class="line"></span><br><span class="line">    all_preds, all_label = all_preds[<span class="number">0</span>], all_label[<span class="number">0</span>]</span><br><span class="line">    accuracy = <span class="built_in">simple_accuracy</span>(all_preds, all_label)</span><br><span class="line"></span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"\n"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Validation Results"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Global Steps: %d"</span> % global_step)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Valid Loss: %2.5f"</span> % eval_losses.avg)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Valid Accuracy: %2.5f"</span> % accuracy)</span><br><span class="line"></span><br><span class="line">    writer.<span class="built_in">add_scalar</span>(<span class="string">"test/accuracy"</span>, scalar_value=accuracy, global_step=global_step)</span><br><span class="line">    return accuracy</span><br><span class="line"></span><br><span class="line">def <span class="built_in">simple_accuracy</span>(preds, labels):</span><br><span class="line">    return (preds == labels).<span class="built_in">mean</span>()</span><br></pre></td></tr></tbody></table></figure></li><li>train 首先，在while循环开始训练之前，定义loss, global_step, best_acc等全局变量；然后开启while循环，开始训练；最后通过global_step和total_step结束训练。训练过程中需要用到<strong>tqdm</strong>库。 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">model.zero_grad()</span><br><span class="line">set_seed(args)  # Added here <span class="keyword">for</span> reproducibility (even between python 2 <span class="keyword">and</span> 3)</span><br><span class="line">losses = AverageMeter()</span><br><span class="line">global_step, best_acc = 0, 0</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_iterator = tqdm(train_loader,</span><br><span class="line">                            <span class="attribute">desc</span>=<span class="string">"Training (X / X Steps) (loss=X.X)"</span>,</span><br><span class="line">                            <span class="attribute">bar_format</span>=<span class="string">"{l_bar}{r_bar}"</span>,</span><br><span class="line">                            <span class="attribute">dynamic_ncols</span>=<span class="literal">True</span>,</span><br><span class="line">                            <span class="attribute">disable</span>=args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-1, 0])</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">step</span>, batch <span class="keyword">in</span> enumerate(epoch_iterator):</span><br><span class="line">        batch = tuple(t.<span class="keyword">to</span>(args.device) <span class="keyword">for</span> t <span class="keyword">in</span> batch)</span><br><span class="line">        x, y = batch</span><br><span class="line">        loss = model(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.gradient_accumulation_steps &gt; 1:</span><br><span class="line">            loss = loss / args.gradient_accumulation_steps</span><br><span class="line">        <span class="keyword">if</span> args.fp16:</span><br><span class="line">            with amp.scale_loss(loss, optimizer) as scaled_loss:</span><br><span class="line">                scaled_loss.backward()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">step</span> + 1) % args.gradient_accumulation_steps == 0:</span><br><span class="line">            losses.update(loss.item()<span class="number">*a</span>rgs.gradient_accumulation_steps)</span><br><span class="line">            <span class="keyword">if</span> args.fp16:</span><br><span class="line">                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)</span><br><span class="line">            scheduler.<span class="keyword">step</span>()</span><br><span class="line">            optimizer.<span class="keyword">step</span>()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            global_step += 1</span><br><span class="line"></span><br><span class="line">            epoch_iterator.set_description(</span><br><span class="line">                <span class="string">"Training (%d / %d Steps) (loss=%2.5f)"</span> % (global_step, t_total, losses.val)</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> args.local_rank <span class="keyword">in</span> [-1, 0]:</span><br><span class="line">                writer.add_scalar(<span class="string">"train/loss"</span>, <span class="attribute">scalar_value</span>=losses.val, <span class="attribute">global_step</span>=global_step)</span><br><span class="line">                writer.add_scalar(<span class="string">"train/lr"</span>, <span class="attribute">scalar_value</span>=scheduler.get_lr()[0], <span class="attribute">global_step</span>=global_step)</span><br><span class="line">            <span class="keyword">if</span> global_step % args.eval_every == 0 <span class="keyword">and</span> args.local_rank <span class="keyword">in</span> [-1, 0]:</span><br><span class="line">                accuracy = valid(args, model, writer, test_loader, global_step)</span><br><span class="line">                <span class="keyword">if</span> best_acc &lt; accuracy:</span><br><span class="line">                    save_model(args, model)</span><br><span class="line">                    best_acc = accuracy</span><br><span class="line">                model.train()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> global_step % t_total == 0:</span><br><span class="line">                break</span><br><span class="line">    losses.reset()</span><br><span class="line">    <span class="keyword">if</span> global_step % t_total == 0:</span><br><span class="line">        break</span><br></pre></td></tr></tbody></table></figure></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;是目前主流的深度学习框架，本文主要介绍使用&lt;strong&gt;Pytorch&lt;/strong&gt;训练模型的主要流程，包括超参数定义、训练数据准备、配置GPU、加载模型、加在模型参数、训练日志打印、模型参数保存等步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Using Ngnix To Deploy A File Share Service</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/28/Using-Ngnix-To-Deploy-A-File-Share-Service/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/28/Using-Ngnix-To-Deploy-A-File-Share-Service/</id>
    <published>2024-06-28T14:34:58.000Z</published>
    <updated>2024-06-28T14:34:58.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>使用Nginx，可以在服务器上搭建一个文件分享服务，通过网络可以下载指定文件夹下的文件。 <span id="more"></span> ### 安装 </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install nginx</span><br></pre></td></tr></tbody></table></figure> ### 配置服务 安装后nginx会自动运行，监听80端口，我们需要修改默认配置，指定端口和root目录，并开启autoindex。 <figure class="highlight gradle"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/etc/</span>nginx<span class="regexp">/sites-enabled/</span><span class="keyword">default</span></span><br></pre></td></tr></tbody></table></figure> <figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> {</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">30001</span> default_server;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">30001</span> default_server;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># SSL configuration</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># listen 443 ssl default_server;</span></span><br><span class="line">        <span class="comment"># listen [::]:443 ssl default_server;</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Note: You should disable gzip for SSL traffic.</span></span><br><span class="line">        <span class="comment"># See: https://bugs.debian.org/773332</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Read up on ssl_ciphers to ensure a secure configuration.</span></span><br><span class="line">        <span class="comment"># See: https://bugs.debian.org/765782</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Self signed certs generated by the ssl-cert package</span></span><br><span class="line">        <span class="comment"># Don't use them in a production server!</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># include snippets/snakeoil.conf;</span></span><br><span class="line"></span><br><span class="line">        <span class="attribute">root</span> /apprun/datasets/;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add index.php to the list if you are using PHP</span></span><br><span class="line">        <span class="attribute">index</span> index.html index.htm index.nginx-debian.html;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">server_name</span> _;</span><br><span class="line"></span><br><span class="line">        <span class="section">location</span> / {</span><br><span class="line">                <span class="comment"># First attempt to serve request as file, then</span></span><br><span class="line">                <span class="comment"># as directory, then fall back to displaying a 404.</span></span><br><span class="line">                <span class="comment"># try_files $uri $uri/ =404;</span></span><br><span class="line">                <span class="attribute">autoindex</span> <span class="literal">on</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass PHP scripts to FastCGI server</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#location ~ \.php$ {</span></span><br><span class="line">        <span class="comment">#       include snippets/fastcgi-php.conf;</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#       # With php-fpm (or other unix sockets):</span></span><br><span class="line">        <span class="comment">#       fastcgi_pass unix:/var/run/php/php7.4-fpm.sock;</span></span><br><span class="line">        <span class="comment">#       # With php-cgi (or other tcp sockets):</span></span><br><span class="line">        <span class="comment">#       fastcgi_pass 127.0.0.1:9000;</span></span><br><span class="line">        <span class="comment">#}</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># deny access to .htaccess files, if Apache's document root</span></span><br><span class="line">        <span class="comment"># concurs with nginx's one</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#location ~ /\.ht {</span></span><br><span class="line">        <span class="comment">#       deny all;</span></span><br><span class="line">        <span class="comment">#}</span></span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure> ### 重启服务 <figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo nginx -t</span></span><br><span class="line"><span class="attribute">sudo nginx -s reload</span></span><br></pre></td></tr></tbody></table></figure><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Nginx，可以在服务器上搭建一个文件分享服务，通过网络可以下载指定文件夹下的文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/"/>
    
      <category term="Nginx" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/Nginx/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/tags/Ubuntu/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Nginx" scheme="http://blog.zhaoyongsheng.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Using Baidu Yun Python Client In Ubuntu Server</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/28/Using-Baidu-Yun-Python-Client-In-Ubuntu-Server/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/28/Using-Baidu-Yun-Python-Client-In-Ubuntu-Server/</id>
    <published>2024-06-28T02:05:08.000Z</published>
    <updated>2024-06-28T02:05:08.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>百度云盘为Ubuntu Server提供了一个Python客户端<strong>pyby</strong>，可以在服务器上上传和下载百度云盘里面的文件。 <span id="more"></span> ### 安装 </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests bypy -i https:<span class="regexp">//</span>pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure> ### 授权 运行以下指令会生成一个用于授权的链接，复制链接在浏览器中打开并登陆，则会生成授权码，按照提示复制到terminal中即可。 <figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">python</span> -m bypy <span class="literal">info</span></span><br></pre></td></tr></tbody></table></figure> ### 使用 百度云盘会为<strong>bypy</strong>客户端单独生成一个文件夹，使用相关指令可以查看文件夹的信息、上传文件、下载文件等。 #### 查看百度云盘的空间 <figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bypy</span> info</span><br><span class="line"><span class="attribute">Quota</span>: <span class="number">5</span>.<span class="number">005</span>TB</span><br><span class="line"><span class="attribute">Used</span>: <span class="number">2</span>.<span class="number">416</span>GB</span><br></pre></td></tr></tbody></table></figure> #### 查看百度云盘的列表 <figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bypy list</span></span><br></pre></td></tr></tbody></table></figure> #### 上传文件 <figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bypy -v upload <span class="built_in">file</span>.path /remote_dir</span><br></pre></td></tr></tbody></table></figure> #### 其他指令 <figure class="highlight powershell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> bypy downdir k8s <span class="comment">#直接下载bypy目录下k8s</span></span><br><span class="line"><span class="variable">$</span> bypy syncup <span class="comment">#把当前目录同步到云盘</span></span><br><span class="line"><span class="variable">$</span> bypy syncdown <span class="comment">#把云盘内容同步到当前目录</span></span><br><span class="line"><span class="variable">$</span> bypy downdir / <span class="comment">#把云盘内容同步到当前目录</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$</span> bypy <span class="built_in">compare</span> <span class="comment">#比较本地当前目录和云盘（程序的）根目录</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-v</span> <span class="comment">#运行时添加-v参数，会显示进度详情。</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-d</span> <span class="comment">#运行时添加-d，会显示一些调试信息</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-ddd</span> <span class="comment">#显示更多http通讯信息</span></span><br></pre></td></tr></tbody></table></figure><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;百度云盘为Ubuntu Server提供了一个Python客户端&lt;strong&gt;pyby&lt;/strong&gt;，可以在服务器上上传和下载百度云盘里面的文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/tags/Ubuntu/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
  </entry>
  
  <entry>
    <title>Using Python Virtual Environment In Jupyter Notebook</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/</id>
    <published>2024-06-27T15:16:01.000Z</published>
    <updated>2024-06-27T15:16:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>介绍如何在<strong>Jupyter Notebook</strong>中使用Python的虚拟环境。 <span id="more"></span> ### 安装 </p><figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --<span class="keyword">user</span> <span class="title">ipykernel</span> -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure> ### 添加虚拟环境 创建python虚拟环境请参照<a href="https://youngsonzhao.github.io/2024/02/21/Using-Python-Virtual-Environment/">Using Python Virtual Environment</a> 创建好虚拟环境后，可以使用一下命令在Jupyter中注册虚拟环境。 <figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m ipykernel install --<span class="keyword">user</span> <span class="title">--name</span>=.sam</span><br></pre></td></tr></tbody></table></figure> ### 在Jupyter Notebook中使用虚拟环境 在<strong>Jupyter Notebook</strong>页面的左下角按钮，可以切换kernel，如下图所示： <img alt="切换kernel" data-src="/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/ipykernel.png"><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍如何在&lt;strong&gt;Jupyter Notebook&lt;/strong&gt;中使用Python的虚拟环境。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/"/>
    
      <category term="Jupyter Notebook" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/Jupyter-Notebook/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/tags/Python/"/>
    
      <category term="Jupyter Notebook" scheme="http://blog.zhaoyongsheng.com/tags/Jupyter-Notebook/"/>
    
      <category term="Venv" scheme="http://blog.zhaoyongsheng.com/tags/Venv/"/>
    
  </entry>
  
  <entry>
    <title>Using Python Virtual Environment</title>
    <link href="http://blog.zhaoyongsheng.com/2024/02/21/Using-Python-Virtual-Environment/"/>
    <id>http://blog.zhaoyongsheng.com/2024/02/21/Using-Python-Virtual-Environment/</id>
    <published>2024-02-21T14:18:48.000Z</published>
    <updated>2024-02-21T14:18:48.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>在Python中使用虚拟环境可以将包安装在独立的环境中，既不会污染主环境，也不会相互影响。因此在跑测试环境的时候，推荐使用虚拟环境。本文主要介绍虚拟环境的使用方法。 <span id="more"></span> ### 安装 使用以下指令安装虚拟环境： </p><figure class="highlight cmake"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> virtualenv</span><br><span class="line">sudo apt-get <span class="keyword">install</span> python3.<span class="number">10</span>-venv</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="使用">使用</h3><h4 id="初始化环境">初始化环境</h4><p>安装成功后，使用以下指令可以创建一个虚拟环境。 </p><figure class="highlight jboss-cli"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv <span class="string">.venv</span></span><br></pre></td></tr></tbody></table></figure> 所创建的虚拟环境目录为：<strong>.venv</strong> #### 激活环境 创建环境后，可以使用以下指令激活环境，让Python在虚拟环境中运行。 <figure class="highlight gradle"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> ~<span class="regexp">/.venv/</span>bin/active</span><br></pre></td></tr></tbody></table></figure> 激活后，我们可以在虚拟环境中正常运行python代码以及使用pip安装包。 #### 失活环境 <figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">deactive</span></span><br></pre></td></tr></tbody></table></figure><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Python中使用虚拟环境可以将包安装在独立的环境中，既不会污染主环境，也不会相互影响。因此在跑测试环境的时候，推荐使用虚拟环境。本文主要介绍虚拟环境的使用方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/tags/Python/"/>
    
      <category term="Venv" scheme="http://blog.zhaoyongsheng.com/tags/Venv/"/>
    
  </entry>
  
  <entry>
    <title>ROS2 Learning Notes - Moveit</title>
    <link href="http://blog.zhaoyongsheng.com/2024/02/03/ROS2-Learning-Notes-Moveit/"/>
    <id>http://blog.zhaoyongsheng.com/2024/02/03/ROS2-Learning-Notes-Moveit/</id>
    <published>2024-02-03T06:55:09.000Z</published>
    <updated>2024-02-03T06:55:09.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://moveit.ros.org/">MoveIt</a>是机器人运动规划与控制的重要开源包，本文主要讲解<strong>MoveIt</strong>的使用步骤和经验。 <span id="more"></span> ### Installation </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-joint-trajectory-controller</span><br><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-forward-command-controller</span><br><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-joint-state-broadcaster</span><br></pre></td></tr></tbody></table></figure><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://moveit.ros.org/&quot;&gt;MoveIt&lt;/a&gt;是机器人运动规划与控制的重要开源包，本文主要讲解&lt;strong&gt;MoveIt&lt;/strong&gt;的使用步骤和经验。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/ROS/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/tags/ROS/"/>
    
      <category term="Moveit" scheme="http://blog.zhaoyongsheng.com/tags/Moveit/"/>
    
  </entry>
  
  <entry>
    <title>Config Frp Client And Auto-Start As A Service</title>
    <link href="http://blog.zhaoyongsheng.com/2024/02/02/Config-Frp-Client-And-Auto-Start-As-A-Service/"/>
    <id>http://blog.zhaoyongsheng.com/2024/02/02/Config-Frp-Client-And-Auto-Start-As-A-Service/</id>
    <published>2024-02-02T05:23:37.000Z</published>
    <updated>2024-02-02T05:23:37.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Frp</strong>是一个搭建隧道实现内网穿透的常用工具，在之前的<a href="https://youngsonzhao.github.io/2023/02/15/Expose-Local-Devices-Behind-A-NAT-To-The-Internet-Using-FRP/">笔记</a>中我们已经讲了如何在服务器配置<strong>frps</strong>和在路由器配置<strong>frpc</strong>。今天主要记录一下如何在<strong>Ubuntu 22.04</strong>系统上配置<strong>frpc</strong>，实现<strong>ssh</strong>远程登陆访问。 <span id="more"></span> ### Download <a href="https://github.com/fatedier/frp">Frp</a>代码托管在github上，我们可以到<a href="https://github.com/fatedier/frp/releases">release</a>下载对应操作系统版本的最新包。 </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="regexp">//gi</span>thub.com<span class="regexp">/fatedier/</span>frp<span class="regexp">/releases/</span>download<span class="regexp">/v0.54.0/</span>frp_0.<span class="number">54.0</span>_linux_amd64.tar.gz</span><br></pre></td></tr></tbody></table></figure> ### Install 下载好的包是已经编译好的可执行文件，我们只需要将其放置在指定文件夹下即可. <figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mkdir</span> .frp</span><br><span class="line"><span class="attribute">cd</span> .frp</span><br><span class="line"><span class="attribute">tar</span> -xzf frp_0.<span class="number">54</span>.<span class="number">0</span>_linux_amd64.tar.gz</span><br><span class="line"><span class="attribute">cd</span> frp_0.<span class="number">54</span>.<span class="number">0</span>_linux_amd64</span><br></pre></td></tr></tbody></table></figure> ### Config 配置客户端需要创建一个<strong>frpc.ini</strong>的配置文件，在配置中首先配置<strong>frps</strong>的ip地址，端口号和token，然后配置本地的端口号和服务端的转发端口即可。 <figure class="highlight ini"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[common]</span></span><br><span class="line"><span class="attr">server_addr</span> = <span class="number">192.168</span>.<span class="number">1.1</span></span><br><span class="line"><span class="attr">server_port</span> = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for authentication</span></span><br><span class="line"><span class="attr">token</span> = <span class="number">12345678</span></span><br><span class="line"></span><br><span class="line"><span class="section">[ssh]</span></span><br><span class="line"><span class="attr">type</span> = tcp</span><br><span class="line"><span class="attr">local_ip</span> = <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line"><span class="attr">local_port</span> = <span class="number">22</span></span><br><span class="line"><span class="attr">remote_port</span> = <span class="number">2222</span></span><br></pre></td></tr></tbody></table></figure> 由于需要ssh登陆，所以本地端口为22，服务端端口为2222。 ### Service 将<strong>frpc</strong>设置为服务的形式，可以保证开启自启动。 首先进入到目录内。 <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/systemd/system</span><br><span class="line">sudo <span class="built_in">touch</span> frpc.service</span><br><span class="line">sudo gedit frpc.service</span><br></pre></td></tr></tbody></table></figure> 然后配置服务。 <figure class="highlight ini"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=My Frp Client Service - %i</span><br><span class="line"><span class="attr">After</span>=network.target syslog.target</span><br><span class="line"><span class="attr">Wants</span>=network.target</span><br><span class="line"></span><br><span class="line"><span class="section">[Service]</span></span><br><span class="line"><span class="attr">Type</span>=simple</span><br><span class="line"><span class="attr">Restart</span>=<span class="literal">on</span>-failure</span><br><span class="line"><span class="attr">RestartSec</span>=<span class="number">5</span>s</span><br><span class="line"><span class="attr">ExecStart</span>=/bin/bash -c <span class="string">'/home/robot/.frp/frp_0.54.0_linux_amd64/frpc -c /home/robot/.frp/frp_0.54.0_linux_amd64/frpc.ini'</span></span><br><span class="line"></span><br><span class="line"><span class="section">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=multi-user.target</span><br></pre></td></tr></tbody></table></figure> 最后运行服务。 <figure class="highlight nsis"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="params">system</span>ctl enable frpc.service</span><br><span class="line">sudo <span class="params">system</span>ctl start frpc.service</span><br><span class="line">sudo <span class="params">system</span>ctl status frpc.service</span><br></pre></td></tr></tbody></table></figure> ### SSH Ubuntu 22.04默认ssh连接是关闭的，需要打开端口，开启ssh连接。 首先明确指明ssh连接的端口为22。 <figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vi <span class="regexp">/etc/</span>ssh/ssh_config</span><br><span class="line">port <span class="number">22</span></span><br></pre></td></tr></tbody></table></figure> 然后开启22端口。 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install openssh-server</span><br><span class="line">ufw <span class="built_in">enable</span></span><br><span class="line">sudo ufw <span class="built_in">enable</span></span><br><span class="line">sudo ufw allow 22/tcp</span><br></pre></td></tr></tbody></table></figure> 上述配置完成后，在任意终端，通过ssh指令登陆即可。 <figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh</span> robot@<span class="number">192.168.1.2</span> -p <span class="number">2222</span></span><br></pre></td></tr></tbody></table></figure><p></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Frp&lt;/strong&gt;是一个搭建隧道实现内网穿透的常用工具，在之前的&lt;a href=&quot;https://youngsonzhao.github.io/2023/02/15/Expose-Local-Devices-Behind-A-NAT-To-The-Internet-Using-FRP/&quot;&gt;笔记&lt;/a&gt;中我们已经讲了如何在服务器配置&lt;strong&gt;frps&lt;/strong&gt;和在路由器配置&lt;strong&gt;frpc&lt;/strong&gt;。今天主要记录一下如何在&lt;strong&gt;Ubuntu 22.04&lt;/strong&gt;系统上配置&lt;strong&gt;frpc&lt;/strong&gt;，实现&lt;strong&gt;ssh&lt;/strong&gt;远程登陆访问。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/"/>
    
      <category term="Frp" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/Frp/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/tags/Ubuntu/"/>
    
      <category term="SSH" scheme="http://blog.zhaoyongsheng.com/tags/SSH/"/>
    
      <category term="Frp" scheme="http://blog.zhaoyongsheng.com/tags/Frp/"/>
    
  </entry>
  
  <entry>
    <title>ROS2 Learning Notes URDF &amp; Xacro</title>
    <link href="http://blog.zhaoyongsheng.com/2024/01/21/ROS2-Learning-Notes-URDF-Xacro/"/>
    <id>http://blog.zhaoyongsheng.com/2024/01/21/ROS2-Learning-Notes-URDF-Xacro/</id>
    <published>2024-01-21T10:21:12.000Z</published>
    <updated>2024-01-21T10:21:12.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>URDF</strong>是“<strong>Unified Robot Description Format</strong>”的缩写，在<strong>ROS</strong>中用于定义机器人，格式为<strong>xml</strong>。<a href="http://wiki.ros.org/xacro">xacro</a>是"<strong>XML Macros</strong>"的缩写，可以扩展<strong>xml</strong>语言，通过定义变量、函数，引入文件的形式提升代码的可读性。本文主要讲解<strong>urdf</strong>和<strong>xacro</strong>的基本用法。 <span id="more"></span> ## URDF ### URDF的基本结构 </p><figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">robot</span> <span class="attr">xmlns:xacro</span>=<span class="string">"http://www.ros.org/wiki/xacro"</span> <span class="attr">name</span>=<span class="string">"arduinobot"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">name</span>=<span class="string">"world"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">name</span>=<span class="string">"base_link"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">visual</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">filename</span>=<span class="string">"package://robot_description/meshes/basement.STL"</span> <span class="attr">scale</span>=<span class="string">"0.01 0.01 0.01"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"-0.5 -0.5 0"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">collision</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">filename</span>=<span class="string">"package://robot_description/meshes/basement.STL"</span> <span class="attr">scale</span>=<span class="string">"0.01 0.01 0.01"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"-0.5 -0.5 0"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">collision</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">xacro:default_inertial</span> <span class="attr">mass</span>=<span class="string">"1.0"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">"virtual_joint"</span> <span class="attr">type</span>=<span class="string">"fixed"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span> <span class="attr">link</span>=<span class="string">"world"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span> <span class="attr">link</span>=<span class="string">"base_link"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"0 0 0"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">joint</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">robot</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure> 开头的第一句声明<strong>xml</strong>文件的版本和编码格式。 最顶层的标签是<strong>robot</strong>，需要指明两个属性，一个是<strong>name</strong>，另外一个是<strong>xmlns:xacro</strong>。使用<strong>xacro</strong>，我们可以很方便地定义变量、函数，引入文件，提升xml文件的可读性和可扩展性。 里面的内容包含<strong>link</strong>，<strong>joint</strong>等模块，为机器人的具体定义。 ### Link <strong>Link</strong>定义基本的刚体结构，主要包括三部分内容： <strong>visual</strong>,<strong>collision</strong>和<strong>inertial</strong>三部分。 <figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">name</span>=<span class="string">"base_link"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">visual</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">collision</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">collision</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inertial</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inertial</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure> #### visual <strong>visual</strong>标签主要用于定义刚体的几何形状，用于在<strong>rviz2</strong>中可视化。几何形状可以是简单的几何体，例如<strong>box</strong>, <strong>sphere</strong>和<strong>cylinder</strong>，也可以是负责的几何形状，通过<strong>mesh</strong>标签，引入<strong>STL</strong>文件。 ##### box <figure class="highlight dust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">visual</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">box</span> <span class="attr">size</span>=<span class="string">"$</span></span></span><span class="template-variable">{arm_base_length}</span><span class="language-xml"><span class="tag"><span class="string"> $</span></span></span><span class="template-variable">{arm_base_width}</span><span class="language-xml"><span class="tag"><span class="string"> $</span></span></span><span class="template-variable">{arm_base_height}</span><span class="language-xml"><span class="tag"><span class="string">"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"0 0 $</span></span></span><span class="template-variable">{arm_base_height/2.0}</span><span class="language-xml"><span class="tag"><span class="string">"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">material</span> <span class="attr">name</span>=<span class="string">"orange"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure> ##### cylinder <figure class="highlight dust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">visual</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">cylinder</span> <span class="attr">radius</span>=<span class="string">"$</span></span></span><span class="template-variable">{forearm_radius}</span><span class="language-xml"><span class="tag"><span class="string">"</span> <span class="attr">length</span>=<span class="string">"$</span></span></span><span class="template-variable">{forearm_length}</span><span class="language-xml"><span class="tag"><span class="string">"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"0 0 $</span></span></span><span class="template-variable">{forearm_length/2.0}</span><span class="language-xml"><span class="tag"><span class="string">"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">material</span> <span class="attr">name</span>=<span class="string">"yellow"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure> ##### sphere <figure class="highlight dust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">visual</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">sphere</span> <span class="attr">radius</span>=<span class="string">"$</span></span></span><span class="template-variable">{wheel_radius/2.0}</span><span class="language-xml"><span class="tag"><span class="string">"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"0 0 0"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">material</span> <span class="attr">name</span>=<span class="string">"blue"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure> ##### mesh <figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">visual</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">geometry</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">filename</span>=<span class="string">"package://robot_description/meshes/basement.STL"</span> <span class="attr">scale</span>=<span class="string">"0.01 0.01 0.01"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">geometry</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">origin</span> <span class="attr">xyz</span>=<span class="string">"-0.5 -0.5 0"</span> <span class="attr">rpy</span>=<span class="string">"0 0 0"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">visual</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure> 可视化部分除了定义几何形状，还需要指明坐标系。默认的坐标系为几何中心，通过<strong>origin</strong>标签去定义坐标系与默认坐标系的关系，<strong>xyz</strong>指明两个坐标系之间的平移关系，<strong>rpy</strong>指明两个坐标系之间的旋转关系。 #### collision <strong>collision</strong>标签主要用于定义碰撞检测的范围，在<strong>gazebo</strong>仿真中使用。其定义内容一般与<strong>visual</strong>部分相同，也可以使用简单的几何体代替复杂形状，以提升碰撞检测的性能。 #### inertial <strong>inertial</strong>标签主要用于定义刚体的转动惯量，在<strong>gazebo</strong>仿真中用于运动建模。几何体的转动惯量由其质量分布和几何形状共同决定，计算复杂。对于质量分布均匀的简单几何体，其转动惯量有具体的公式表达。一般使用<strong>xacro:macro</strong>定义函数实现，具体可以参见后面章节。<p></p><h3 id="joint">Joint</h3><p>关节主要定义两个<strong>link</strong>之间的坐标系相对位置关系和相对运动关系。 </p><figure class="highlight abnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;joint name<span class="operator">=</span><span class="string">"joint_4"</span> type<span class="operator">=</span><span class="string">"revolute"</span>&gt;</span><br><span class="line">    &lt;parent link<span class="operator">=</span><span class="string">"claw_support"</span>/&gt;</span><br><span class="line">    &lt;child link<span class="operator">=</span><span class="string">"gripper_right"</span>/&gt;</span><br><span class="line">    &lt;origin xyz<span class="operator">=</span><span class="string">"-0.04 0.13 -0.1"</span> rpy<span class="operator">=</span><span class="string">"0 0 0"</span>/&gt;</span><br><span class="line">    &lt;axis xyz<span class="operator">=</span><span class="string">"0 0 1"</span>/&gt;</span><br><span class="line">    &lt;limit lower<span class="operator">=</span><span class="string">"-${pi/2}"</span> upper<span class="operator">=</span><span class="string">"0"</span> effort<span class="operator">=</span><span class="string">"${effort}"</span> velocity<span class="operator">=</span><span class="string">"${velocity}"</span>/&gt;</span><br><span class="line">&lt;/joint&gt;</span><br></pre></td></tr></tbody></table></figure> 其中<strong>parent</strong>和<strong>child</strong>分别指明了两个<strong>link</strong>，子link要在父link中运动。<strong>type</strong>指明了运动关系，包括固定式的<strong>fixed</strong>，关节旋转式的<strong>revolute</strong>等等。 #### fixed<p></p><h4 id="revolute">revolute</h4><h2 id="xacro">Xacro</h2><h3 id="定义变量">定义变量</h3><figure class="highlight dust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">xacro:property</span> <span class="attr">name</span>=<span class="string">"effort"</span> <span class="attr">value</span>=<span class="string">"30.0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">xacro:property</span> <span class="attr">name</span>=<span class="string">"velocity"</span> <span class="attr">value</span>=<span class="string">"10.0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">xacro:macro</span> <span class="attr">name</span>=<span class="string">"default_inertial"</span> <span class="attr">params</span>=<span class="string">"mass"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">inertial</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">mass</span> <span class="attr">value</span>=<span class="string">"$</span></span></span><span class="template-variable">{mass}</span><span class="language-xml"><span class="tag"><span class="string">"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">inertia</span> <span class="attr">ixx</span>=<span class="string">"1.0"</span> <span class="attr">ixy</span>=<span class="string">"0.0"</span> <span class="attr">ixz</span>=<span class="string">"0.0"</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml">                    <span class="attr">iyy</span>=<span class="string">"1.0"</span> <span class="attr">iyz</span>=<span class="string">"0.0"</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml">                    <span class="attr">izz</span>=<span class="string">"1.0"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">inertial</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">xacro:macro</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">xacro:macro</span> <span class="attr">name</span>=<span class="string">"default_transmission"</span> <span class="attr">params</span>=<span class="string">"number"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">transmission</span> <span class="attr">name</span>=<span class="string">"transmission_$</span></span></span><span class="template-variable">{number}</span><span class="language-xml"><span class="tag"><span class="string">"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span>transmission_interface/SimpleTransmission<span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">actuator</span> <span class="attr">name</span>=<span class="string">"motor_$</span></span></span><span class="template-variable">{number}</span><span class="language-xml"><span class="tag"><span class="string">"</span> <span class="attr">role</span>=<span class="string">"actuator1"</span>/&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">joint</span> <span class="attr">name</span>=<span class="string">"joint_$</span></span></span><span class="template-variable">{number}</span><span class="language-xml"><span class="tag"><span class="string">"</span> <span class="attr">role</span>=<span class="string">"joint1"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">            <span class="tag">&lt;<span class="name">mechanical_reduction</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">mechanical_reduction</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;/<span class="name">joint</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">transmission</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">xacro:macro</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure><h3 id="定义函数">定义函数</h3><figure class="highlight clean"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">### 引入文件</span><br></pre></td></tr></tbody></table></figure><p><xacro:include filename="$(find robot_description)/urdf/robot_gazebo.xacro"> <xacro:include filename="$(find robot_description)/urdf/robot_ros2_control.xacro"> ``` <link name="base_plate"> <visual> <geometry> <mesh filename="package://robot_description/meshes/base_plate.STL" scale="0.01 0.01 0.01"> </mesh></geometry> <origin rpy="0 0 0" xyz="-0.39 -0.39 -0.56"> </origin></visual> <collision> <geometry> <mesh filename="package://robot_description/meshes/base_plate.STL" scale="0.01 0.01 0.01"> </mesh></geometry> <origin rpy="0 0 0" xyz="-0.39 -0.39 -0.56"> </origin></collision> <xacro:default_inertial mass="1.0">  <joint name="joint_1" type="revolute"> <parent link="base_link"> <child link="base_plate"> <axis xyz="0 0 1"> <origin rpy="0 0 0" xyz="0 0 0.307"> <limit lower="-${pi/2}" upper="${pi/2}" effort="${effort}" velocity="${velocity}"> </limit></origin></axis></child></parent></joint></xacro:default_inertial></xacro:include></xacro:include></p><pre><code>&lt;link name="forward_drive_arm"&gt;    &lt;visual&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/forward_drive_arm.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="0 -${pi/2} ${pi/2}" xyz="0.19 0.06 -0.08"/&gt;    &lt;/visual&gt;    &lt;collision&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/forward_drive_arm.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="0 -${pi/2} ${pi/2}" xyz="0.19 0.06 -0.08"/&gt;    &lt;/collision&gt;    &lt;xacro:default_inertial mass="1.0"/&gt;&lt;/link&gt;&lt;joint name="joint_2" type="revolute"&gt;    &lt;parent link="base_plate"/&gt;    &lt;child link="forward_drive_arm"/&gt;    &lt;axis xyz="1 0 0"/&gt;    &lt;origin rpy="0 0 0" xyz="-0.02 0 0.35"/&gt;    &lt;limit lower="-${pi/2}" upper="${pi/2}" effort="${effort}" velocity="${velocity}"/&gt;&lt;/joint&gt;&lt;link name="horizontal_arm"&gt;    &lt;visual&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/horizontal_arm.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="${pi/2} 0 ${pi/2}" xyz="-0.03 -0.4 -0.06"/&gt;    &lt;/visual&gt;    &lt;collision&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/horizontal_arm.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="${pi/2} 0 ${pi/2}" xyz="-0.03 -0.4 -0.06"/&gt;    &lt;/collision&gt;    &lt;xacro:default_inertial mass="1.0"/&gt;&lt;/link&gt;&lt;joint name="joint_3" type="revolute"&gt;    &lt;parent link="forward_drive_arm"/&gt;    &lt;child link="horizontal_arm"/&gt;    &lt;axis xyz="1 0 0"/&gt;    &lt;origin rpy="0 0 0" xyz="0 0 0.8"/&gt;    &lt;limit lower="-${pi/2}" upper="${pi/2}" effort="${effort}" velocity="${velocity}"/&gt;&lt;/joint&gt;&lt;link name="claw_support"&gt;    &lt;visual&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/claw_support.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="0 0 ${pi/2}" xyz="0 -0.05 -0.15"/&gt;    &lt;/visual&gt;    &lt;collsion&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/claw_support.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin rpy="0 0 ${pi/2}" xyz="0 -0.05 -0.15"/&gt;    &lt;/collsion&gt;    &lt;xacro:default_inertial mass="0.05"/&gt;&lt;/link&gt;&lt;joint name="horizontal_arm_to_claw_support" type="fixed"&gt;    &lt;parent link="horizontal_arm"/&gt;    &lt;child link="claw_support"/&gt;    &lt;origin rpy="0 0 0" xyz="0 0.82 0"/&gt;&lt;/joint&gt;&lt;link name="gripper_right"&gt;    &lt;visual&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/right_finger.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin xyz="-0.1 0.5 -0.1" rpy="0 0 -${pi/2}"/&gt;    &lt;/visual&gt;    &lt;collision&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/right_finger.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin xyz="-0.1 0.5 -0.1" rpy="0 0 -${pi/2}"/&gt;    &lt;/collision&gt;    &lt;xacro:default_inertial mass="0.01"/&gt;&lt;/link&gt;&lt;link name="gripper_left"&gt;    &lt;visual&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/left_finger.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin xyz="-0.04 0.5 -0.1" rpy="0 0 -${pi/2}"/&gt;    &lt;/visual&gt;    &lt;collision&gt;        &lt;geometry&gt;            &lt;mesh filename="package://robot_description/meshes/left_finger.STL" scale="0.01 0.01 0.01"/&gt;        &lt;/geometry&gt;        &lt;origin xyz="-0.04 0.5 -0.1" rpy="0 0 -${pi/2}"/&gt;    &lt;/collision&gt;    &lt;xacro:default_inertial mass="0.01"/&gt;&lt;/link&gt;&lt;joint name="joint_5" type="revolute"&gt;    &lt;parent link="claw_support"/&gt;    &lt;child link="gripper_left"/&gt;    &lt;origin xyz="-0.22 0.13 -0.1" rpy="0 0 0"/&gt;    &lt;axis xyz="0 0 1"/&gt;    &lt;mimic joint="joint_4" multiplier="-1"/&gt;    &lt;limit lower="0" upper="${pi/2}" effort="${effort}" velocity="${velocity}"/&gt;&lt;/joint&gt;&lt;xacro:default_transmission number="1"/&gt;&lt;xacro:default_transmission number="2"/&gt;&lt;xacro:default_transmission number="3"/&gt;&lt;xacro:default_transmission number="4"/&gt;</code></pre></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;URDF&lt;/strong&gt;是“&lt;strong&gt;Unified Robot Description Format&lt;/strong&gt;”的缩写，在&lt;strong&gt;ROS&lt;/strong&gt;中用于定义机器人，格式为&lt;strong&gt;xml&lt;/strong&gt;。&lt;a href=&quot;http://wiki.ros.org/xacro&quot;&gt;xacro&lt;/a&gt;是&quot;&lt;strong&gt;XML Macros&lt;/strong&gt;&quot;的缩写，可以扩展&lt;strong&gt;xml&lt;/strong&gt;语言，通过定义变量、函数，引入文件的形式提升代码的可读性。本文主要讲解&lt;strong&gt;urdf&lt;/strong&gt;和&lt;strong&gt;xacro&lt;/strong&gt;的基本用法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/ROS/"/>
    
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/tags/ROS/"/>
    
      <category term="URDF" scheme="http://blog.zhaoyongsheng.com/tags/URDF/"/>
    
      <category term="Xacro" scheme="http://blog.zhaoyongsheng.com/tags/Xacro/"/>
    
  </entry>
  
  <entry>
    <title>ROS2 Learning Notes Advanced Concepts</title>
    <link href="http://blog.zhaoyongsheng.com/2024/01/14/ROS2-Learning-Notes-Advanced-Concepts/"/>
    <id>http://blog.zhaoyongsheng.com/2024/01/14/ROS2-Learning-Notes-Advanced-Concepts/</id>
    <published>2024-01-14T08:48:44.000Z</published>
    <updated>2024-01-14T08:48:44.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要记录<strong>ROS2</strong>的高级概念，包括<strong>ROS2</strong>的核心概念，例如<strong>Action</strong>, <strong>Lifecycle Nodes</strong>, <strong>Executors</strong>和<strong>Components</strong>。 <span id="more"></span> ## 环境 </p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Ubuntu</span> <span class="number">22</span>.<span class="number">04</span></span><br><span class="line"><span class="attribute">ROS2</span> Humble</span><br></pre></td></tr></tbody></table></figure> ## Actions <strong>Actions</strong>服务于执行时间较长的人物，同样以<strong>client/server</strong>的模式运行，执行过程中可以取消，也可以发送反馈信息。 通常是由三个<strong>Services</strong>和两个<strong>Topic</strong>组成。 * Services * Send Goal * Cancel Goal Request * Request Result * Topics * Feedback * Goal status <strong>Actions</strong>可以有多个<strong>Clients</strong>，同一个<strong>Client</strong>也可发送多个<strong>Goal</strong>。 ### 定义Action #### Server <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="keyword">from</span> unittest <span class="keyword">import</span> result</span><br><span class="line"><span class="keyword">import</span> rclpy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</span><br><span class="line"><span class="keyword">from</span> rclpy.action <span class="keyword">import</span> ActionServer</span><br><span class="line"><span class="keyword">from</span> rclpy.action.server <span class="keyword">import</span> ServerGoalHandle</span><br><span class="line"><span class="keyword">from</span> my_robot_interfaces.action <span class="keyword">import</span> CountUntil</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CountUntilServerNode</span>(<span class="title class_ inherited__">Node</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(<span class="string">"count_until_server"</span>)</span><br><span class="line">        self.count_until_server_ = ActionServer(self, CountUntil, <span class="string">"count_until"</span>, execute_callback=self.execute_callback)</span><br><span class="line">        self.get_logger().info(<span class="string">"Action server has been started."</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">execute_callback</span>(<span class="params">self, goal_handle: ServerGoalHandle</span>):</span><br><span class="line">        <span class="comment"># Get request from goal</span></span><br><span class="line">        target_number = goal_handle.request.target_number</span><br><span class="line">        period = goal_handle.request.period</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Execute the action</span></span><br><span class="line">        self.get_logger().info(<span class="string">"Executing the goal"</span>)</span><br><span class="line">        counter = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(target_number):</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line">            self.get_logger().info(<span class="string">"counter = "</span> + <span class="built_in">str</span>(counter))</span><br><span class="line">            time.sleep(period)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Once done, set goal final state</span></span><br><span class="line">        goal_handle.succeed()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># and send the result</span></span><br><span class="line">        result = CountUntil.Result()</span><br><span class="line">        result.reached_number = counter</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args=<span class="literal">None</span></span>):</span><br><span class="line">    rclpy.init(args=args)</span><br><span class="line">    node = CountUntilServerNode()</span><br><span class="line">    rclpy.spin(node)</span><br><span class="line">    rclpy.shutdown()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure> 使用<strong>ActionServer</strong>创建server, 指定类型、名称和回调函数。类型定义了Action数据传输的数据类型，名称为唯一的辨识符，回调函数则定义了server收到goal之后的处理逻辑。 回调函数的处理逻辑包含三部分，1. 处理goal中的request参数，2. 执行action， 3. 返回执行结果。 #### Client <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="keyword">from</span> cgitb <span class="keyword">import</span> reset</span><br><span class="line"><span class="keyword">import</span> rclpy</span><br><span class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</span><br><span class="line"><span class="keyword">from</span> rclpy.action <span class="keyword">import</span> ActionClient</span><br><span class="line"><span class="keyword">from</span> rclpy.action.server <span class="keyword">import</span> ServerGoalHandle</span><br><span class="line"><span class="keyword">from</span> rclpy.action.client <span class="keyword">import</span> ClientGoalHandle</span><br><span class="line"><span class="keyword">from</span> my_robot_interfaces.action <span class="keyword">import</span> CountUntil</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CountUntilClientNode</span>(<span class="title class_ inherited__">Node</span>): <span class="comment"># MODIFY NAME</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(<span class="string">"count_until_client"</span>) <span class="comment"># MODIFY NAME</span></span><br><span class="line">        self.count_until_client_ = ActionClient(self, CountUntil, <span class="string">"count_until"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">send_goal</span>(<span class="params">self, target_number, period</span>):</span><br><span class="line">        <span class="comment"># Wait for the server</span></span><br><span class="line">        self.count_until_client_.wait_for_server()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create a goal</span></span><br><span class="line">        goal = CountUntil.Goal()</span><br><span class="line">        goal.target_number = target_number</span><br><span class="line">        goal.period = period</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the goal</span></span><br><span class="line">        self.get_logger().info(<span class="string">"Sending goal"</span>)</span><br><span class="line">        future = self.count_until_client_.send_goal_async(goal=goal)</span><br><span class="line">        future.add_done_callback(self.goal_response_callback)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">goal_response_callback</span>(<span class="params">self, future</span>):</span><br><span class="line">        self.goal_handle_: ClientGoalHandle = future.result()</span><br><span class="line">        <span class="keyword">if</span> self.goal_handle_.accepted:</span><br><span class="line">                self.goal_handle_.get_result_async().add_done_callback(self.goal_result_callback)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">goal_result_callback</span>(<span class="params">self, future</span>):</span><br><span class="line">         result = future.result().result</span><br><span class="line">         self.get_logger().info(<span class="string">"Result: "</span> + <span class="built_in">str</span>(result.reached_number))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args=<span class="literal">None</span></span>):</span><br><span class="line">    rclpy.init(args=args)</span><br><span class="line">    node = CountUntilClientNode()</span><br><span class="line">    node.send_goal(<span class="number">6</span>, <span class="number">1.0</span>)</span><br><span class="line">    rclpy.spin(node)</span><br><span class="line">    rclpy.shutdown()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure> 使用<strong>ActionClient</strong>定义client, 主要包括类型，名称。发送goal的步骤是等待server就位，然后创建goal，赋值request参数，然后异步发送goal。异步发送接口会返回future对象，用来表明是未来才能执行完的。可是通过<strong>add_done_callback</strong>添加回调函数。回调函数有两层，第一层是server接收到goal之后accept/reject结果回调，第二层是accept之后，action执行结果的回调。<p></p><h4 id="acceptreject">Accept/Reject</h4><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.count_until_server_ = <span class="constructor">ActionServer(<span class="params">self</span>, CountUntil, <span class="string">"count_until"</span>, <span class="params">goal_callback</span>=<span class="params">self</span>.<span class="params">goal_callback</span>, <span class="params">execute_callback</span>=<span class="params">self</span>.<span class="params">execute_callback</span>)</span></span><br><span class="line">def goal<span class="constructor">_callback(<span class="params">self</span>, <span class="params">goal_request</span>: CountUntil.Goal)</span>:</span><br><span class="line">    self.get<span class="constructor">_logger()</span>.info(<span class="string">"Received a goal"</span>)</span><br><span class="line">    # Validate the goal request</span><br><span class="line">    <span class="keyword">if</span> goal_request.target_number &lt;= <span class="number">0</span>:</span><br><span class="line">        self.get<span class="constructor">_logger()</span>.info(<span class="string">"Rejecting the goal"</span>)</span><br><span class="line">        return GoalResponse.REJECT</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.get<span class="constructor">_logger()</span>.info(<span class="string">"Accepting the goal"</span>)</span><br><span class="line">        return GoalResponse.ACCEPT</span><br></pre></td></tr></tbody></table></figure><p>在定义server的时候，添加<strong>goal_callback</strong>来根据request参数处理accept和reject参数的情况。</p><h4 id="goal-state-machine">Goal State Machine</h4><p>状态机是针对goal的，而不是针对client和server的。 ##### Goal * Accepted * Executing * Succeed * Aborted * Cancelled * Succeed * Aborted * Cancelled * Rejected</p><h4 id="set-goal-result">Set Goal Result</h4><ul><li>Server <figure class="highlight nix"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Once done, set goal final state</span></span><br><span class="line"><span class="comment"># goal_handle.succeed()</span></span><br><span class="line">goal_handle.<span class="built_in">abort</span>()</span><br></pre></td></tr></tbody></table></figure></li><li>Client <figure class="highlight scss"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">goal_result_callback</span>(self, future):</span><br><span class="line">    status = future.<span class="built_in">result</span>().status</span><br><span class="line">    result = future.<span class="built_in">result</span>().result</span><br><span class="line">    if status == GoalStatus.STATUS_SUCCEEDED:</span><br><span class="line">        self.<span class="built_in">get_logger</span>().<span class="built_in">info</span>(<span class="string">"Goal succeeded."</span>)</span><br><span class="line">    elif status == GoalStatus.STATUS_ABORTED:</span><br><span class="line">        self.<span class="built_in">get_logger</span>().<span class="built_in">info</span>(<span class="string">"Goal aborted."</span>)</span><br><span class="line">    self.<span class="built_in">get_logger</span>().<span class="built_in">info</span>(<span class="string">"Result: "</span> + <span class="built_in">str</span>(result.reached_number))</span><br></pre></td></tr></tbody></table></figure> #### Send Goal Feedback</li><li>Server <figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">execute_callback</span>(<span class="params">self, goal_handle: ServerGoalHandle</span>):</span><br><span class="line">    <span class="comment"># Get request from goal</span></span><br><span class="line">    target_number = goal_handle.request.target_number</span><br><span class="line">    period = goal_handle.request.period</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Execute the action</span></span><br><span class="line">    self.get_logger().info(<span class="string">"Executing the goal"</span>)</span><br><span class="line">    feedback = CountUntil.Feedback()</span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(target_number):</span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        self.get_logger().info(<span class="string">"counter = "</span> + <span class="built_in">str</span>(counter))</span><br><span class="line">        feedback.current_number = counter</span><br><span class="line">        goal_handle.publish_feedback(feedback=feedback)</span><br><span class="line">        time.sleep(period)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Once done, set goal final state</span></span><br><span class="line">    <span class="comment"># goal_handle.succeed()</span></span><br><span class="line">    goal_handle.abort()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># and send the result</span></span><br><span class="line">    result = CountUntil.Result()</span><br><span class="line">    result.reached_number = counter</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure></li><li>Client <figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">future = self.count_until_client_.send<span class="constructor">_goal_async(<span class="params">goal</span>=<span class="params">goal</span>, <span class="params">feedback_callback</span>=<span class="params">self</span>.<span class="params">goal_feedback_callback</span>)</span></span><br><span class="line">def goal<span class="constructor">_feedback_callback(<span class="params">self</span>, <span class="params">feedback_msg</span>)</span>:</span><br><span class="line">    number = feedback_msg.feedback.current_number</span><br><span class="line">    self.get<span class="constructor">_logger()</span>.info(<span class="string">"Got feedback: "</span> + str(number))</span><br></pre></td></tr></tbody></table></figure></li></ul><h4 id="cancel-goal">Cancel Goal</h4><ul><li>Server <figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">self.count_until_server_ = <span class="constructor">ActionServer(<span class="params">self</span>, CountUntil, <span class="string">"count_until"</span>, <span class="params">goal_callback</span>=<span class="params">self</span>.<span class="params">goal_callback</span>, <span class="params">cancel_callback</span>=<span class="params">self</span>.<span class="params">cancel_callback</span>, <span class="params">execute_callback</span>=<span class="params">self</span>.<span class="params">execute_callback</span>, <span class="params">callback_group</span>=ReentrantCallbackGroup()</span>)</span><br><span class="line"></span><br><span class="line">def cancel<span class="constructor">_callback(<span class="params">self</span>, <span class="params">goal_handle</span>: ServerGoalHandle)</span>:</span><br><span class="line">    self.get<span class="constructor">_logger()</span>.info(<span class="string">"Received a cancel request."</span>)</span><br><span class="line">    return CancelResponse.ACCEPT # <span class="keyword">or</span> REJECT</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(target_number):</span><br><span class="line"><span class="keyword">if</span> goal_handle.is_cancel_requested:</span><br><span class="line">    self.get<span class="constructor">_logger()</span>.info(<span class="string">"Canceling the goal"</span>)</span><br><span class="line">    goal_handle.canceled<span class="literal">()</span></span><br><span class="line">    result.reached_number = counter</span><br><span class="line">    return result</span><br></pre></td></tr></tbody></table></figure></li><li>Client <figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.timer_ = <span class="variable language_">self</span>.create_timer(<span class="number">2.0</span>, <span class="variable language_">self</span>.cancel_goal)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cancel_goal</span>(<span class="params"><span class="variable language_">self</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.get_logger().info(<span class="string">"Send a cancel request"</span>)</span><br><span class="line">    <span class="variable language_">self</span>.goal_handle_.cancel_goal_async()</span><br><span class="line">    <span class="variable language_">self</span>.timer_.cancel()</span><br></pre></td></tr></tbody></table></figure></li><li>多线程</li></ul><h2 id="lifecycle-nodes">Lifecycle Nodes</h2><p>四种种状态： unconfigured, inactive, active, finalized 五种转换： on_configure, on_activate, on_deactivate, on_cleanup，on_shutdown 存在的意义： * 适用于硬件通信 * 重新配置更容易 * 按照顺序初始化 * 提前分配资源和内存 * 多节点之前同步初始化</p><p><strong>Lifecycle Node</strong>在<strong>nav2</strong>和<strong>ros2_control</strong>中经常使用。 ### 实现Lifecycle Node #### on_configure 起到初始化的作用 </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">on_configure</span>(<span class="params">self, previous_state: LifecycleState</span>):</span><br><span class="line">    self.get_logger().info(<span class="string">"In on_configure."</span>)</span><br><span class="line">    self.number_publisher_ = self.create_publisher(Int64, <span class="string">"number"</span>, <span class="number">10</span>)</span><br><span class="line">    self.number_timer_ = self.create_timer(</span><br><span class="line">        <span class="number">1.0</span> / self.publish_frequency_, self.publish_number)</span><br><span class="line">    self.get_logger().info(<span class="string">"Number publisher has been started."</span>)</span><br><span class="line">    <span class="keyword">return</span> TransitionCallbackReturn.SUCCESS</span><br></pre></td></tr></tbody></table></figure> #### on_cleanup 节点结束后，释放资源 <figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Destroy ROS2 communications , disconnect from HW</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_cleanup</span>(<span class="params"><span class="variable language_">self</span>, <span class="symbol">previous_state:</span> <span class="title class_">LifecycleState</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.get_logger().info(<span class="string">"In on_cleanup."</span>)</span><br><span class="line">    <span class="variable language_">self</span>.destroy_lifecycle_publisher(<span class="variable language_">self</span>.number_publisher_)</span><br><span class="line">    <span class="variable language_">self</span>.destroy_timer(<span class="variable language_">self</span>.number_timer_)</span><br><span class="line">    <span class="keyword">return</span> <span class="title class_">TransitionCallbackReturn</span>.<span class="variable constant_">SUCCESS</span></span><br></pre></td></tr></tbody></table></figure> #### on_activate <figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Activate/Enable HW</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_activate</span>(<span class="params"><span class="variable language_">self</span>, <span class="symbol">previous_state:</span> <span class="title class_">LifecycleState</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.get_logger().info(<span class="string">"In on_activate."</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">super</span>().on_activate(previous_state)</span><br></pre></td></tr></tbody></table></figure> #### on_deactivate <figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deactivate/Disable HW</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_deactivate</span>(<span class="params"><span class="variable language_">self</span>, <span class="symbol">previous_state:</span> <span class="title class_">LifecycleState</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.get_logger().info(<span class="string">"In on_deactivate."</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">super</span>().on_deactivate(previous_state)</span><br></pre></td></tr></tbody></table></figure> #### on_shutdown <figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def on<span class="constructor">_shutdown(<span class="params">self</span>, <span class="params">previous_state</span>: LifecycleState)</span>:</span><br><span class="line">    self.get<span class="constructor">_logger()</span>.info(<span class="string">"In on_shutdown."</span>)</span><br><span class="line">    self.destroy<span class="constructor">_lifecycle_publisher(<span class="params">self</span>.<span class="params">number_publisher_</span>)</span></span><br><span class="line">    self.destroy<span class="constructor">_timer(<span class="params">self</span>.<span class="params">number_timer_</span>)</span></span><br><span class="line">    return TransitionCallbackReturn.SUCCESS</span><br></pre></td></tr></tbody></table></figure> #### on_error <figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Process errors, deactivate + cleanup</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_error</span>(<span class="params"><span class="variable language_">self</span>, <span class="symbol">previous_state:</span> <span class="title class_">LifecycleState</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.get_logger().info(<span class="string">"In on_error."</span>)</span><br><span class="line">    <span class="variable language_">self</span>.destroy_lifecycle_publisher(<span class="variable language_">self</span>.number_publisher_)</span><br><span class="line">    <span class="variable language_">self</span>.destroy_timer(<span class="variable language_">self</span>.number_timer_)</span><br><span class="line">    <span class="comment"># do some checks, if ok then return SUCCESS, if not FAILURE</span></span><br><span class="line">    <span class="keyword">return</span> <span class="title class_">TransitionCallbackReturn</span>.<span class="variable constant_">FAILURE</span></span><br></pre></td></tr></tbody></table></figure> ### 使用Lifecycle Node 我们可是使用命令行控制<strong>Lifecycle Node</strong>的状态切换。 <figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ros2 lifecycle nodes <span class="comment">--查看所有lifecycle nodes</span></span><br><span class="line">ros2 lifecycle <span class="built_in">list</span> /number_publisher <span class="comment">--查看指定lifecycle node可以接受的命令</span></span><br><span class="line">ros2 lifecycle <span class="keyword">set</span> /number_publisher configure/<span class="built_in">activate</span>/deactivate/cleanup/shutdown <span class="comment">--设置指定lifecycle node的状态</span></span><br><span class="line">ros2 lifecycle <span class="keyword">get</span> /number_publisher <span class="comment">--查看指定lifecycle的状态</span></span><br></pre></td></tr></tbody></table></figure> <strong>Lifecycle Node</strong>的接口都是<strong>service</strong>，所以可以使用<strong>service client</strong>访问。<p></p><h2 id="executors">Executors</h2><p><strong>Executors</strong>用于阻塞程序，等待程序的执行,分为<strong>SingleThreadExecutor</strong>和<strong>MultiThreadExecutor</strong>。 <strong>rclpy.spin()</strong>默认调用的是<strong>SingleThreadExecutor</strong>。 <strong>MultiThreadExecutor</strong>需要提供<strong>callback_groups</strong>。 * ReentrantCallbackGroup 真正的多线程，可以重新进入，无需等待callback执行完成 * MutuallyExclusiveCallbackGroup 单线程，需要等待callback执行完成才能再进入</p><h2 id="components">Components</h2><p>在一个可执行文件中，可以启动多个节点，</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录&lt;strong&gt;ROS2&lt;/strong&gt;的高级概念，包括&lt;strong&gt;ROS2&lt;/strong&gt;的核心概念，例如&lt;strong&gt;Action&lt;/strong&gt;, &lt;strong&gt;Lifecycle Nodes&lt;/strong&gt;, &lt;strong&gt;Executors&lt;/strong&gt;和&lt;strong&gt;Components&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/ROS/"/>
    
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/tags/ROS/"/>
    
      <category term="Advanced Concepts" scheme="http://blog.zhaoyongsheng.com/tags/Advanced-Concepts/"/>
    
  </entry>
  
  <entry>
    <title>ROS2 Learning Notes Navigation 2 Stack</title>
    <link href="http://blog.zhaoyongsheng.com/2023/12/29/ROS2-Learning-Notes-Navigation-2-Stack/"/>
    <id>http://blog.zhaoyongsheng.com/2023/12/29/ROS2-Learning-Notes-Navigation-2-Stack/</id>
    <published>2023-12-29T08:49:48.000Z</published>
    <updated>2023-12-29T08:49:48.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要为ROS2导航功能模块的学习笔记。 <span id="more"></span> ### Navigation2 Stack * 安装工具包 </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-navigation2</span><br><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-nav2-bringup</span><br><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-turtlebot3</span><br></pre></td></tr></tbody></table></figure> * 修复依赖问题 <figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">wget</span> http://archive.ubuntu.com/ubuntu/pool/universe/l/lua5.<span class="number">2</span>/liblua5.<span class="number">2</span>-<span class="number">0</span>_5.<span class="number">2</span>.<span class="number">4</span>-<span class="number">2</span>_amd64.deb</span><br><span class="line"><span class="attribute">sudo</span> dpkg -i liblua5.<span class="number">2</span>-<span class="number">0</span>_5.<span class="number">2</span>.<span class="number">4</span>-<span class="number">2</span>_amd64.deb</span><br></pre></td></tr></tbody></table></figure> ### Generate a map with SLAM #### Make a Robot Move in the World * 配置环境变量 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vi ~/.zshrc</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">TURTLEBOT3_MODEL</span>=waffle</span><br></pre></td></tr></tbody></table></figure> * 启动gazebo环境 <figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="built_in">launch</span> turtlebot3_gazebo turtlebot3_world.<span class="built_in">launch</span>.py</span><br></pre></td></tr></tbody></table></figure> * 启动键盘控制 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="built_in">run</span> turtlebot3_teleop teleop_keyboard</span><br></pre></td></tr></tbody></table></figure> #### Create and Save a Map with SLAM 首先启动gazebo仿真环境，生成机器人。 然后启动<strong>cartographer</strong>。 <figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="built_in">launch</span> turtlebot3_cartographer cartographer.<span class="built_in">launch</span>.py use_sim_time:=True</span><br></pre></td></tr></tbody></table></figure> 启动键盘控制机器人在环境中移动就可以建图。 最后保存地图。 <figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="keyword">run</span><span class="language-bash"> nav2_map_server map_saver_cli -f maps/my_map</span></span><br></pre></td></tr></tbody></table></figure> 地图保存后可以使用<strong>gimp</strong>编辑保存。 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install gimp</span><br><span class="line">gimp</span><br></pre></td></tr></tbody></table></figure> ### Navigation in Map #### Fix Some Errors <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-rmw-cyclonedds-cpp</span><br><span class="line">sudo vi ~/.zshrc</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">RMW_IMPLEMENTATION</span>=rmw_cyclonedds_cpp</span><br></pre></td></tr></tbody></table></figure> <figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="regexp">/opt/</span>ros<span class="regexp">/humble/</span>share<span class="regexp">/turtlebot3_navigation2/</span>param</span><br><span class="line">sudo vi waffle.yaml</span><br><span class="line"><span class="comment"># robot_model_type: "differential"</span></span><br><span class="line">robot_model_type: <span class="string">"nav2_amcl::DifferentialMotionModel"</span></span><br></pre></td></tr></tbody></table></figure> #### Navigation 首先运行机器人的仿真环境，然后开启<strong>navigation</strong>节点 <figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="built_in">launch</span> turtlebot3_gazebo turtlebot3_world.<span class="built_in">launch</span>.py</span><br><span class="line">cd</span><br><span class="line">ros2 <span class="built_in">launch</span> turtlebot3_navigation2 navigation2.<span class="built_in">launch</span>.py use_sim_time:=True map:=maps/my_map.yaml</span><br></pre></td></tr></tbody></table></figure> 需要提供地图的配置文件，成功载入后使用<strong>2D Pose Estimation</strong>工具初始化机器人的位置。<p></p><h4 id="use-experiences">Use Experiences</h4><p>在<strong>rqt</strong>的插件里面可以配置参数。</p><p>Nav2架构</p><p>输入：TF, Map, Sensor Data 目标： global planner --&gt; local planner(controller) --&gt; hardware controller</p><p>Behavior Tree--&gt;决策树 #### Odometry 根据底盘的运动信息定位机器人。 随着时间和距离有累积误差，但是短时间内定位很稳定。 map frame -&gt; odom frame -&gt; base_link。 根据轮子的编码器计算，根据速度计算位置。也可以使用<strong>diff_drive_controller</strong>实现。 还可以使用<strong>robot_localization</strong>包自己实现更复杂的算法, 发布<strong>filtered topic</strong>。 Topic名称为：<strong>odom</strong>，数据类型为<strong>nav_msgs/Odometry.msg</strong>。</p><h4 id="sensor-data">Sensor Data</h4><ul><li>wheel encoder</li><li>lidar 发布<strong>sensor_msgs/msg/LaserScan</strong>消息，名字为<strong>scan</strong>。</li><li>camera 发布<strong>sensor_msgs/msg/Image</strong>消息，名字为<strong>camera/image_raw</strong>。</li></ul><h4 id="hardware-controller">Hardware Controller</h4><p>发布的消息类型为<strong>geometry_msgs/Twist</strong>。 既可以自己实现电机驱动器，在这样的情况下，需要自己实现<strong>Odometry</strong>。 也可以使用ros2自带的<strong>diff_drive_controller</strong>。</p><h4 id="slam_toolbox">slam_toolbox</h4><p>除了使用<strong>cartographer</strong>建图，我们还可以使用<strong>slam_toolbox</strong>建图。步骤如下所示： </p><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ros2 <span class="built_in">launch</span> turtlebot3_gazebo turtlebot3_world.<span class="built_in">launch</span>.py</span><br><span class="line">ros2 <span class="built_in">launch</span> nav2_bringup navigation_launch.py use_sim_time:=True</span><br><span class="line">ros2 <span class="built_in">launch</span> slam_toolbox online_async_launch.py use_sim_time:=True</span><br><span class="line">ros2 <span class="built_in">run</span> rviz2 rviz2</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="tf_transformations">tf_transformations</h4><ul><li>安装 <figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install ros-humble-tf-transformations</span><br><span class="line">sudo apt-<span class="built_in">get</span> install python3-transforms3d</span><br></pre></td></tr></tbody></table></figure></li></ul><h4 id="通过程序发送初始位置">通过程序发送初始位置</h4><p>使用<strong>nav2</strong>提供的命令接口实现。 </p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="attribute">import</span> rclpy</span><br><span class="line"><span class="attribute">from</span> nav2_simple_commander.robot_navigator import BasicNavigator</span><br><span class="line"><span class="attribute">from</span> geometry_msgs.msg import PoseStamped</span><br><span class="line"><span class="attribute">import</span> tf_transformations</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attribute">def</span> main():</span><br><span class="line">    <span class="comment"># init</span></span><br><span class="line">    <span class="attribute">rclpy</span>.init()</span><br><span class="line">    <span class="attribute">nav</span> = BasicNavigator()</span><br><span class="line">    <span class="comment"># set initial pose</span></span><br><span class="line">    <span class="attribute">q_x</span>, q_y, q_z, q_w = tf_transformations.quaternion_from_euler(<span class="number">0</span>.<span class="number">0</span>, <span class="number">0</span>.<span class="number">0</span>, <span class="number">0</span>.<span class="number">0</span>)</span><br><span class="line">    <span class="attribute">initial_pose</span> = PoseStamped()</span><br><span class="line">    <span class="attribute">initial_pose</span>.header.frame_id = 'map'</span><br><span class="line">    <span class="attribute">initial_pose</span>.header.stamp = nav.get_clock().now().to_msg()</span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.position.x = <span class="number">0</span>.<span class="number">0</span></span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.position.y = <span class="number">0</span>.<span class="number">0</span></span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.position.z = <span class="number">0</span>.<span class="number">0</span></span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.orientation.x = q_x</span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.orientation.y = q_y</span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.orientation.z = q_z</span><br><span class="line">    <span class="attribute">initial_pose</span>.pose.orientation.w = q_w</span><br><span class="line"></span><br><span class="line">    <span class="attribute">nav</span>.setInitialPose(initial_pose=initial_pose)</span><br><span class="line">    <span class="attribute">nav</span>.waitUntilNav2Active()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shutdown</span></span><br><span class="line">    <span class="attribute">rclpy</span>.shutdown()</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="通过程序发送目标位置">通过程序发送目标位置</h4><figure class="highlight gams"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line">import rclpy</span><br><span class="line">from nav2_simple_commander.robot_navigator import BasicNavigator</span><br><span class="line">from geometry_msgs.msg import PoseStamped</span><br><span class="line">import tf_transformations</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    # init</span><br><span class="line">    rclpy.init()</span><br><span class="line">    nav = BasicNavigator()</span><br><span class="line">    # <span class="keyword">set</span> initial <span class="comment">pose</span></span><br><span class="line">    q_x, q_y, q_z, q_w <span class="comment">= tf_transformations.quaternion_from_euler(0.0, 0.0, 0.0)</span></span><br><span class="line">    initial_pose <span class="comment">= PoseStamped()</span></span><br><span class="line">    initial_pose.header.frame_id <span class="comment">=</span> <span class="comment">'map'</span></span><br><span class="line">    initial_pose.header.stamp <span class="comment">= nav.get_clock().now().to_msg()</span></span><br><span class="line">    initial_pose.pose.position.x <span class="comment">= 0.0</span></span><br><span class="line">    initial_pose.pose.position.y <span class="comment">= 0.0</span></span><br><span class="line">    initial_pose.pose.position.z <span class="comment">= 0.0</span></span><br><span class="line">    initial_pose.pose.orientation.x <span class="comment">= q_x</span></span><br><span class="line">    initial_pose.pose.orientation.y <span class="comment">= q_y</span></span><br><span class="line">    initial_pose.pose.orientation.z <span class="comment">= q_z</span></span><br><span class="line">    initial_pose.pose.orientation.w <span class="comment">= q_w</span></span><br><span class="line"></span><br><span class="line">    nav.setInitialPose(initial_pose=initial_pose)</span><br><span class="line">    nav.waitUntilNav2Active()</span><br><span class="line"></span><br><span class="line">    # <span class="keyword">set</span> <span class="comment">goal pose</span></span><br><span class="line">    q_x, q_y, q_z, q_w <span class="comment">= tf_transformations.quaternion_from_euler(0.0, 0.0, 1.57)</span></span><br><span class="line">    goal_pose <span class="comment">= PoseStamped()</span></span><br><span class="line">    goal_pose.header.frame_id <span class="comment">=</span> <span class="comment">'map'</span></span><br><span class="line">    goal_pose.header.stamp <span class="comment">= nav.get_clock().now().to_msg()</span></span><br><span class="line">    goal_pose.pose.position.x <span class="comment">= 3.5</span></span><br><span class="line">    goal_pose.pose.position.y <span class="comment">= 1.0</span></span><br><span class="line">    goal_pose.pose.position.z <span class="comment">= 0.0</span></span><br><span class="line">    goal_pose.pose.orientation.x <span class="comment">= q_x</span></span><br><span class="line">    goal_pose.pose.orientation.y <span class="comment">= q_y</span></span><br><span class="line">    goal_pose.pose.orientation.z <span class="comment">= q_z</span></span><br><span class="line">    goal_pose.pose.orientation.w <span class="comment">= q_w</span></span><br><span class="line">    nav.goToPose(goal_pose)</span><br><span class="line"></span><br><span class="line">    while <span class="comment">not nav.isTaskComplete():</span></span><br><span class="line">        feedback <span class="comment">= nav.getFeedback()</span></span><br><span class="line">        # print(feedback)</span><br><span class="line">    </span><br><span class="line">    # shutdown</span><br><span class="line">    rclpy.shutdown()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if <span class="comment">__name__ ==</span> <span class="comment">'__main__'</span><span class="comment">:</span></span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><h4 id="waypoints发送一系列点">Waypoints发送一系列点</h4><p><embed src="nav2_test.py"> #### 继续学习 * ros2_control * robotic arm (moveit 2) * advanced ros2 core concepts</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要为ROS2导航功能模块的学习笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/ROS/"/>
    
    
      <category term="ROS" scheme="http://blog.zhaoyongsheng.com/tags/ROS/"/>
    
      <category term="Navigation" scheme="http://blog.zhaoyongsheng.com/tags/Navigation/"/>
    
  </entry>
  
</feed>
