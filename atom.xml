<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dr. Zhao&#39;s Blog</title>
  
  <subtitle>Focus on Scientific Research</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.zhaoyongsheng.com/"/>
  <updated>2025-05-03T13:18:28.000Z</updated>
  <id>http://blog.zhaoyongsheng.com/</id>
  
  <author>
    <name>Dr. Zhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Installation and Usage of Anaconda</title>
    <link href="http://blog.zhaoyongsheng.com/2025/05/03/Installation-and-Usage-of-Anaconda/"/>
    <id>http://blog.zhaoyongsheng.com/2025/05/03/Installation-and-Usage-of-Anaconda/</id>
    <published>2025-05-03T13:18:28.000Z</published>
    <updated>2025-05-03T13:18:28.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://www.anaconda.com/docs/getting-started/anaconda/install">Anaconda</a>是一个Python环境管理工具, 可以方便地管理Python环境, 安装和管理Python包, 以及创建虚拟环境等。本文将介绍Anaconda的安装和使用方法。与python虚拟环境相比，Anaconda可以方便的选择合适的python版本。</p><span id="more"></span><h2 id="安装-配置Anaconda"><a href="#安装-配置Anaconda" class="headerlink" title="安装&amp;配置Anaconda"></a>安装&amp;配置Anaconda</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>请到<a href="https://www.anaconda.com/docs/getting-started/anaconda/install">Anaconda官网</a>下载并安装最新版的Anaconda。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh</span><br><span class="line">bash ~/Anaconda3-2024.10-1-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>将以下配置添加到<code>~/.zshrc</code>文件中</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span><br><span class="line"># !! Contents within this block are managed by 'conda init' !!</span><br><span class="line">__conda_setup="$('/home/robot/anaconda3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)"</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval "$__conda_setup"</span><br><span class="line">else</span><br><span class="line">    if [ -f "/home/robot/anaconda3/etc/profile.d/conda.sh" ]; then</span><br><span class="line">        . "/home/robot/anaconda3/etc/profile.d/conda.sh"</span><br><span class="line">    else</span><br><span class="line">        export PATH="/home/robot/anaconda3/bin:$PATH"</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"># conda deactivate</span><br><span class="line"># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span><br></pre></td></tr></tbody></table></figure><h2 id="设置ZSH自动提醒"><a href="#设置ZSH自动提醒" class="headerlink" title="设置ZSH自动提醒"></a>设置ZSH自动提醒</h2><h3 id="下载插件"><a href="#下载插件" class="headerlink" title="下载插件"></a>下载插件</h3><p>将插件下载到<code>~/.oh-my-zsh/custom/plugins/</code>目录下。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/conda-incubator/conda-zsh-completion.git ~/.oh-my-zsh/custom/plugins/conda-zsh-completion</span><br></pre></td></tr></tbody></table></figure><h3 id="配置zshrc"><a href="#配置zshrc" class="headerlink" title="配置zshrc"></a>配置zshrc</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.zshrc</span><br><span class="line"># 在插件中添加conda-zsh-completion</span><br><span class="line">plugins=(git conda-zsh-completion)</span><br><span class="line"># 配置路径并自动加载插件</span><br><span class="line">fpath+=~/.oh-my-zsh/custom/plugins/conda-zsh-completion</span><br><span class="line">autoload -Uz compinit &amp;&amp; compinit</span><br><span class="line">source ~/.zshrc</span><br></pre></td></tr></tbody></table></figure><h2 id="关闭自动激活base环境"><a href="#关闭自动激活base环境" class="headerlink" title="关闭自动激活base环境"></a>关闭自动激活base环境</h2><h3 id="Terminal中关闭"><a href="#Terminal中关闭" class="headerlink" title="Terminal中关闭"></a>Terminal中关闭</h3><p>执行以下指令即可</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set auto_activate_base false</span><br></pre></td></tr></tbody></table></figure><h3 id="VSCode中关闭"><a href="#VSCode中关闭" class="headerlink" title="VSCode中关闭"></a>VSCode中关闭</h3><p>在 settings 中搜索 activateEnvironment，取消对其的勾选。<br><img data-src="/2025/05/03/Installation-and-Usage-of-Anaconda/activeEnvironment.png"></p><h2 id="使用Anaconda"><a href="#使用Anaconda" class="headerlink" title="使用Anaconda"></a>使用Anaconda</h2><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n myenv python=3.10</span><br></pre></td></tr></tbody></table></figure><h3 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a>激活虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate myenv</span><br></pre></td></tr></tbody></table></figure><h3 id="退出虚拟环境"><a href="#退出虚拟环境" class="headerlink" title="退出虚拟环境"></a>退出虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></tbody></table></figure><h3 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a>删除虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n myenv --all</span><br></pre></td></tr></tbody></table></figure><h3 id="查看所有虚拟环境"><a href="#查看所有虚拟环境" class="headerlink" title="查看所有虚拟环境"></a>查看所有虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></tbody></table></figure><h3 id="查看当前虚拟环境"><a href="#查看当前虚拟环境" class="headerlink" title="查看当前虚拟环境"></a>查看当前虚拟环境</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.anaconda.com/docs/getting-started/anaconda/install&quot;&gt;Anaconda&lt;/a&gt;是一个Python环境管理工具, 可以方便地管理Python环境, 安装和管理Python包, 以及创建虚拟环境等。本文将介绍Anaconda的安装和使用方法。与python虚拟环境相比，Anaconda可以方便的选择合适的python版本。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Anaconda" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Anaconda/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Anaconda" scheme="http://blog.zhaoyongsheng.com/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>Key Value Cache and Optimization Method for Transformer</title>
    <link href="http://blog.zhaoyongsheng.com/2025/02/10/Key-Value-Cache-and-Optimization-Method-for-Transformer/"/>
    <id>http://blog.zhaoyongsheng.com/2025/02/10/Key-Value-Cache-and-Optimization-Method-for-Transformer/</id>
    <published>2025-02-10T11:36:54.000Z</published>
    <updated>2025-02-10T11:36:54.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><span id="more"></span><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Transformers KV Caching Explained</a></li><li><a href="https://neptune.ai/blog/transformers-key-value-caching">Transformers Key-Value Caching Explained</a></li><li><a href=""></a></li><li><a href=""></a></li><li><a href=""></a></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      
      
        &lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;

&lt;h3 id=&quot;参考链接&quot;&gt;&lt;a href=&quot;#参考链接&quot; class=&quot;headerlink&quot; title=&quot;参考链接&quot;&gt;&lt;/a&gt;参考链接&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Train" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Train/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="Parallel Training" scheme="http://blog.zhaoyongsheng.com/tags/Parallel-Training/"/>
    
  </entry>
  
  <entry>
    <title>Parallel Training Scheme Of Large Model</title>
    <link href="http://blog.zhaoyongsheng.com/2025/02/10/Parallel-Training-Scheme-Of-Large-Model/"/>
    <id>http://blog.zhaoyongsheng.com/2025/02/10/Parallel-Training-Scheme-Of-Large-Model/</id>
    <published>2025-02-10T11:32:25.000Z</published>
    <updated>2025-02-10T11:32:25.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><span id="more"></span><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.high-flyer.cn/en/blog/hai-llm/#3d%E5%B9%B6%E8%A1%8C">HAI-LLM：高效且轻量的大模型训练工具</a></li><li><a href=""></a></li><li><a href=""></a></li><li><a href=""></a></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      
      
        &lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;

&lt;h3 id=&quot;参考链接&quot;&gt;&lt;a href=&quot;#参考链接&quot; class=&quot;headerlink&quot; title=&quot;参考链接&quot;&gt;&lt;/a&gt;参考链接&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Train" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Train/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="Parallel Training" scheme="http://blog.zhaoyongsheng.com/tags/Parallel-Training/"/>
    
  </entry>
  
  <entry>
    <title>FLOPs Analysis of Transformer Models</title>
    <link href="http://blog.zhaoyongsheng.com/2025/02/10/FLOPs-Analysis-of-Transformer-Models/"/>
    <id>http://blog.zhaoyongsheng.com/2025/02/10/FLOPs-Analysis-of-Transformer-Models/</id>
    <published>2025-02-10T11:12:51.000Z</published>
    <updated>2025-02-10T11:12:51.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><span id="more"></span><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://blog.eleuther.ai/transformer-math/">Transformer Math 101</a></li><li><a href="https://www.gaohongnan.com/playbook/training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li><li><a href="https://www.adamcasson.com/posts/transformer-flops">Transformer FLOPs</a></li><li><a href="https://github.com/HarleysZhang/llm_note/blob/main/1-transformer_model/llm%E5%8F%82%E6%95%B0%E9%87%8F-%E8%AE%A1%E7%AE%97%E9%87%8F-%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E5%88%86%E6%9E%90.md">llm参数量-计算量-显存占用分析</a></li><li><a href="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4">The FLOPs Calculus of Language Model Training</a></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      
      
        &lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;

&lt;h3 id=&quot;参考链接&quot;&gt;&lt;a href=&quot;#参考链接&quot; class=&quot;headerlink&quot; title=&quot;参考链接&quot;&gt;&lt;/a&gt;参考链接&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a hre
      
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Transformer/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="FLOPs" scheme="http://blog.zhaoyongsheng.com/tags/FLOPs/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes Of CLIP</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/18/Learning-Notes-Of-CLIP/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/18/Learning-Notes-Of-CLIP/</id>
    <published>2025-01-18T14:23:30.000Z</published>
    <updated>2025-01-18T14:23:30.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://github.com/openai/CLIP">CLIP</a> (Contrastive Language-Image Pre-Training) 是一个基于<strong>Transformer</strong>和<strong>Vision Transformer</strong>结构的语言-视觉预训练模型。<strong>CLIP</strong>模型通过对比学习训练了一个图像编码器和一个文本编码器，让模型自动学习（图片，文本）对的关联关系。给定一张图片，<strong>CLIP</strong>模型可以预测出来与该图片最相关的文字描述。由于<strong>CLIP</strong>模型的训练文本语料是由图片类目和命名实体和一些固定的语式短语(A photo of cat)组成，因此<strong>CLIP</strong>模型可以学习到图像特征与类目和命名实体之间的内在关系。<strong>CLIP</strong>模型不需要使用<strong>ImageNet</strong>数据集1.28M标签样本训练即可达到<strong>ResNet50</strong>的准确率。本文将介绍<strong>CLIP</strong>模型的结构，以及如何使用<strong>CLIP</strong>模型进行图像分类。</p><span id="more"></span><h2 id="CLIP模型结构"><a href="#CLIP模型结构" class="headerlink" title="CLIP模型结构"></a>CLIP模型结构</h2><p><img alt="CLIP模型结构" data-src="/2025/01/18/Learning-Notes-Of-CLIP/clip.png"><br><strong>CLIP</strong>模型由两个部分组成：<strong>Image Encoder</strong>和<strong>Text Encoder</strong>。<strong>Image Encoder</strong>是一个<strong>Vision Transformer</strong>，它将图片编码成$(embed_dim,)$维度的嵌入特征。<strong>Text Encoder</strong>是一个<strong>Transformer</strong>，它将文本编码成$(embed_dim,)$维度的嵌入特征。<strong>CLIP</strong>模型将<strong>Image Encoder</strong>和<strong>Text Encoder</strong>的输出进行对比学习，学习到图像和文本之间的关联关系。</p><p>在预测阶段，将图像标签组成固定语式的文本，分别提取图片编码特征和文本编码特征，然后计算两个嵌入特征的余弦相似度，取最大值作为预测结果。</p><h2 id="CLIP模型核心模块"><a href="#CLIP模型核心模块" class="headerlink" title="CLIP模型核心模块"></a>CLIP模型核心模块</h2><h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><h3 id="Text-Encoder"><a href="#Text-Encoder" class="headerlink" title="Text Encoder"></a>Text Encoder</h3><h3 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h3></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;CLIP&lt;/a&gt; (Contrastive Language-Image Pre-Training) 是一个基于&lt;strong&gt;Transformer&lt;/strong&gt;和&lt;strong&gt;Vision Transformer&lt;/strong&gt;结构的语言-视觉预训练模型。&lt;strong&gt;CLIP&lt;/strong&gt;模型通过对比学习训练了一个图像编码器和一个文本编码器，让模型自动学习（图片，文本）对的关联关系。给定一张图片，&lt;strong&gt;CLIP&lt;/strong&gt;模型可以预测出来与该图片最相关的文字描述。由于&lt;strong&gt;CLIP&lt;/strong&gt;模型的训练文本语料是由图片类目和命名实体和一些固定的语式短语(A photo of cat)组成，因此&lt;strong&gt;CLIP&lt;/strong&gt;模型可以学习到图像特征与类目和命名实体之间的内在关系。&lt;strong&gt;CLIP&lt;/strong&gt;模型不需要使用&lt;strong&gt;ImageNet&lt;/strong&gt;数据集1.28M标签样本训练即可达到&lt;strong&gt;ResNet50&lt;/strong&gt;的准确率。本文将介绍&lt;strong&gt;CLIP&lt;/strong&gt;模型的结构，以及如何使用&lt;strong&gt;CLIP&lt;/strong&gt;模型进行图像分类。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Transformer/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Transformer/Vision-Transformer/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="CLIP" scheme="http://blog.zhaoyongsheng.com/tags/CLIP/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes Of Segment Anything (SAM)</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/</id>
    <published>2025-01-05T12:01:01.000Z</published>
    <updated>2025-01-05T12:01:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>由<strong>Meta</strong>开发的<a href="https://github.com/facebookresearch/segment-anything?tab=readme-ov-file">Segment Anything (SAM)</a>是图像分割领域的SOTA模型,其核心思想是<strong>Prompt Tuning</strong>，通过<strong>Prompt Tuning</strong>，SAM在无需训练的情况下，可以完成各种图像分割任务，包括但不限于<strong>语义分割</strong>、<strong>实例分割</strong>、<strong>全景分割</strong>、<strong>抠图</strong>、<strong>图像编辑</strong>等。本文将重点介绍<strong>SAM</strong>的模型结构、算法原理、训练方法以及pytorch实现。</p><span id="more"></span><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img alt="Segment Anything (SAM)模型结构" data-src="/2025/01/05/Learning-Notes-Of-Segment-Anything-SAM/sam.png"><br>SAM的模型结构如上图所示，首先由<strong>ViT</strong>提取图像的Embedding特征，然后。</p><ul><li>flowchart</li></ul><pre class="mermaid">flowchart LR    Sam --&gt; ImageEncoderViT    Sam --&gt; PromptEncoder    Sam --&gt; MaskDecoder    ImageEncoderViT --&gt; PatchEmbed    ImageEncoderViT --&gt; PosEmbedding    ImageEncoderViT --&gt; Blocks    ImageEncoderViT --&gt; Neck</pre><p><strong>SAM</strong>模型由三个核心模块组成，分别是<strong>ImageEncoderViT</strong>、<strong>PromptEncoder</strong>和<strong>MaskDecoder</strong>。其中<strong>ImageEncoderViT</strong>是<strong>ViT</strong>模型，主要用于提取图像嵌入向量特征。<strong>PromptEncoder</strong>是<strong>Prompt Tuning</strong>模块，主要用于处理prompt特征，。<strong>MaskDecoder</strong>是<strong>Mask Decoder</strong>模块，主要输出分割结果。</p><h2 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h2><h3 id="ImageEncoderViT"><a href="#ImageEncoderViT" class="headerlink" title="ImageEncoderViT"></a>ImageEncoderViT</h3><p><strong>ImageEncoderViT</strong>是<strong>ViT</strong>模型，主要用于提取图像嵌入向量特征。相对于原生的<strong>ViT</strong>模型，<strong>ImageEncoderViT</strong>模块做了一些改进。首先去掉了分类用的class token，然后增加了序列特征融合的<strong>Neck</strong>模块，最后位置编码修改为了可配置的绝对和相对位置编码。</p><pre class="mermaid">flowchart LR    ImageEncoderViT --&gt; PatchEmbed    ImageEncoderViT --&gt; PosEmbedding    ImageEncoderViT --&gt; Blocks    ImageEncoderViT --&gt; Neck    PatchEmbed --&gt; nn.Conv2d    PatchEmbed --&gt; nn.permute    PosEmbedding --&gt; absolute_pos_embed    PosEmbedding --&gt; relative_pos_embed    Neck --&gt; nn.Conv2d    Neck --&gt; nn.LayerNorm</pre><h4 id="PatchEmbed"><a href="#PatchEmbed" class="headerlink" title="PatchEmbed"></a>PatchEmbed</h4><p><strong>PatchEmbed</strong>主要由<strong>nn.Conv2d</strong>和<strong>nn.permute</strong>组成，主要作用是把图像转换为<strong>patch</strong>，然后进行embedding。</p><h4 id="PosEmbedding"><a href="#PosEmbedding" class="headerlink" title="PosEmbedding"></a>PosEmbedding</h4><p><strong>PosEmbedding</strong>分为绝对值位置编码和相对位置编码，在模型初始化时由超参数控制。绝对值位置编码由可以学习的参数组成。</p><h4 id="Blocks"><a href="#Blocks" class="headerlink" title="Blocks"></a>Blocks</h4><p><strong>Blocks</strong>是<strong>ViT</strong>模型的主体部分，由多个<strong>Block</strong>组成，每个<strong>Block</strong>包含<strong>Attention</strong>和<strong>MLP</strong>两部分。<strong>Attention</strong>模块的数学操作与<strong>ViT</strong>论文中操作保持一致，只是输入模块的Tensor维度略有不同，原始论文中是(batch_size, seq_len, embed_dim)，而<strong>SAM</strong>模型中是(batch_size, height_patches, width_patches, embed_dim)。<strong>MLP</strong>模块的数学操作与<strong>ViT</strong>论文中操作保持一致。</p><ul><li>mlp_ratio表示缩放因子，即MLP的隐藏层维度与输入维度的比值，MLP模块的隐藏层维度不直接定义，而是由嵌入特征维度和该缩放因子共同决定。</li></ul><h5 id="window-partition"><a href="#window-partition" class="headerlink" title="window_partition"></a>window_partition</h5><p><strong>SAM</strong>模型在计算<strong>Attention</strong>时，除了引入多头注意力机制，在嵌入特征维度空间内进行分区处理，还引入了<strong>window_partition</strong>操作，在序列长度维度（实际上是在图像二维空间维度）进行分区处理。目标是在更小的二维空间尺度上进行多头注意力学习，用于学习局部特征而非全局特征。它的主要作用包括以下几点：</p><ul><li><ol><li><strong>分块处理图像</strong></li></ol><ul><li>窗口分区的目标是对patch序列进一步划分，让模型在局部patch上进行多头注意力学习，而非全部patch上进行学习。</li><li>这种分割方式使模型能够专注于局部特征的提取，同时保留空间上下文信息。</li></ul></li><li><ol start="2"><li><strong>加速平行计算</strong></li></ol><ul><li>通过窗口分区，不同分区的作为batch来处理，充分利用了GPU的并行计算特性，加速了模型训练。</li></ul></li><li><ol start="3"><li><strong>窗口内处理细节特征</strong></li></ol><ul><li>每个窗口内的像素或patch是模型分析的基础单元，可以使用不同的注意力机制来捕捉图像的<strong>局部特征</strong>。在Transformer中，每个窗口的特征会被编码后用于后续的自注意力计算。</li></ul></li><li><ol start="4"><li><strong>减少全局依赖，提升局部感知能力</strong></li></ol><ul><li>图像分块可以<strong>减少单次计算中需要考虑的上下文范围</strong>，使模型能够在不同的层次专注于局部和全局特征的结合，提高特征学习的层次化表达能力。</li></ul></li><li><p>代码实现</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x: torch.Tensor, window_size: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]]:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Partition into non-overlapping windows with padding if needed.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x (tensor): input tokens with [B, H, W, C].</span></span><br><span class="line"><span class="string">        window_size (int): window size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: windows after partition with [B * num_windows, window_size, window_size, C].</span></span><br><span class="line"><span class="string">        (Hp, Wp): padded height and width before partition</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line"></span><br><span class="line">    pad_h = (window_size - H % window_size) % window_size</span><br><span class="line">    pad_w = (window_size - W % window_size) % window_size</span><br><span class="line">    <span class="keyword">if</span> pad_h &gt; <span class="number">0</span> <span class="keyword">or</span> pad_w &gt; <span class="number">0</span>:</span><br><span class="line">        x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, pad_w, <span class="number">0</span>, pad_h))</span><br><span class="line">    Hp, Wp = H + pad_h, W + pad_w</span><br><span class="line"></span><br><span class="line">    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows, (Hp, Wp)</span><br></pre></td></tr></tbody></table></figure></li></ul><h4 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h4><p><strong>Neck</strong>主要由<strong>nn.Conv2d</strong>和<strong>nn.LayerNorm</strong>组成，主要作用是把<strong>Blocks</strong>提取的嵌入特征扩展映射到指定维度作为输出。<strong>SAM</strong>模型分别用了两层<strong>nn.Conv2d</strong>和<strong>nn.LayerNorm</strong>，用于提升模型的泛化能力。第一层<strong>nn.Conv2d</strong>的kernel_size为1,stride为1,可视为等效全链接层。第二层<strong>nn.Conv2d</strong>的kernel_size为3,stride为1,padding为1，可视为卷积层，将邻域特征做了融合处理，进一步提升了<strong>Neck</strong>模块的特征融合与表达能力。</p><h3 id="PromptEncoder"><a href="#PromptEncoder" class="headerlink" title="PromptEncoder"></a>PromptEncoder</h3><p>该模块的作用是将prompt生成编码，用于分割解码器的输入。prompt支持三种类型：</p><ul><li><ol><li>point</li></ol><ul><li>提供一个点坐标的list，每个点提供一个label，0表示负点（分割结果中不包含本点），1表示正点（分割结果中包含本点）。</li></ul></li><li><ol start="2"><li>box</li></ol><ul><li>提供一个box的list，每个box有左上角坐标和右下角坐标。</li></ul></li><li><ol start="3"><li>mask</li></ol><ul><li>提供一个图像模板。</li></ul></li></ul><p>首先，定义一个随机的不可训练的PositionalEmbedding，维度为（2, embed_dim）,用于表示不同二维图像空间的位置编码，一半维度用$sin$函数处理，另外一般维度用$cos$函数处理。<br>其次，定义4个可训练的PointEmbedding，分别是正向点、负向点，box的左上角点和box的右下角点。</p><p>对于每一个point，首先利用point的(x,y)坐标与PositionalEmbedding相乘，得到(1, embed_dim)的向量，然后根据point label情况与对应的PointEmbedding相加，得到(1, embed_dim)的向量，作为该point的prompt编码。</p><p>对于每一个box，拆成左上角和右下角两个point，采用point的处理方式，得到(2, embed_dim)的向量。</p><p>对于每一个mask，使用三个卷积层进行操作，提取mask编码。</p><p>值的注意的是，point和box的编码是稀疏编码，仅有位置信息。而mask得到的是稠密编码，还有mask对应物体的语义信息。</p><ul><li><p>sequence</p><pre class="mermaid">sequenceDiagram  Alice-&gt;&gt;John: Hello John, how are you?  John--&gt;&gt;Alice: Great!  Alice-)John: See you later!</pre></li><li><p>mindmap</p><pre class="mermaid">mindmapRoot  A    B    C</pre></li><li><p>流程图</p><pre class="mermaid">graph TD;  A[输入图像] --&gt; B[ViT提取Embedding特征];  B --&gt; C[Prompt Tuning];  C --&gt; D[语义分割];  C --&gt; E[实例分割];  C --&gt; F[全景分割];  C --&gt; G[抠图];  C --&gt; H[图像编辑];  D --&gt; I[输出分割结果];  E --&gt; I;  F --&gt; I;  G --&gt; I;  H --&gt; I;  I --&gt; J[结束];</pre></li><li><p>甘特图</p><pre class="mermaid">gantt  title A Gantt Diagram  dateFormat YYYY-MM-DD  section Section      A task          :a1, 2014-01-01, 30d      Another task    :after a1, 20d  section Another      Task in Another :2014-01-12, 12d      another task    :24d</pre></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由&lt;strong&gt;Meta&lt;/strong&gt;开发的&lt;a href=&quot;https://github.com/facebookresearch/segment-anything?tab=readme-ov-file&quot;&gt;Segment Anything (SAM)&lt;/a&gt;是图像分割领域的SOTA模型,其核心思想是&lt;strong&gt;Prompt Tuning&lt;/strong&gt;，通过&lt;strong&gt;Prompt Tuning&lt;/strong&gt;，SAM在无需训练的情况下，可以完成各种图像分割任务，包括但不限于&lt;strong&gt;语义分割&lt;/strong&gt;、&lt;strong&gt;实例分割&lt;/strong&gt;、&lt;strong&gt;全景分割&lt;/strong&gt;、&lt;strong&gt;抠图&lt;/strong&gt;、&lt;strong&gt;图像编辑&lt;/strong&gt;等。本文将重点介绍&lt;strong&gt;SAM&lt;/strong&gt;的模型结构、算法原理、训练方法以及pytorch实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Vision-Transformer/"/>
    
      <category term="Segment Anything" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Vision-Transformer/Segment-Anything/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Segment Anything" scheme="http://blog.zhaoyongsheng.com/tags/Segment-Anything/"/>
    
  </entry>
  
  <entry>
    <title>Masked Auto Encoder (MAE) For Vision Transformer</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/</id>
    <published>2025-01-04T06:26:43.000Z</published>
    <updated>2025-01-04T06:26:43.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked Auto Encoder (MAE)</a>成功将<strong>Vision Transformer</strong>应用到了图像领域的自监督预训练中，并在多个视觉任务中取得了SOTA的表现，是视觉自监督预训练的重要突破。本文将介绍<strong>MAE</strong>的原理和实现细节，将重点介绍<strong>MAE</strong>相对于<a href="https://youngsonzhao.github.io/2024/07/05/Learning-Note-Of-Vision-Transformer/">Vision Transformer</a>和<a href="https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/">Transformer</a>的创新点。</p><span id="more"></span><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img alt="MAE Vision Transformer模型架构图" data-src="/2025/01/04/Masked-Auto-Encoder-MAE-For-Vision-Transformer/mae.png"></p><p>相对于<strong>Vision Transformer</strong>，<strong>MAE</strong>的模型架构图如上所示，主要创新点如下：</p><ul><li><ol><li>将图像的patches按照随机概率mask掉，在encoder的输入中仅保留未被mask的patches，随机mask的概率是75%。</li></ol></li><li><ol start="2"><li>使用一个轻量级的decoder来处理encoder编码的未被mask的patches，和被mask的patches，来重建原始图像。</li></ol></li><li><ol start="3"><li>encoder和decoder的大小是非对称的，由此可以提升非监督预训练的效率和效果。</li></ol></li><li><ol start="4"><li>预训练结束后，decoder被丢弃，仅保留encoder，用于下游任务。</li></ol></li><li><ol start="5"><li>MAE中可以使用global pooling来代替class token（可选的，通过参数配置）</li></ol></li><li><ol start="6"><li>MAE中的position embedding是2D sin-cos embedding，而不是<strong>Vision Transformer</strong>中的learnable position embedding。</li></ol></li></ul><h2 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h2><h3 id="Random-Masking"><a href="#Random-Masking" class="headerlink" title="Random Masking"></a>Random Masking</h3><p><strong>MAE</strong>最核心的模块是随机掩码生成模块，可以以指定概率随机生成掩码。<br>具体步骤如下所示：</p><ul><li><ol><li>在batch和sequence维度上生成随机噪声，(batch_size, seq_len)</li></ol></li><li><ol start="2"><li>使用argsort函数对随机噪声在sequence维度进行升序排序，得到升序排序结果的索引矩阵ids_shuffle，该索引矩阵记录的是排名在此处的元素在原有随机噪声矩阵中的位置，(batch_size, seq_len)</li></ol></li><li><ol start="3"><li>继续使用argsort函数对索引矩阵进行升序排序，得到索引矩阵的索引矩阵ids_restore，该索引矩阵记录的是原有随机噪声矩阵的每个值得排名，(batch_size, seq_len)</li></ol></li><li><ol start="4"><li>根据mask_ratio参数，从ids_shuffle中保留前len_keep个元素，得到被保留的元素的索引矩阵ids_keep，(batch_size, len_keep)</li></ol></li><li><ol start="5"><li>使用gather函数，根据ids_keep的索引，从原始序列中生成masked的序列x_masked，(batch_size, len_keep, dim)</li></ol></li><li><ol start="6"><li>在sequence维度生成一个前len_keep个值是0，后seq_len-len_keep个值是1的mask矩阵，维度为(batch_size, seq_len)，然后使用gather函数和ids_restore索引，生成掩码矩阵mask，(batch_size, seq_len)，其中mask中值为0的元素表示被保留的元素，值为1的元素表示被mask的元素</li></ol></li><li><ol start="7"><li>返回被mask保留的序列x_masked，掩码矩阵mask，以及表明每个元素排名的ids_restore索引矩阵，其中</li></ol><ul><li>x_masked用于encoder编码器的输入</li><li>ids_restore在decoder中使用，用于将mask_token向量填充到被mask的序列Token中，融合encoder输出的序列Token特征，生成原始序列特征。</li><li>mask掩码矩阵在计算loss时使用，由于被mask掉的token对应的mask值是1,因此在计算loss时，仅计算被mask掉的token的loss.<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))</span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)</span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder就是使用ViT对x_masked进行编码，得到编码后的特征序列. 值的注意的是，在encoder中保留了class token。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder并不是将encoder输出的编码特征与输出序列进行cross attention，而是利用ids_restore索引矩阵将encoder输出的编码特征恢复至原有序列特征位置，被mask掉的序列特征统一用一个可学习的mask_token向量填充，然后进行decoder编码最终恢复原始图像。</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf&quot;&gt;Masked Auto Encoder (MAE)&lt;/a&gt;成功将&lt;strong&gt;Vision Transformer&lt;/strong&gt;应用到了图像领域的自监督预训练中，并在多个视觉任务中取得了SOTA的表现，是视觉自监督预训练的重要突破。本文将介绍&lt;strong&gt;MAE&lt;/strong&gt;的原理和实现细节，将重点介绍&lt;strong&gt;MAE&lt;/strong&gt;相对于&lt;a href=&quot;https://youngsonzhao.github.io/2024/07/05/Learning-Note-Of-Vision-Transformer/&quot;&gt;Vision Transformer&lt;/a&gt;和&lt;a href=&quot;https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/&quot;&gt;Transformer&lt;/a&gt;的创新点。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Vision-Transformer/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
      <category term="Masked Auto Encoder" scheme="http://blog.zhaoyongsheng.com/tags/Masked-Auto-Encoder/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes Of Pytorch</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/03/Learning-Notes-Of-Pytorch/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/03/Learning-Notes-Of-Pytorch/</id>
    <published>2025-01-03T14:13:14.000Z</published>
    <updated>2025-01-03T14:13:14.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Pytorch</strong>是开源的深度学习框架，由Facebook的Deep Learning Team开发，2016年发布。本文主要记录<strong>Pytorch</strong>学习过程中的一些知识点。</p><span id="more"></span><h2 id="Tensor维度操作"><a href="#Tensor维度操作" class="headerlink" title="Tensor维度操作"></a>Tensor维度操作</h2><h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.flatten(<span class="built_in">input</span>, start_dim=<span class="number">0</span>, end_dim=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p>将输入张量展平，从<code>start_dim</code>到<code>end_dim</code>的维度展平。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>start_dim：开始展平的维度，默认为 0。</li><li>end_dim：结束展平的维度，默认为 -1，表示最后一个维度。</li></ul><h3 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze"></a>unsqueeze</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim)</span><br></pre></td></tr></tbody></table></figure><p>在指定维度上增加一个维度，维度大小为1。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim：要增加的维度。</li></ul><h3 id="squeeze"><a href="#squeeze" class="headerlink" title="squeeze"></a>squeeze</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><p>删除张量中所有大小为1的维度。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim：要删除的维度，默认为 None，表示删除所有大小为1的维度。</li></ul><h3 id="transpose"><a href="#transpose" class="headerlink" title="transpose"></a>transpose</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1)</span><br></pre></td></tr></tbody></table></figure><p>交换张量的两个维度。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim0：要交换的第一个维度。</li><li>dim1：要交换的第二个维度。</li></ul><h3 id="permute"><a href="#permute" class="headerlink" title="permute"></a>permute</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.permute(<span class="built_in">input</span>, dims)</span><br></pre></td></tr></tbody></table></figure><p>重新排列张量的多个维度。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dims：新的维度顺序。</li></ul><h3 id="einsum"><a href="#einsum" class="headerlink" title="einsum"></a>einsum</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.einsum(<span class="string">'nhwpqc-&gt;nchpwq'</span>, x)</span><br></pre></td></tr></tbody></table></figure><p>通过爱因斯坦求和约定计算张量的各种操作。</p><h2 id="Tensor排序操作"><a href="#Tensor排序操作" class="headerlink" title="Tensor排序操作"></a>Tensor排序操作</h2><h3 id="argmax"><a href="#argmax" class="headerlink" title="argmax"></a>argmax</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>返回张量中指定维度上最大值的索引。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>keepdim：是否保持维度，默认为 False。</li></ul><h3 id="topk"><a href="#topk" class="headerlink" title="topk"></a>topk</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.topk(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>返回张量中指定维度上k个最大值的索引和值。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>k：返回的最大值的数量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>largest：是否返回最大值，默认为 True。</li></ul><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>对张量进行排序，返回排序后的张量和索引。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>descending：是否降序排序，默认为 False。</li></ul><h3 id="argsort"><a href="#argsort" class="headerlink" title="argsort"></a>argsort</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argsort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>对张量进行排序，返回排序后的索引。<br>参数说明：</p><ul><li>input：输入的张量。</li><li>dim：指定维度，默认为 None，表示所有维度。</li><li>descending：是否降序排序，默认为 False。</li></ul><h2 id="Tensor数学操作"><a href="#Tensor数学操作" class="headerlink" title="Tensor数学操作"></a>Tensor数学操作</h2><h3 id="register-buffer"><a href="#register-buffer" class="headerlink" title="register_buffer"></a>register_buffer</h3><p>在 PyTorch 中，register_buffer 是一个用于注册缓冲区（buffer）的方法。缓冲区是那些不需要进行梯度计算的张量，通常用于保存模型的状态信息或常量数据。与参数（parameters）不同，缓冲区不会被优化器更新，但它们会被保存和加载到模型的状态字典（state dict）中。</p><p>register_buffer 的作用:</p><ul><li>保存状态信息：缓冲区可以保存一些不参与训练但需要随模型一起保存和加载的数据，例如批量归一化层中的运行均值和方差、位置编码等。</li><li>不参与梯度计算：缓冲区中的张量不会被自动求导机制追踪，因此不会占用额外的显存来存储梯度信息。</li><li>自动管理设备：缓冲区会跟随模型移动到不同的设备（如 CPU 或 GPU），确保其始终与模型的其他部分保持一致。</li><li>包含在模型的状态字典中：当调用 model.state_dict() 时，缓冲区会被包含在返回的字典中，从而可以在保存和加载模型时一并处理。</li></ul><h3 id="convTranspose2d"><a href="#convTranspose2d" class="headerlink" title="convTranspose2d"></a>convTranspose2d</h3><p>转置卷积操作，也称为反卷积操作，用于将输入张量进行上采样，从而生成更大的输出张量。转置卷积操作与普通卷积操作相反，普通卷积操作是将输入张量进行下采样，从而生成更小的输出张量。<br>具体操作细节可以参照这篇文章<a href="https://numbersmithy.com/understanding-transposed-convolutions-in-pytorch/">Understanding transposed convolutions in PyTorch</a></p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><p><code>GELU</code>（Gaussian Error Linear Unit）是深度学习中常用的激活函数，它结合了<code>ReLU</code>和<code>tanh</code>函数的优势，并具有更好的梯度传播特性。<code>GELU</code>函数的数学表达式为：<br>$$<br>GELU(x) = x \cdot \Phi(x)<br>$$<br>其中，$\Phi(x)$ 是标准正态分布的累积分布函数（CDF）。<code>GELU</code>函数在深度学习模型中广泛使用，因为它可以有效地缓解梯度消失问题，并提高模型的性能。</p><p>GELU函数相对于RELU函数的优势再于，在x=0处，GELU函数的导数是1，而RELU函数的导数在x=0处是0，这会导致梯度消失问题。GELU函数在x=0处有平滑的过渡，使得梯度在x=0处不会消失，从而提高了模型的训练效果。</p><p>！<a href="gelu.png">GELU激活函数曲线图</a></p><h2 id="TorchVision"><a href="#TorchVision" class="headerlink" title="TorchVision"></a>TorchVision</h2><h3 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h3><p><code>torchvision.transforms</code> 是 PyTorch 提供的一个模块，用于对图像进行预处理和增强。它提供了一系列常用的图像变换操作，如裁剪、缩放、旋转、翻转、归一化等。这些变换操作可以用于数据增强，以提高模型的泛化能力。</p><p>以下是一些常用的 <code>torchvision.transforms</code> 操作：</p><ol><li><strong>裁剪（Crop）</strong>：从图像中裁剪出指定大小的区域。</li><li><strong>缩放（Resize）</strong>：将图像缩放到指定的大小。</li><li><strong>旋转（Rotate）</strong>：将图像旋转指定角度。</li><li><strong>翻转（Flip）</strong>：将图像左右翻转或上下翻转。</li><li><strong>正则化（Normalize）</strong>：将图像像素值归一化到指定范围。</li><li><strong>转换颜色空间（ColorJitter）</strong>：对图像的颜色进行随机调整，如亮度、对比度、饱和度和色调。</li><li><strong>转换通道顺序（ToTensor）</strong>：将图像从 PIL 或 NumPy 数组格式转换为 PyTorch 张量格式。</li><li><strong>添加噪声（AddNoise）</strong>：向图像中添加随机噪声。</li><li><strong>随机裁剪（RandomCrop）</strong>：从图像中随机裁剪出指定大小的区域。</li><li><strong>随机旋转（RandomRotation）</strong>：将图像随机旋转指定角度。</li></ol><p>通过<strong>Compose</strong> 操作可以将多个变换操作组合成一个变换操作。</p><h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><h3 id="PyTorch-JIT优化"><a href="#PyTorch-JIT优化" class="headerlink" title="PyTorch JIT优化"></a>PyTorch JIT优化</h3><p><code>PyTorch JIT</code>（Just-In-Time 编译）提供了针对模型加速和优化的功能，<code>TorchScript</code> 是其核心技术之一，用于将 PyTorch 动态计算图转化为静态图以实现更高效的执行。JIT 格式（<code>.pt</code> 或 <code>.jit</code> 文件）在保存和执行模型时具备以下优化：</p><ol><li><strong>图优化（Graph Optimization）</strong></li></ol><p>JIT 模型在编译和执行时会进行以下图级优化：</p><ul><li><strong>算子融合（Operator Fusion）</strong>：将多个算子（如卷积和 ReLU）合并为一个，以减少内存访问和计算开销。例如，将 <code>Conv2d</code> 和 <code>BatchNorm</code> 融合。</li><li><strong>常量折叠（Constant Folding）</strong>：将编译时已知的常量值计算出结果，从而减少运行时的计算。</li><li><strong>死代码消除（Dead Code Elimination）</strong>：删除未使用的计算和变量，以减小模型规模和提高运行速度。</li></ul><ol start="2"><li><strong>内存优化</strong></li></ol><ul><li><strong>内存分配优化</strong>：JIT 运行时管理内存使用，减少不必要的内存分配和释放操作，降低内存分配的开销。</li><li><strong>Tensor 内存复用</strong>：避免不必要的张量拷贝和中间结果的存储，提升内存利用效率。</li></ul><ol start="3"><li><strong>静态图执行（Static Graph Execution）</strong><br>TorchScript 将动态计算图编译为静态图，提供以下优势：</li></ol><ul><li><strong>消除 Python 解释器开销</strong>：TorchScript 代码运行在 C++ 后端而不是 Python 解释器，减少解释器的调用开销。</li><li><strong>跨平台部署</strong>：生成的 JIT 模型可以在支持 C++ API 的平台运行，无需依赖 Python 环境。</li></ul><ol start="4"><li><strong>计算优化</strong></li></ol><ul><li><strong>多线程并行化</strong>：JIT 模型能更好地利用多线程进行并行计算。</li><li><strong>矢量化</strong>：生成的代码可以利用底层硬件指令集（如 SIMD）进行矢量化计算，提升处理速度。</li></ul><ol start="5"><li><strong>更好的硬件支持</strong><br>TorchScript 支持对硬件加速器（如 GPU、TPU）的专门优化：</li></ol><ul><li><strong>CUDA 核心优化</strong>：在 GPU 上自动选择更高效的 CUDA 核。</li><li><strong>张量核心（Tensor Core）利用</strong>：在支持 Tensor Core 的硬件上自动启用更快的矩阵乘法和卷积操作。</li></ul><ol start="6"><li><strong>序列化和部署优化</strong></li></ol><ul><li><strong>更小的模型文件</strong>：TorchScript JIT 格式比传统 Python 模型的依赖项少，生成的文件体积通常更小。</li><li><strong>跨语言支持</strong>：JIT 编译模型生成的文件可以通过 C++、Java 或其他语言加载和运行，使得部署更加灵活。</li></ul><ul><li>总结<br>JIT 优化使得 PyTorch 模型在推理阶段速度更快、内存利用率更高，且具有跨平台和跨语言的部署能力。这些特性对于生产环境中的深度学习模型部署至关重要。</li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;是开源的深度学习框架，由Facebook的Deep Learning Team开发，2016年发布。本文主要记录&lt;strong&gt;Pytorch&lt;/strong&gt;学习过程中的一些知识点。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Visualization Of Transformer&#39;s Attention Score</title>
    <link href="http://blog.zhaoyongsheng.com/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/"/>
    <id>http://blog.zhaoyongsheng.com/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/</id>
    <published>2025-01-02T06:10:38.000Z</published>
    <updated>2025-01-02T06:10:38.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/">Transformer</a>模型在自然语言处理、视觉检测、分割和识别等任务中都得到了广泛应用，其核心思想是通过自注意力机制来捕捉学习输入序列中的嵌入特征，多头注意力模块中的注意力分数（Attention Score）是自注意力机制中的重要操作，可以很好的体现出输入序列各Token的关联关系。本文将介绍如何使用<a href="https://altair-viz.github.io/user_guide/saving_charts.html">Altair</a>对于Attention Score进行可视化，以方便对模型进行分析。</p><span id="more"></span><h3 id="工具包安装"><a href="#工具包安装" class="headerlink" title="工具包安装"></a>工具包安装</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install vl-convert-python <span class="comment"># Altair保存数据依赖该包</span></span><br><span class="line">pip install altair</span><br></pre></td></tr></tbody></table></figure><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>Attention Score可视化步骤包括：</p><ol><li>载入数据集和模型参数</li><li>模型进行前向推理，以获取Attention Score</li><li>使用Altair对Attention Score进行可视化并将结果保存为png文件</li></ol><h4 id="载入数据集和模型参数"><a href="#载入数据集和模型参数" class="headerlink" title="载入数据集和模型参数"></a>载入数据集和模型参数</h4><p>步骤主要有四步：</p><ol><li>载入key-value格式的config配置文件，以获取模型参数和路径</li><li>利用config配置信息载入数据集和tokenizer</li><li>利用config配置信息创建模型，并从训练好的checkpoint中载入模型参数<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_config, get_weight_file_path</span><br><span class="line">config = get_config()</span><br><span class="line">display(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> get_dataset</span><br><span class="line">train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> get_model</span><br><span class="line">model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_weight_file_path</span><br><span class="line">model_filename = get_weight_file_path(config, <span class="string">'22'</span>)</span><br><span class="line"><span class="comment"># model_filename = get_weight_file_path(config, config['preload'])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Preloading model from <span class="subst">{model_filename}</span>'</span>)</span><br><span class="line">state = torch.load(model_filename, weights_only=<span class="literal">True</span>)</span><br><span class="line">model.load_state_dict(state[<span class="string">'model_state_dict'</span>])</span><br></pre></td></tr></tbody></table></figure></li></ol><h4 id="模型前向推理"><a href="#模型前向推理" class="headerlink" title="模型前向推理"></a>模型前向推理</h4><ul><li><p>载入验证数据集的一个batch数据</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_next_val_batch</span>(<span class="params">val_dataloader, tokenizer_src, tokenizer_tgt, device</span>):</span><br><span class="line">    <span class="string">"""Load the next batch from the validation set</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        val_dataloader: validation dataloader</span></span><br><span class="line"><span class="string">        tokenizer_src: source tokenizer</span></span><br><span class="line"><span class="string">        tokenizer_tgt: target tokenizer</span></span><br><span class="line"><span class="string">        device: cuda or cpu</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        batch: next batch from the validation set</span></span><br><span class="line"><span class="string">        encoder_input_tokens: source sentence tokens</span></span><br><span class="line"><span class="string">        decoder_input_tokens: target sentence tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Load a sample batch from the validation set</span></span><br><span class="line">    batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(val_dataloader))</span><br><span class="line">    encoder_input = batch[<span class="string">"encoder_input"</span>].to(device)</span><br><span class="line">    encoder_mask = batch[<span class="string">"encoder_mask"</span>].to(device)</span><br><span class="line">    decoder_input = batch[<span class="string">"decoder_input"</span>].to(device)</span><br><span class="line">    decoder_mask = batch[<span class="string">"decoder_mask"</span>].to(device)</span><br><span class="line"></span><br><span class="line">    encoder_input_tokens = [tokenizer_src.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> encoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line">    decoder_input_tokens = [tokenizer_tgt.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> decoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># check that the batch size is 1</span></span><br><span class="line">    <span class="keyword">assert</span> encoder_input.size(</span><br><span class="line">        <span class="number">0</span>) == <span class="number">1</span>, <span class="string">"Batch size must be 1 for validation"</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> batch, encoder_input_tokens, decoder_input_tokens</span><br></pre></td></tr></tbody></table></figure></li><li><p>模型前向推理</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device</span>):</span><br><span class="line">    <span class="string">"""Greedy decode the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model: the model to decode</span></span><br><span class="line"><span class="string">        src: the source sequence</span></span><br><span class="line"><span class="string">        src_mask: the source mask</span></span><br><span class="line"><span class="string">        tokenizer_src: source tokenizer</span></span><br><span class="line"><span class="string">        tokenizer_tgt: target tokenizer</span></span><br><span class="line"><span class="string">        max_len: the maximum length of the input sequence</span></span><br><span class="line"><span class="string">        device: cuda or cpu</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sos_idx = tokenizer_tgt.token_to_id(<span class="string">'[SOS]'</span>)</span><br><span class="line">    eos_idx = tokenizer_tgt.token_to_id(<span class="string">'[EOS]'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Precompute the encoder output and reuse it for every token we get from the decoder</span></span><br><span class="line">    encoder_output = model.encode(src, src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the decoder input with the sos token</span></span><br><span class="line">    decoder_input = torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(sos_idx).type_as(src).to(device)</span><br><span class="line">    <span class="comment"># Predict the next token until reaching the max_len or the eos token</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> decoder_input.size(<span class="number">1</span>) == max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># Build mask for the target (decoder input)</span></span><br><span class="line">        decoder_mask = causal_mask(decoder_input.size(<span class="number">1</span>)).type_as(src).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the decoder and get the output</span></span><br><span class="line">        decoder_output = model.decode(decoder_input, encoder_output, src_mask, decoder_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the next token</span></span><br><span class="line">        prob = model.project(decoder_output[:, -<span class="number">1</span>]) <span class="comment"># (1, vocab_tgt_len)</span></span><br><span class="line">        <span class="comment"># Select the next token with the maximum probability (greedy search)</span></span><br><span class="line">        _, next_token = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.cat((decoder_input, torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(next_token.item()).type_as(src).to(device)), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the next token is the eos token</span></span><br><span class="line">        <span class="keyword">if</span> next_token.item() == eos_idx:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decoder_input.squeeze(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>Attention Score可视化</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">"%.3d %s"</span> % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">                <span class="string">"%.3d %s"</span> % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        columns=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_map</span>(<span class="params">attn_type: <span class="built_in">str</span>, layer: <span class="built_in">int</span>, head: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="keyword">if</span> attn_type == <span class="string">"encoder"</span>:</span><br><span class="line">        attn = model.encoder.encoder_blocks[layer].multi_head_attention.score</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">"decoder"</span>:</span><br><span class="line">        attn = model.decoder.decoder_blocks[layer].self_attention_block.score</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">"encoder-decoder"</span>:</span><br><span class="line">        attn = model.decoder.decoder_blocks[layer].cross_attention_block.score</span><br><span class="line">    <span class="keyword">return</span> attn[<span class="number">0</span>, head].data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn_type, layer, head, row_tokens, col_tokens, max_sentence_len</span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        get_attn_map(attn_type, layer, head),</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">"col_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">"row_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            color=<span class="string">"value"</span>,</span><br><span class="line">            tooltip=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#.title(f"Layer {layer} Head {head}")</span></span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>, title=<span class="string">f"Layer <span class="subst">{layer}</span> Head <span class="subst">{head}</span>"</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_all_attention_maps</span>(<span class="params">attn_type: <span class="built_in">str</span>, layers: <span class="built_in">list</span>[<span class="built_in">int</span>], heads: <span class="built_in">list</span>[<span class="built_in">int</span>], row_tokens: <span class="built_in">list</span>, col_tokens, max_sentence_len: <span class="built_in">int</span></span>):</span><br><span class="line">    charts = []</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        rowCharts = []</span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))</span><br><span class="line">        charts.append(alt.hconcat(*rowCharts))</span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(*charts)</span><br></pre></td></tr></tbody></table></figure></li></ul><p>整个流程代码如下所是：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Using device: <span class="subst">{device}</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> load_next_val_batch</span><br><span class="line">batch, encoder_input_tokens, decoder_input_tokens = load_next_val_batch(val_dataloader, tokenizer_src, tokenizer_tgt, device)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> greedy_decode</span><br><span class="line">model_out = greedy_decode(model, batch[<span class="string">'encoder_input'</span>].to(device), batch[<span class="string">'encoder_mask'</span>].to(device), tokenizer_src, tokenizer_tgt, config[<span class="string">'src_seq_len'</span>], device)</span><br><span class="line">display(model_out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">layers = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">heads = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">sentence_len = encoder_input_tokens.index(<span class="string">"[PAD]"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder Self-Attention</span></span><br><span class="line"><span class="built_in">map</span> = get_all_attention_maps(<span class="string">"encoder"</span>, layers, heads, encoder_input_tokens, encoder_input_tokens, <span class="built_in">min</span>(<span class="number">20</span>, sentence_len))</span><br><span class="line"><span class="built_in">map</span>.save(<span class="string">'chart.png'</span>, ppi=<span class="number">200</span>)</span><br></pre></td></tr></tbody></table></figure><p><img alt="Encoder Attention Score可视化图" data-src="/2025/01/02/Visualization-Of-Transformer-s-Attention-Score/encoder_attention_score.png"></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://youngsonzhao.github.io/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/&quot;&gt;Transformer&lt;/a&gt;模型在自然语言处理、视觉检测、分割和识别等任务中都得到了广泛应用，其核心思想是通过自注意力机制来捕捉学习输入序列中的嵌入特征，多头注意力模块中的注意力分数（Attention Score）是自注意力机制中的重要操作，可以很好的体现出输入序列各Token的关联关系。本文将介绍如何使用&lt;a href=&quot;https://altair-viz.github.io/user_guide/saving_charts.html&quot;&gt;Altair&lt;/a&gt;对于Attention Score进行可视化，以方便对模型进行分析。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Pytorch/Transformer/"/>
    
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/tags/Notes/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="Attention" scheme="http://blog.zhaoyongsheng.com/tags/Attention/"/>
    
      <category term="Visualization" scheme="http://blog.zhaoyongsheng.com/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>Visualization and Analysis Of Parameters And Architectures Of Models</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/</id>
    <published>2024-12-31T03:30:09.000Z</published>
    <updated>2024-12-31T03:30:09.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要介绍模型参数量的分析方法和模型结构的可视化方法。本文主要有两部分内容，第一部分是使用<a href="https://github.com/TylerYep/torchinfo">torchinfo</a>分析模型参数量，第二部分是使用<a href="https://github.com/lanpa/torch-summary">torchview</a>工具包对模型结构和模块进行可视化。</p><span id="more"></span><h2 id="Parameter-and-Memory-Analysis"><a href="#Parameter-and-Memory-Analysis" class="headerlink" title="Parameter and Memory Analysis"></a><strong>Parameter and Memory Analysis</strong></h2><p>模型参数分析参考文章：<a href="https://medium.com/the-owl/how-to-get-model-summary-in-pytorch-57db7824d1e3#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImFiODYxNGZmNjI4OTNiYWRjZTVhYTc5YTc3MDNiNTk2NjY1ZDI0NzgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMTA0MjY2MjgwMDUyOTgwMTQyMDAiLCJlbWFpbCI6InlvbmdzaGVuZy56aGFvLmNzY0BnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNzM1NTc0MDgyLCJuYW1lIjoiWW91bmdzb24gWmhhbyIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NLUWltYTZjZ054WnFOZW1majF1LXdXRW5Cd09wZWlMRERFenJ3Y01aNTNSY1JocXM0PXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IllvdW5nc29uIiwiZmFtaWx5X25hbWUiOiJaaGFvIiwiaWF0IjoxNzM1NTc0MzgyLCJleHAiOjE3MzU1Nzc5ODIsImp0aSI6ImU2OTRmMzEyZDAwZTA3Yjg3NWU1NzBmMDIxZTlkY2YyMWEwOTQ0MjIifQ.jB6C-8q7ZWwyWSwEa0a3Y4smtKiCtXT7zSkJQBqzNARU5zvDn_w90Funus3UgKcYEFh4g80KwKv3uEiDlnicvcQO93-LqOihbz2n8ljKuKpOyQpuTnaGQ4YayxmoLRwOHC4gzTpZkVadrDnc1BiBoyoXiFNKCE54wJIToD5Dj7qJFBC43djXHG6MlGnyt4TRJEu2ovbnpvef8LqryEXnpv1KVmlFdak_gsrnD4NSsQCQJd3Z_BJBls1nRt8Ph0QEi2SzZjnu7HbNBWS9cQxYTNllR4HP_umrd8GHBQWzQjPay0X0MXC9-j0jCV78Da_NtPJtWqv4T-88b16XZx1Ydw">How to get Model Summary in PyTorch</a></p><h3 id="torchinfo"><a href="#torchinfo" class="headerlink" title="torchinfo"></a><strong>torchinfo</strong></h3><p><a href="https://github.com/TylerYep/torchinfo">torchinfo</a>是一个用于分析PyTorch模型参数的工具包，可以方便地查看模型的参数量、计算量、内存占用等信息。</p><ul><li><p>torchinfo的安装方法如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchinfo</span><br></pre></td></tr></tbody></table></figure></li><li><p>torchinfo的使用方法如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line">model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># print model summary information</span></span><br><span class="line">summary(model, input_data=(torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>)).to(device)), device=device.<span class="built_in">type</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>torchinfo的模型summary信息(以Transformer)如下：</p><figure class="highlight nestedtext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">=========================================================================================================</span><br><span class="line">Layer (type:depth-idx)                                  Output Shape              Param #</span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Transformer                                             [16, 256, 22466]          --</span></span><br><span class="line"><span class="attribute">├─InputEmbedding</span><span class="punctuation">:</span> <span class="string">1-1                                   [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Embedding</span><span class="punctuation">:</span> <span class="string">2-1                                   [16, 256, 512]            8,038,912</span></span><br><span class="line"><span class="attribute">├─PositionalEncoding</span><span class="punctuation">:</span> <span class="string">1-2                               [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Dropout</span><span class="punctuation">:</span> <span class="string">2-2                                     [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">├─Encoder</span><span class="punctuation">:</span> <span class="string">1-3                                          [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─ModuleList</span><span class="punctuation">:</span> <span class="string">2-3                                  --                        --</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-1                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-2                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-3                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-4                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-5                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    │    └─EncoderBlock</span><span class="punctuation">:</span> <span class="string">3-6                           [16, 256, 512]            3,150,340</span></span><br><span class="line"><span class="attribute">│    └─LayerNormalization</span><span class="punctuation">:</span> <span class="string">2-4                          [16, 256, 512]            2</span></span><br><span class="line"><span class="attribute">├─InputEmbedding</span><span class="punctuation">:</span> <span class="string">1-4                                   [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Embedding</span><span class="punctuation">:</span> <span class="string">2-5                                   [16, 256, 512]            11,502,592</span></span><br><span class="line"><span class="attribute">├─PositionalEncoding</span><span class="punctuation">:</span> <span class="string">1-5                               [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─Dropout</span><span class="punctuation">:</span> <span class="string">2-6                                     [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">├─Decoder</span><span class="punctuation">:</span> <span class="string">1-6                                          [16, 256, 512]            --</span></span><br><span class="line"><span class="attribute">│    └─ModuleList</span><span class="punctuation">:</span> <span class="string">2-7                                  --                        --</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-7                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-8                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-9                           [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-10                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-11                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    │    └─DecoderBlock</span><span class="punctuation">:</span> <span class="string">3-12                          [16, 256, 512]            4,200,966</span></span><br><span class="line"><span class="attribute">│    └─LayerNormalization</span><span class="punctuation">:</span> <span class="string">2-8                          [16, 256, 512]            2</span></span><br><span class="line"><span class="attribute">├─ProjectionLayer</span><span class="punctuation">:</span> <span class="string">1-7                                  [16, 256, 22466]          --</span></span><br><span class="line"><span class="attribute">│    └─Linear</span><span class="punctuation">:</span> <span class="string">2-9                                      [16, 256, 22466]          11,525,058</span></span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Total params</span><span class="punctuation">:</span> <span class="string">75,174,402</span></span><br><span class="line"><span class="attribute">Trainable params</span><span class="punctuation">:</span> <span class="string">75,174,402</span></span><br><span class="line"><span class="attribute">Non-trainable params</span><span class="punctuation">:</span> <span class="string">0</span></span><br><span class="line"><span class="attribute">Total mult-adds (G)</span><span class="punctuation">:</span> <span class="string">1.20</span></span><br><span class="line"><span class="attribute">=========================================================================================================</span></span><br><span class="line"><span class="attribute">Input size (MB)</span><span class="punctuation">:</span> <span class="string">8.49</span></span><br><span class="line"><span class="attribute">Forward/backward pass size (MB)</span><span class="punctuation">:</span> <span class="string">3521.18</span></span><br><span class="line"><span class="attribute">Params size (MB)</span><span class="punctuation">:</span> <span class="string">300.70</span></span><br><span class="line"><span class="attribute">Estimated Total Size (MB)</span><span class="punctuation">:</span> <span class="string">3830.37</span></span><br><span class="line">=========================================================================================================</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="Parameter-Analysis-Step-by-Step"><a href="#Parameter-Analysis-Step-by-Step" class="headerlink" title="Parameter Analysis Step by Step"></a><strong>Parameter Analysis Step by Step</strong></h3><p><strong>Transformer</strong>模型的参数如下所示：</p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">src_vocab_size</span>: <span class="number">15701</span></span><br><span class="line"><span class="attribute">tgt_vocab_size</span>: <span class="number">22466</span></span><br><span class="line"><span class="attribute">src_seq_len</span>: <span class="number">256</span></span><br><span class="line"><span class="attribute">tgt_seq_len</span>: <span class="number">256</span></span><br><span class="line"><span class="attribute">embed_size</span>: <span class="number">512</span></span><br><span class="line"><span class="attribute">hidden_size</span>: <span class="number">2048</span></span><br><span class="line"><span class="attribute">num_heads</span>: <span class="number">8</span></span><br><span class="line"><span class="attribute">num_encoder_layers</span>: <span class="number">6</span></span><br><span class="line"><span class="attribute">num_decoder_layers</span>: <span class="number">6</span></span><br><span class="line"><span class="attribute">dropout</span>: <span class="number">0</span>.<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><p><strong>Transformer</strong>模型参数量大小跟<strong>src_vocab_size</strong>、<strong>tgt_vocab_size</strong>、<strong>embed_size</strong>、<strong>hidden_size</strong>、<strong>num_encoder_layers</strong>、<strong>num_decoder_layers</strong>有关。</p><h4 id="InputEmbedding"><a href="#InputEmbedding" class="headerlink" title="InputEmbedding"></a><strong>InputEmbedding</strong></h4><p><strong>InputEmbedding</strong>本质上是一个词表嵌入向量矩阵，用Lookup Table实现，其大小取决于词表中的词数和每个词的嵌入向量维度。</p><ul><li><p>对于源序列，其<strong>InputEmbedding</strong>的参数量大小为：<br>$$<br>P_{ie}^{src} = src_vocab_size \times embed_size = 15701 \times 512 = 8038912<br>$$<br>其中$P_{ie}^{src}$为源序列的<strong>InputEmbedding</strong>参数量大小。</p></li><li><p>对于目标序列，其<strong>InputEmbedding</strong>的参数量大小为：<br>$$<br>P_{ie}^{tgt} = tgt_vocab_size \times embed_size = 22466 \times 512 = 11502592<br>$$<br>其中$P_{ie}^{tgt}$为目标序列的<strong>InputEmbedding</strong>参数量大小。</p></li></ul><h4 id="PositionalEncoding"><a href="#PositionalEncoding" class="headerlink" title="PositionalEncoding"></a><strong>PositionalEncoding</strong></h4><p><strong>PositionalEncoding</strong>是由不同频率的正弦和余弦函数生成的绝对位置编码，是一个唯一确定的位置编码表，不存在可训练的参数，因此其参数量为0。</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a><strong>Encoder</strong></h4><p><strong>Encoder</strong>由6个<strong>EncoderBlock</strong>和1个<strong>LayerNormalization</strong>组成，因此其参数量大小为：<br>$$<br>P_{enc} = 6 \times P_{encb} + P_{ln} = 6 \times 3150340 + 2 = 18902042<br>$$<br>其中$P_{enc}$为<strong>Encoder</strong>的参数量大小，$P_{encb}$为<strong>EncoderBlock</strong>的参数量大小，$P_{ln}$为<strong>LayerNormalization</strong>的参数量大小。</p><h5 id="EncoderBlock"><a href="#EncoderBlock" class="headerlink" title="EncoderBlock"></a><strong>EncoderBlock</strong></h5><p><strong>EncoderBlock</strong>由一个<strong>MultiHeadAttentionBlock</strong>、一个<strong>FeedForwardBlock</strong>和两个<strong>ResidualConnection</strong>组成，因此其参数量大小为：<br>$$<br>P_{encb} = P_{mhab} + P_{ffb} + 2 \times P_{rc} = 1050624 + 2099712 + 2 \times 2 = 3150340<br>$$<br>其中$P_{encb}$为<strong>EncoderBlock</strong>的参数量大小，$P_{mhab}$为<strong>MultiHeadAttentionBlock</strong>的参数量大小，$P_{ffb}$为<strong>FeedForwardBlock</strong>的参数量大小，$P_{rc}$为<strong>ResidualConnection</strong>的参数量大小。</p><h6 id="ResidualConnection"><a href="#ResidualConnection" class="headerlink" title="ResidualConnection"></a><strong>ResidualConnection</strong></h6><p><strong>ResidualConnection</strong>模块包含一个<strong>LayerNormalization</strong>和一个<strong>Add</strong>操作，因此其参数量s大小为：<br>$$<br>P_{rc} = P_{ln} = 2<br>$$<br>其中$P_{rc}$为<strong>ResidualConnection</strong>的参数量大小，$P_{ln}$为<strong>LayerNormalization</strong>的参数量大小。</p><h6 id="MultiHeadAttentionBlock"><a href="#MultiHeadAttentionBlock" class="headerlink" title="MultiHeadAttentionBlock"></a><strong>MultiHeadAttentionBlock</strong></h6><p><strong>MultiHeadAttentionBlock</strong>由四个<strong>Linear</strong>模块组成，因此其参数量大小为：<br>$$<br>P_{mhab} = 4 \times P_{linear} = 4 \times (embed_size \times embed_size + embed_size) = 4 \times (512 \times 512 + 512 \times 512 + 512) = 1050624<br>$$<br>其中$P_{linear}$为<strong>MultiHeadAttentionBlock</strong>模块中的线性映射层，共有4个，$embed_size$为<strong>Linear</strong>模块的输入和输出维度。</p><h6 id="FeedForwardBlock"><a href="#FeedForwardBlock" class="headerlink" title="FeedForwardBlock"></a><strong>FeedForwardBlock</strong></h6><p><strong>FeedForwardBlock</strong>由两个<strong>Linear</strong>、一个<strong>ReLU</strong>和一个<strong>Dropout</strong>组成，其中<strong>ReLU</strong>和<strong>Dropout</strong>模块不包含可训练的参数，因此其参数量大小为：<br>$$<br>P_{ffb} = embed_size \times hidden_size + hidden_size + hidden_size \times embed_size + embed_size = 512 \times 2048 + 2048 + 2048 \times 512 + 512 = 2099712<br>$$<br>其中$P_{ffb}$为<strong>FeedForwardBlock</strong>的参数量大小，$embed_size$为<strong>Linear</strong>模块的输入和输出维度，$hidden_size$为<strong>FeedForwardBlock</strong>的隐藏层维度。</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a><strong>Decoder</strong></h4><p><strong>Decoder</strong>由6个<strong>DecoderBlock</strong>和1个<strong>LayerNormalization</strong>组成，因此其参数量大小为：<br>$$<br>P_{dec} = 6 \times P_{decb} + P_{ln} = 6 \times 4200966 + 2 = 25205798<br>$$<br>其中$P_{dec}$为<strong>Decoder</strong>的参数量大小，$P_{decb}$为<strong>DecoderBlock</strong>的参数量大小，$P_{ln}$为<strong>LayerNormalization</strong>的参数量大小。</p><h5 id="DecoderBlock"><a href="#DecoderBlock" class="headerlink" title="DecoderBlock"></a><strong>DecoderBlock</strong></h5><p><strong>DecoderBlock</strong>由两个<strong>MultiHeadAttentionBlock</strong>、一个<strong>FeedForwardBlock</strong>和三个<strong>ResidualConnection</strong>组成，因此其参数量大小为：<br>$$<br>P_{decb} = P_{smhab} + P_{cmhab} + P_{ffb} + 3 \times P_{rc} = 1050624 + 1050624 + 2099712 + 3 \times 2 = 4200966<br>$$<br>其中$P_{decb}$为<strong>DecoderBlock</strong>的参数量大小，$P_{smhab}$为<strong>SelfMultiHeadAttentionBlock</strong>的参数量大小，$P_{cmhab}$为<strong>CrossMultiHeadAttentionBlock</strong>的参数量大小，$P_{ffb}$为<strong>FeedForwardBlock</strong>的参数量大小，$P_{rc}$为<strong>ResidualConnection</strong>的参数量大小。</p><h4 id="ProjectionLayer"><a href="#ProjectionLayer" class="headerlink" title="ProjectionLayer"></a><strong>ProjectionLayer</strong></h4><p><strong>ProjectionLayer</strong>由一个<strong>Linear</strong>和一个<strong>Dropout</strong>组成，因此其参数量大小为：<br>$$<br>P_{pl} = embedding_size \times tgt_vocab_size + tgt_vocab_size = 512 \times 22466 + 22466 = 11525058<br>$$</p><h4 id="Total-Parameters"><a href="#Total-Parameters" class="headerlink" title="Total Parameters"></a><strong>Total Parameters</strong></h4><p><strong>Transformer</strong>模型的总参数量为：<br>$$<br>P_{total} = P_{ie}^{src} + P_{ie}^{tgt} + P_{enc} + P_{dec} + P_{pl} = 8038912 + 11502592 + 18902042 + 25205798 + 11525058 = 75174402<br>$$</p><h3 id="Memory-Analysis"><a href="#Memory-Analysis" class="headerlink" title="Memory Analysis"></a><strong>Memory Analysis</strong></h3><p><strong>Transformer</strong>模型的参数都是Float32格式的，一个参数占用4字节，因此参数量占用内存大小为：<br>$$<br>M_{total} = P_{total} * 4 / 1000 / 1000 = 75174402 * 4 / 1000 / 1000 = 300.70MB<br>$$</p><h4 id="中间激活值"><a href="#中间激活值" class="headerlink" title="中间激活值"></a>中间激活值</h4><p>中间激活值是为了计算梯度而需要保留的中间结果，具体计算方法和步骤可以参照<a href="https://mingchao.wang/4KTgtnFc/">这里</a>。</p><h2 id="Architecture-Visualization"><a href="#Architecture-Visualization" class="headerlink" title="Architecture Visualization"></a><strong>Architecture Visualization</strong></h2><p><a href="https://github.com/mert-kurttutan/torchview">torchview</a>是pytorch模型结构常用的可视化工具，可以分层查看模型结构和模型的输入输出。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p><strong>torchview</strong>是基于<strong>graphviz</strong>实现的，因此需要先安装<strong>graphviz</strong>，然后安装<strong>torchview</strong>。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install graphviz</span><br><span class="line">pip install graphviz</span><br><span class="line">pip install torchview</span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a><strong>使用</strong></h3><p><strong>torchview</strong>的使用步骤很简单，首先定义模型(步骤见上面章节)，然后定义模型的输入数据，最后调用<strong>torchview</strong>函数即可。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchview <span class="keyword">import</span> draw_graph</span><br><span class="line">model_graph = draw_graph(model, input_data=(torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">2</span>, (<span class="number">16</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)).to(device), torch.randint(<span class="number">1</span>, (<span class="number">16</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>)).to(device)), depth=<span class="number">2</span>, device=device.<span class="built_in">type</span>, expand_nested=<span class="literal">True</span>, save_graph=<span class="literal">True</span>, filename=<span class="string">'transformer'</span>)</span><br><span class="line">model_graph.visual_graph</span><br></pre></td></tr></tbody></table></figure><p><img alt="Transformer模型结构（depth=2, expand_nested=True）" data-src="/2024/12/31/Visualization-and-Analysis-Of-Parameters-And-Architectures-Of-Models/transformer.png"></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍模型参数量的分析方法和模型结构的可视化方法。本文主要有两部分内容，第一部分是使用&lt;a href=&quot;https://github.com/TylerYep/torchinfo&quot;&gt;torchinfo&lt;/a&gt;分析模型参数量，第二部分是使用&lt;a href=&quot;https://github.com/lanpa/torch-summary&quot;&gt;torchview&lt;/a&gt;工具包对模型结构和模块进行可视化。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Model" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Model/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Model/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
      <category term="Model" scheme="http://blog.zhaoyongsheng.com/tags/Model/"/>
    
  </entry>
  
  <entry>
    <title>Architecture Of Transformer And PyTorch Implementation</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/</id>
    <published>2024-12-23T15:38:41.000Z</published>
    <updated>2024-12-23T15:38:41.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文详细介绍<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf">Attention Is All You Need</a>论文中Transformer模型架构、各个模块的数学原理、输入输出、参数规模，以及PyTorch实现。</p><span id="more"></span><h2 id="模型整体架构"><a href="#模型整体架构" class="headerlink" title="模型整体架构"></a>模型整体架构</h2><p><img alt="Transformer模型结构图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/transformer-architecture.png"><br>原始的Transformer模型是<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>的经典结构，<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>分别由$N_e$个<strong>Encoder Block</strong>和$N_d$个<strong>Decoder Block</strong>残差堆叠组成。编码器的输入是源序列，尺寸是(batch_size, src_seq_len)，输出是Encoder Embedding，尺寸是(batch_size, src_seq_len, embed_size)，解码器的输入是目标序列，尺寸是(batch_size, tgt_seq_len)和Encoder Embedding，输出是Decoder Embedding，尺寸是(batch_size, seq_len, embed_size)。</p><ul><li>参考连接<ul><li><a href="https://xiaosheng.blog/2022/06/28/use-pytorch-to-implement-transformer">使用 Pytorch 一步一步实现 Transformer Encoder</a></li><li><a href="https://mingchao.wang/crmAf7MS/">Transformer</a></li><li><a href="https://github.com/hkproj/pytorch-transformer/tree/main">pytorch-transformer</a></li><li><a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference</a></li><li><a href="https://www.zhihu.com/question/347678607">如何理解Transformer论文中的positional encoding，和三角函数有什么关系？</a></li></ul></li></ul><h2 id="各模块数学原理与代码实现"><a href="#各模块数学原理与代码实现" class="headerlink" title="各模块数学原理与代码实现"></a>各模块数学原理与代码实现</h2><h3 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a><strong>Input Embedding</strong></h3><p><strong>Input Embedding</strong>（输入嵌入）是将离散Token序列转换为连续向量序列的过程。Transformer模型的输入是文本字符串，首先利用<a href="">Tokenizer</a>将文本转换为Token序列，然后利用<a href="">nn.Embedding</a>将每一个Token保存为一个embed_size维的Embedding向量。<strong>nn.Embedding</strong>本质是一个Lookup Table，Key是Token，Value是Embedding向量，关键参数有两个，vocab_size表示词表中的Token数量, embed_size表示Embedding向量的维度。</p><p>Input Embedding是(vocab_size, embed_size)的参数矩阵，在模型训练时会同步更新该参数矩阵。如果想要冻结Input Embedding的参数矩阵，可以设置embedding.weight.requires_grad = False。Input Embedding的实现代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InputEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Embedding layer for input tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size: <span class="built_in">int</span>, embed_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the embedding layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vocab_size: size of the vocabulary</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(InputEmbedding, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.embedding.weight.data.uniform_(-<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the embedding layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            embedded tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.embedding(x) * math.sqrt(self.embed_size)</span><br></pre></td></tr></tbody></table></figure><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a><strong>Positional Encoding</strong></h3><p><strong>Positional Encoding</strong>（位置编码）是Transformer模型中用于给每个Token添加位置信息的机制。由于Transformer模型中没有循环神经网络（RNN）中的序列位置顺序信息，因此需要通过位置编码来提供序列的顺序信息，在论文中使用了正弦和余弦函数来编码位置信息。<br>在 Transformer 模型中，<strong>Positional Encoding (位置编码)</strong> 用于为输入序列引入位置信息，以弥补自注意力机制（Self-Attention）缺少序列顺序感知能力的不足。它的设计基于正弦和余弦函数的数学原理，使得不同位置的编码具有可解析的相对位置信息。</p><h4 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a><strong>数学公式</strong></h4><p>给定一个位置 $pos$ 和一个维度 $i$（嵌入向量的第 $i$ 个维度），位置编码的值由以下公式定义：<br>$$<br>PE(pos, k) =<br>\begin{cases}<br>\sin\left(\frac{1}{10000^{\frac{2i}{embed_size}}} * pos\right)&amp; \text{if } k = 2i \<br>\cos\left(\frac{1}{10000^{\frac{2i}{embed_size}}} * pos\right)&amp; \text{if } k = 2i+1<br>\end{cases}<br>$$<br>其中</p><ul><li>$pos$ 是Token在序列中的位置（从 0 开始，最大值是seq_len - 1）。</li><li>$k$ 是Embedding的维度索引（从 0 开始，最大值是embed_size - 1）。</li><li>$embed_size$ 是Embedding的维度（embed_size）。<br><img alt="正弦位置编码示意图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/sine_frequencies_plot.png"></li></ul><h4 id="位置编码可视化"><a href="#位置编码可视化" class="headerlink" title="位置编码可视化"></a><strong>位置编码可视化</strong></h4><p>不同序列长度和嵌入特征维度下的位置编码可视化如下：<br><img alt="位置编码可视化（embed_size=512, seq_len=256）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap.png"><br><img alt="位置编码可视化（embed_size=256, seq_len=128）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap_1.png"><br><img alt="位置编码可视化（embed_size=128, seq_len=64）" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/positional_encoding_heatmap_2.png"></p><ol><li><p><strong>频率范围：</strong></p><ul><li>分母中的 $10000^{\frac{2i}{embed_size}}$ 利用指数函数设计了一个频率缩放机制，使得高维度对应的频率更低，低维度对应的频率更高。</li><li>这种设计让每个维度的编码值具有不同的频率特性，确保编码中包含丰富的位置信息。</li><li>对于每个维度 $i$，通过不同的频率编码（由 $10000^{\frac{2i}{embed_size}}$ 控制）生成一个独特的信号，使得高维信息捕获细粒度的相对位置信息。</li><li>嵌入特征的维度（embed_size）越高，位置编码的粒度越细，信息丰富度越大。</li></ul></li><li><p><strong>正弦与余弦函数：</strong></p><ul><li>使用正弦和余弦为奇数维度和偶数维度生成不同的变化模式。</li><li>正弦和余弦函数可以保证位置编码的值在[-1, 1]范围内。</li></ul></li></ol><h4 id="核心性质"><a href="#核心性质" class="headerlink" title="核心性质"></a><strong>核心性质</strong></h4><ol><li><p><strong>绝对与相对位置：</strong></p><ul><li>位置编码是对所有位置$(pos, k)$的绝对编码，且编码唯一（由嵌入特征维度决定）。</li><li>位置编码的设计确保了任意两个位置之间的相对位置信息可以被简单计算：<br>$$<br>PE(pos_1) - PE(pos_2)<br>$$<br>这一性质帮助模型直接捕获相对位置。</li></ul></li><li><p><strong>序列长度无关：</strong></p><ul><li>由于正弦函数和余弦函数是周期性函数，其频率系数由嵌入特征的维度（embed_size）和嵌入特征的索引（$2i$）决定，因此位置编码的值与序列长度无关，位置编码可以为任意长度的序列编码，且相同序列索引（$pos$）的位置编码相同。</li><li></li></ul></li><li><p><strong>无需训练：</strong></p><ul><li>位置编码的值是通过公式计算的，无需学习参数。这减少了模型参数量，同时保持了良好的泛化能力。</li></ul></li></ol><h3 id="改进与替代方法"><a href="#改进与替代方法" class="headerlink" title="改进与替代方法"></a><strong>改进与替代方法</strong></h3><p>尽管正弦-余弦位置编码有诸多优点，但在后续研究中，也提出了许多替代方法：</p><ol><li><p><strong>可学习的位置编码：</strong><br>通过将位置编码视为可学习的参数矩阵，使模型在特定任务上更具适应性。</p></li><li><p><strong>相对位置编码（Relative Positional Encoding）：</strong><br>提取位置之间的相对关系，如 Transformer-XL 和 T5 中的改进版本。</p></li><li><p><strong>旋转位置编码（RoPE, Rotary Positional Embedding）：</strong><br>在 Transformers 中通过旋转操作高效捕获相对位置信息。</p></li></ol><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h4><p>为了提升计算效率，将位置编码中的频率系数做如下优化：<br>$$<br>\begin{align*}<br>\frac{1}{10000^{\frac{2i}{embed_size}}}<br>  &amp;= \exp(\log(\frac{1}{10000^{\frac{2i}{embed_size}}})) \<br>  &amp;= \exp(\log({10000^{\frac{2i}{embed_size}}}^{-1})) \<br>  &amp;= \exp(-1 * \log({10000^{\frac{2i}{embed_size}}})) \<br>  &amp;= \exp(-1 * {\frac{2i}{embed_size}} * \log(10000)) \<br>  &amp;= \exp(-{\frac{\log(10000)}{embed_size}} * {2i})<br>\end{align*}<br>$$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Positional encoding layer for input tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the positional encoding layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            max_len: maximum length of the input sequence</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create positional encoding, a matrix of shape (seq_len, embed_size)</span></span><br><span class="line">        pe = torch.zeros(seq_len, embed_size)</span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, seq_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># (seq_len, 1), unsqueeze: add a new dimension</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, embed_size, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / embed_size)) <span class="comment"># (embed_size/2,)</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * div_term)</span><br><span class="line">        <span class="comment"># self.pe = self.pe.unsqueeze(0).transpose(0, 1) # (seq_len, 1, embed_size)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># (1, seq_len, embed_size), 1 for batch size</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># register buffer, so it doesn't need to be trained but can be saved</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the positional encoding layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            positional encoded tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># x = x + self.pe[:, :x.size(1), :].requires_grad(False)</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>), :].detach()</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></tbody></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a><strong>Layer Normalization</strong></h3><p><strong>Layer Normalization</strong>（层归一化）是一种常用于深度学习模型的正则化技术，特别是在序列建模和自然语言处理任务中。它旨在通过标准化每一层的激活值来提高模型的训练稳定性和收敛速度。</p><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a><strong>基本原理</strong></h4><p>在神经网络中，每一层的激活值可能因输入数据的变化而幅度不一，这种变化被称为<strong>内部分布漂移（Internal Covariate Shift）</strong>。Layer Normalization 通过对每一层的神经元输出进行标准化，减少这种漂移的影响。</p><p>Layer Normalization 的标准化是针对<strong>层的维度</strong>（即神经元的集合）进行的，而不是像 Batch Normalization 针对<strong>样本的维度</strong>。具体而言，对于每个输入样本，Layer Normalization 会对其当前层的激活值进行归一化处理。</p><h4 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a><strong>数学表达</strong></h4><p>假设某层的输入是一个向量 $\mathbf{h} = [h_1, h_2, …, h_d]$，其中 $d$ 是该层的神经元数。Layer Normalization 的计算过程如下：</p><ol><li><p><strong>计算均值和方差</strong>：<br>$$<br>\mu = \frac{1}{d} \sum_{i=1}^d h_i, \quad \sigma^2 = \frac{1}{d} \sum_{i=1}^d (h_i - \mu)^2<br>$$</p></li><li><p><strong>标准化</strong>：<br>$$<br>\hat{h}_i = \frac{h_i - \mu}{\sqrt{\sigma^2 + \epsilon}}<br>$$<br>其中，$\epsilon$ 是一个小常数，用于防止分母为零。</p></li><li><p><strong>添加可学习参数</strong>（仿射变换）：<br>$$<br>h_i^{\text{norm}} = \gamma \hat{h}_i + \beta<br>$$<br>其中，$\gamma$ 和 $\beta$ 是可学习参数，用于恢复网络的表达能力。</p></li></ol><h4 id="与-Batch-Normalization-的对比"><a href="#与-Batch-Normalization-的对比" class="headerlink" title="与 Batch Normalization 的对比"></a><strong>与 Batch Normalization 的对比</strong></h4><table><thead><tr><th>特性</th><th>Layer Normalization</th><th>Batch Normalization</th></tr></thead><tbody><tr><td><strong>归一化维度</strong></td><td>每层神经元（样本内部）</td><td>样本维度（批次内的样本间）</td></tr><tr><td><strong>依赖批大小</strong></td><td>不依赖</td><td>依赖批大小</td></tr><tr><td><strong>适用场景</strong></td><td>RNN、Transformer 等序列模型</td><td>CNN 等需要大批量训练的模型</td></tr><tr><td><strong>计算开销</strong></td><td>较小</td><td>可能较大</td></tr></tbody></table><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a><strong>优点</strong></h4><ol><li><strong>适用于小批量或单样本训练</strong>：因为不依赖批次统计信息，Layer Normalization 能很好地适用于小批量甚至单样本训练。</li><li><strong>改善收敛性</strong>：通过减少激活值的变化，模型更容易收敛。</li><li><strong>在序列任务中的优势</strong>：特别适合 Recurrent Neural Network（RNN）和 Transformer，因为它不会破坏时间步的依赖关系。</li></ol><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a><strong>应用场景</strong></h4><ul><li>Transformer 中广泛使用（例如 BERT 和 GPT 模型）。</li><li>在序列建模（如语言模型、时间序列预测）和小批量训练任务中效果显著。</li></ul><p>通过 Layer Normalization，深度学习模型可以更快地学习、避免梯度爆炸或消失，并增强泛化能力。</p><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps: <span class="built_in">float</span> = <span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the layer normalization layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            eps: epsilon value for numerical stability</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).__init__()</span><br><span class="line">        self.eps = eps <span class="comment"># epsilon value for numerical stability</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>)) <span class="comment"># scale parameter, learnable parameter</span></span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>)) <span class="comment"># shift parameter, learnable parameter</span></span><br><span class="line">        <span class="comment"># self.beta = nn.Parameter(torch.ones(embed_size))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the layer normalization layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            normalized tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br></pre></td></tr></tbody></table></figure><h3 id="Feed-Forward-Block"><a href="#Feed-Forward-Block" class="headerlink" title="Feed Forward Block"></a><strong>Feed Forward Block</strong></h3><p><strong>Feed Forward Block</strong> 是 Transformer 架构中的一个重要模块，主要用于将序列特征进行非线性变换，以提升模型的表达能力。它由两个全连接层组成，第一个层用于处理输入序列特征，第二个层用于处理第一个层的输出。</p><p>主要参数有两个：</p><ul><li>embed_size: 输入序列特征的维度</li><li>hidden_size: 全连接层的隐藏层维度</li></ul><h4 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a><strong>数学原理</strong></h4><ol><li><p><strong>第一层全连接层</strong>：将输入序列特征 $X$ 通过线性变换映射到一个更高维度的空间，得到 $Y$。<br>$$<br>Y = W_1X + b_1<br>$$<br>其中$X \in \mathbb{R}^{seq_len \times embed_size}$为输入序列特征，$Y \in \mathbb{R}^{seq_len \times hidden_size}$为第一层全连接层的输出序列特征。$W_1$和$b_1$是可学习的参数，$W_1 \in \mathbb{R}^{hidden_size \times embed_size}$, $b_1 \in \mathbb{R}^{hidden_size}$。</p></li><li><p><strong>激活函数</strong>：在第一层全连接层之后，通常会添加一个非线性激活函数，如 ReLU 或 GELU，以引入非线性变换。</p></li><li><p><strong>Dropout</strong>：为了防止过拟合，通常会在第二层全连接层之前添加 Dropout。</p></li><li><p><strong>第二层全连接层</strong>：将第一层全连接层的输出$Y$通过线性变换映射回原始空间，得到 $Z$。<br>$$<br>Z = W_2Y + b_2<br>$$<br>其中$Y \in \mathbb{R}^{seq_len \times hidden_size}$为第一个全连接层的输出特征，$Z \in \mathbb{R}^{seq_len \times embed_size}$为第二层全连接层的输出序列特征。$W_2$和$b_2$是可学习的参数，$W_2 \in \mathbb{R}^{embed_size \times hidden_size}$, $b_2 \in \mathbb{R}^{embed_size}$。</p></li></ol><h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, hidden_size: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the feedforward layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            hidden_size: size of the hidden layer</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForwardBlock, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(embed_size, hidden_size) <span class="comment"># (embed_size, hidden_size), linear layer</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, embed_size) <span class="comment"># (hidden_size, embed_size), linear layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""Forward pass of the feedforward layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.linear1(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.relu(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.dropout(x) <span class="comment"># (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        x = self.linear2(x) <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure><h3 id="Multi-Head-Attention-Block"><a href="#Multi-Head-Attention-Block" class="headerlink" title="Multi Head Attention Block"></a><strong>Multi Head Attention Block</strong></h3><p><strong>Multi Head Attention Block</strong> 是 Transformer 架构中的核心模块，主要用于处理序列特征之间的交互关系，提取序列编码特征。它由三个主要部分组成：<strong>Query, Key, Value</strong>，以及多头注意力机制。</p><p>主要参数有三个：</p><ul><li>embed_size: 输入序列特征的维度</li><li>num_heads: 多头注意力机制的heads数量</li><li>dropout: Dropout层的概率，用于防止过拟合，在<strong>Query</strong>和<strong>Key</strong>相乘之后使用</li></ul><h4 id="数学原理-1"><a href="#数学原理-1" class="headerlink" title="数学原理"></a><strong>数学原理</strong></h4><p><img alt="多头注意力机制网络架构图" data-src="/2024/12/23/Architecture-Of-Transformer-And-PyTorch-Implementation/multi_head_self_attention.png"><br>如上图所是，多头注意力机制由<strong>Scaled Dot-Product Attention</strong>和<strong>Multi-Head Attention</strong>两部分组成。<br><strong>Scaled Dot-Product Attention</strong> 是多头注意力机制基础部分，其公式如下：<br>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{head_size}})V<br>$$<br>其中$Q, K, V \in \mathbb{R}^{batch_size \times num_heads \times seq_len \times head_size}$分别表示Query, Key, Value序列特征，$head_size$表示序列特征头维度，$\sqrt{head_size}$是为了防止$QK^T$过大导致的梯度消失问题。</p><p>在计算$\frac{QK^T}{\sqrt{head_size}}$之后，需要进行三步操作：</p><ol><li>需要使用<strong>mask</strong>对得到的注意力权重进行掩码操作，防止模型关注到无效的位置。</li><li>使用<strong>softmax</strong>对注意力权重进行归一化，得到最终的注意力权重矩阵。</li><li>将注意力权重矩阵进行<strong>dropout</strong>操作，防止过拟合。</li></ol><p><strong>Multi-Head Attention</strong> 是多头注意力机制的最终实现，是在<strong>Scaled Dot-Product Attention</strong>的基础上进行了线性映射，以提高多头注意力模块的表达能力和拟合能力。<br>对输入序列特征$Q, K, V$进行线性映射，得到新的序列特征，公式如下：<br>$$<br>\begin{align*}<br>Q^{‘} &amp;= W_{q}Q + b_{q} \<br>K^{‘} &amp;= W_{k}K + b_{k} \<br>V^{‘} &amp;= W_{v}V + b_{v}<br>\end{align*}<br>$$</p><p>对输出序列特征$O$进行线性映射，得到最终的输出序列特征，公式如下：<br>$$<br>O^{‘} = W_{o}O + b_{o}<br>$$</p><h4 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a><strong>代码实现</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Multi-head attention block.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size: <span class="built_in">int</span>, num_heads: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">"""Initialize the multi-head attention layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: size of the embedding vector</span></span><br><span class="line"><span class="string">            num_heads: number of attention heads</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttentionBlock, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> embed_size % num_heads == <span class="number">0</span>, <span class="string">"Embedding size must be divisible by number of heads"</span></span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_size = embed_size // num_heads</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define the linear layers</span></span><br><span class="line">        self.query = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for query seq</span></span><br><span class="line">        self.key = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for key seq</span></span><br><span class="line">        self.value = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for value seq</span></span><br><span class="line">        </span><br><span class="line">        self.out = nn.Linear(embed_size, embed_size) <span class="comment"># (embed_size, embed_size), linear layer for output</span></span><br><span class="line">    <span class="comment"># @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout: nn.Dropout = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Compute the attention scores and apply the softmax function. key and value are the same shape and key_seq_len = value_seq_len.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: input tensor of shape (batch_size, head_nums, query_seq_len, head_size)</span></span><br><span class="line"><span class="string">            key: input tensor of shape (batch_size, head_nums, key_seq_len, head_size)</span></span><br><span class="line"><span class="string">            value: input tensor of shape (batch_size, head_nums, value_seq_len, head_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, key_seq_len) or (batch_size, 1, query_seq_len, key_seq_len)</span></span><br><span class="line"><span class="string">            dropout: optional dropout layer</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            embedding: attention embedding tensor of shape (batch_size, num_heads, seq_len, head_size)</span></span><br><span class="line"><span class="string">            score: attention scores tensor of shape (batch_size, num_heads, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># compute the attention scores, QK^T/(sqrt(head_size)</span></span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, head_size) x (batch_size, num_heads, key_seq_len, head_size) --&gt; (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line">        score = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(query.size(-<span class="number">1</span>)) <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply mask if provided</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># print(type(mask))</span></span><br><span class="line">            <span class="comment"># print(mask.shape)</span></span><br><span class="line">            score = score.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># apply softmax function</span></span><br><span class="line">        score = torch.softmax(score, dim=-<span class="number">1</span>) <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply dropout</span></span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            score = dropout(score)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute the weighted sum of values as embedding</span></span><br><span class="line">        <span class="comment"># key_seq_len = value_seq_len</span></span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, key_seq_len) x (batch_size, num_heads, value_seq_len, head_size) --&gt; (batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        embedding = torch.matmul(score, value) <span class="comment">#(batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        <span class="keyword">return</span> embedding, score</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""Forward pass of the multi-head attention layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            key: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            value: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, seq_len) or (batch_size, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Linear transformations for query, key, and value</span></span><br><span class="line">        query = self.query(query) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line">        key = self.key(key) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line">        value = self.value(value) <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># split the embedding into multiple heads</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_size) --&gt; (batch_size, seq_len, num_heads, head_size) --&gt; (batch_size, num_heads, seq_len, head_size)</span></span><br><span class="line">        <span class="comment"># view: reshape tensor without changing its data</span></span><br><span class="line">        <span class="comment"># transpose: swap the positions of two axes in an array</span></span><br><span class="line">        batch_size = query.shape[<span class="number">0</span>]</span><br><span class="line">        query = query.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, query_seq_len, head_size)</span></span><br><span class="line">        key = key.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, key_seq_len, head_size)</span></span><br><span class="line">        value = value.view(batch_size, -<span class="number">1</span>, self.num_heads, self.head_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (batch_size, num_heads, value_seq_len, head_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the attention embedding and score</span></span><br><span class="line">        embedding, self.score = self.attention(query, key, value, mask, self.dropout)</span><br><span class="line">        <span class="comment"># (batch_size, num_heads, query_seq_len, head_size) --&gt; (batch_size, query_seq_len, num_heads, head_size) --&gt; (batch_size, query_seq_len, embed_size)</span></span><br><span class="line">        embedding = embedding.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.embed_size) </span><br><span class="line">        <span class="keyword">return</span> self.out(embedding) <span class="comment"># (batch_size, query_seq_len, embed_size) --&gt; (batch_size, query_seq_len, embed_size)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a><strong>Residual Connection</strong></h3><p>从Transformer模型结构图中可以看出，每一个<strong>Encoder Block</strong>和<strong>Decoder Block</strong>都包含<strong>Residual Connection</strong>模块，它将输入与经过子层（Sublayer）的输出相加，然后进行Layer Normalization。这样做的好处是，可以防止梯度消失和梯度爆炸，同时也可以让模型更容易训练。</p><h4 id="数学原理-2"><a href="#数学原理-2" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Residual Connection</strong>的数学原理很简单，就是将输入与经过Dropout操作之后的子层（Sublayer）输出相加，然后进行Layer Normalization。具体公式如下：<br>$$<br>\text{Residual Connection}(x, \text{Sublayer}(x)) = x + \text{Dropout}(\text{Sublayer}(\text{Layer Normalization}(x)))<br>$$</p><ul><li>Layer Normalization：在进行子层操作之前首先进行Layer Normalization操作，用于对输入进行归一化，使得输入具有均值为0和标准差为1的分布，从而提高模型的训练效果。</li><li>Sublayer：表示子层，可以是任何神经网络层，如Multi-Head Attention、Feed Forward Network等</li><li>Dropout：子层操作之后需要进行Dropout操作，用于防止过拟合</li></ul><h4 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight julia"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class ResidualConnection(nn.<span class="built_in">Module</span>):</span><br><span class="line">    def __init__(self, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""Initialize the residual connection layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResidualConnection, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, sublayer: nn.<span class="built_in">Module</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies a sublayer followed by a residual connection and layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            sublayer: nn.Module representing the sublayer to be applied</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># return self.norm(x + self.dropout(sublayer(x)))</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></tbody></table></figure><h3 id="Encoder-Block"><a href="#Encoder-Block" class="headerlink" title="Encoder Block"></a><strong>Encoder Block</strong></h3><p><strong>Encoder Block</strong>是Transformer模型的核心组成部分，它由三个子模块组成：<strong>Multi-Head Attention</strong>，<strong>Feed Forward Block</strong>和<strong>Residual Connection</strong>。</p><h4 id="数学原理-3"><a href="#数学原理-3" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Encoder Block</strong>的数学原理如下：<br>$$<br>\text{Encoder Block}(x) =<br>\begin{cases}<br>\text{Residual Connection}(x, \text{Multi-Head Attention}(x, x, x))&amp; step1 \<br>\text{Residual Connection}(x, \text{Feed Forward Block}(x))&amp; step2<br>\end{cases}<br>$$</p><ul><li>Multi-Head Attention：用于对输入进行自注意力机制，提取输入中的关键信息。</li><li>Feed Forward Network：用于对输入进行非线性变换，增加模型的表达能力。</li></ul><h4 id="代码实现-5"><a href="#代码实现-5" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight ceylon"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> EncoderBlock(nn.Module):</span><br><span class="line">    def <span class="number">__</span>init<span class="number">__</span>(self, embed<span class="number">_</span>size: int, hidden<span class="number">_</span>size: int, num<span class="number">_</span>heads: int, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""Initialize the encoder block layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            hidden_size: hidden size</span></span><br><span class="line"><span class="string">            num_heads: number of heads</span></span><br><span class="line"><span class="string">            dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">super</span>(EncoderBlock, self).<span class="number">__</span>init<span class="number">__</span>()</span><br><span class="line">        self.multi<span class="number">_</span>head<span class="number">_</span>attention = MultiHeadAttentionBlock(embed<span class="number">_</span>size, num<span class="number">_</span>heads, dropout)</span><br><span class="line">        self.feed<span class="number">_f</span>orward = FeedForwardBlock(embed<span class="number">_</span>size, hidden<span class="number">_</span>size, dropout)</span><br><span class="line">        self.residual<span class="number">_</span>connections = nn.ModuleList([ResidualConnection(dropout) <span class="keyword">for</span> <span class="number">_</span> <span class="keyword">in</span> range(<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, src<span class="number">_m</span>ask: torch.Tensor = None) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies encode block layer followed by a feed-forward layer and a residual connection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            src_mask: optional mask tensor of shape (batch_size, 1, 1, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.residual<span class="number">_</span>connections[<span class="number">0</span>](x, lambda x: self.multi<span class="number">_</span>head<span class="number">_</span>attention(x, x, x, src<span class="number">_m</span>ask))</span><br><span class="line">        x = self.residual<span class="number">_</span>connections[<span class="number">1</span>](x, self.feed<span class="number">_f</span>orward)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a><strong>Encoder</strong></h3><p><strong>Encoder</strong>是Transformer模型中的编码器部分，它由多个<strong>Encoder Block</strong>串联组成。</p><h4 id="数学原理-4"><a href="#数学原理-4" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Encoder</strong>的数学原理如下：<br>$$<br>\text{Encoder}(x) = \text{Layer Normalization}(\text{Encoder Block}(\text{Encoder Block}(…(\text{Encoder Block}(x))…)))<br>$$</p><ul><li>Encoder Block：编码器串联多个Encoder Block提取序列特征，原始Transformer模型中使用了6个Encoder Block。</li><li>Layer Normalization：对Encoder Block输出进行归一化，以解决特征迁移问题。</li></ul><h4 id="代码实现-6"><a href="#代码实现-6" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight ceylon"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Encoder(nn.Module):</span><br><span class="line">    def <span class="number">__</span>init<span class="number">__</span>(self, embed<span class="number">_</span>size: int, hidden<span class="number">_</span>size: int, num<span class="number">_</span>heads: int, num<span class="number">_</span>layers: int, dropout: float = <span class="number">0.1</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the encoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            hidden_size: hidden size</span></span><br><span class="line"><span class="string">            num_heads: number of heads</span></span><br><span class="line"><span class="string">            num_layers: number of encoder blocks</span></span><br><span class="line"><span class="string">            dropout: dropout rate, defaults to 0.1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">super</span>(Encoder, self).<span class="number">__</span>init<span class="number">__</span>()</span><br><span class="line">        self.encoder<span class="number">_</span>blocks = nn.ModuleList([EncoderBlock(embed<span class="number">_</span>size, hidden<span class="number">_</span>size, num<span class="number">_</span>heads, dropout) <span class="keyword">for</span> <span class="number">_</span> <span class="keyword">in</span> range(num<span class="number">_</span>layers)])</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies encode layer followed by a stack of encoder blocks and a layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            mask: optional mask tensor of shape (batch_size, 1, 1, seq_len)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> encoder<span class="number">_</span>block <span class="keyword">in</span> self.encoder<span class="number">_</span>blocks:</span><br><span class="line">            x = encoder<span class="number">_</span>block(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></tbody></table></figure><h3 id="Decoder-Block"><a href="#Decoder-Block" class="headerlink" title="Decoder Block"></a><strong>Decoder Block</strong></h3><p><strong>Decoder Block</strong>是Transformer模型的核心组成部分，它由三个子模块组成：<strong>Multi-Head Attention</strong>，<strong>Feed Forward Block</strong>和<strong>Residual Connection</strong>，其中<strong>Multi-Head Attention</strong>模块又根据输入的序列特征不同分成两个实例模块，一个是<strong>self-attention</strong>模块，一个是<strong>cross-attention</strong>模块。</p><p><strong>self-attention</strong>模块用于处理<strong>Decoder Block</strong>的输入序列（实际上为模型的输出序列），而<strong>cross-attention</strong>模块用于处理<strong>Encoder</strong>输出的序列特征和<strong>self-attention</strong>模块处理后的序列特征。</p><h4 id="数学原理-5"><a href="#数学原理-5" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Decoder Block</strong>的数学原理如下：<br>$$<br>\text{Decoder Block}(x) =<br>\begin{cases}<br>\text{Residual Connection}(x, \text{Multi-Head Attention}(x, x, x, tgt_mask)) &amp; step1 \<br>\text{Residual Connection}(x, \text{Multi-Head Attention}(x, enc_output, enc_output, src_mask)) &amp; step2 \<br>\text{Residual Connection}(x, \text{Feed Forward Block}(x) &amp; step3<br>\end{cases}<br>$$</p><ul><li>Multi-Head Attention：多头注意力机制，用于提取序列特征。</li><li>Feed Forward Network：用于对输入进行非线性变换，增加模型的表达能力。</li><li>Residual Connection：残差连接，用于实现梯度 vanishing 和 exploding 的问题。</li></ul><h4 id="代码实现-7"><a href="#代码实现-7" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">DecoderBlock</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">embed_size</span>: <span class="title">int</span>, <span class="title">hidden_size</span>: <span class="title">int</span>, <span class="title">num_heads</span>: <span class="title">int</span>, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        super(<span class="type">DecoderBlock</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        self.self_attention_block = <span class="type">MultiHeadAttentionBlock</span>(<span class="title">embed_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.cross_attention_block = <span class="type">MultiHeadAttentionBlock</span>(<span class="title">embed_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.feed_forward = <span class="type">FeedForwardBlock</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.residual_connections = nn.<span class="type">ModuleList</span>([<span class="type">ResidualConnection(dropout)</span> <span class="title">for</span> <span class="title">_</span> <span class="title">in</span> <span class="title">range</span>(3)])</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">enc_output</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Applies</span> the decoder block.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            x: input tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            enc_output: encoder output tensor of shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            src_mask: source mask tensor of shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask: target mask tensor of shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            output tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        x = self.residual_connections[0](<span class="title">x</span>, <span class="title">lambda</span> <span class="title">x</span>: <span class="title">self</span>.<span class="title">self_attention_block</span>(<span class="title">x</span>, <span class="title">x</span>, <span class="title">x</span>, <span class="title">tgt_mask</span>))</span></span><br><span class="line"><span class="class">        x = self.residual_connections[1](<span class="title">x</span>, <span class="title">lambda</span> <span class="title">x</span>: <span class="title">self</span>.<span class="title">cross_attention_block</span>(<span class="title">x</span>, <span class="title">enc_output</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>))</span></span><br><span class="line"><span class="class">        x = self.residual_connections[2](<span class="title">x</span>, <span class="title">self</span>.<span class="title">feed_forward</span>)</span></span><br><span class="line"><span class="class">        return x</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a><strong>Decoder</strong></h3><p><strong>Decoder</strong>是Transformer模型的解码器部分，它由多个<strong>Decoder Block</strong>串联组成。</p><h4 id="数学原理-6"><a href="#数学原理-6" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Decoder</strong>的数学原理如下：<br>$$<br>\text{Decoder}(x) = \text{Layer Normalization}(\text{Decoder Block}(\text{Decoder Block}(…(\text{Decoder Block}(x))…)))<br>$$</p><ul><li>Decoder Block：解码器串联多个Decoder Block提取序列特征，原始Transformer模型中使用了6个Decoder Block。</li><li>Layer Normalization：对Decoder Block输出进行归一化，以解决特征迁移问题。</li></ul><h4 id="代码实现-8"><a href="#代码实现-8" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Decoder</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">embed_size</span>: <span class="title">int</span>, <span class="title">hidden_size</span>: <span class="title">int</span>, <span class="title">num_heads</span>: <span class="title">int</span>, <span class="title">num_layers</span>: <span class="title">int</span>, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        super(<span class="type">Decoder</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        self.decoder_blocks = nn.<span class="type">ModuleList</span>([<span class="type">DecoderBlock</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">dropout</span>) for _ in range(<span class="title">num_layers</span>)])</span></span><br><span class="line"><span class="class">        self.norm = <span class="type">LayerNormalization</span>()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">enc_output</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Applies</span> the decoder by a stack of decoder blocks and a layer normalization.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            x: input tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">            enc_output: encoder output tensor of shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>) as the input of decoder</span></span><br><span class="line"><span class="class">            src_mask: source mask tensor of shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask: target mask tensor of shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            output tensor of shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        for decoder_block in self.decoder_blocks:</span></span><br><span class="line"><span class="class">            x = decoder_block(<span class="title">x</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>, <span class="title">tgt_mask</span>)</span></span><br><span class="line"><span class="class">        return self.norm(<span class="title">x</span>)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Projection-Layer"><a href="#Projection-Layer" class="headerlink" title="Projection Layer"></a><strong>Projection Layer</strong></h3><p><strong>Projection Layer</strong>是Transformer模型的输出层，作用是解码器的输出序列特征映射到词汇表大小，以生成最终的预测结果。<br>参数有两个：</p><ul><li>vocab_size：词汇表大小。</li><li>embed_size：嵌入向量的维度。</li></ul><h4 id="数学原理-7"><a href="#数学原理-7" class="headerlink" title="数学原理"></a>数学原理</h4><p><strong>Projection Layer</strong>的数学原理如下：<br>$$<br>\text{Projection Layer}(x) = \text{log_softmax}(\text{Linear}(x))<br>$$</p><h4 id="代码实现-9"><a href="#代码实现-9" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight julia"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class ProjectionLayer(nn.<span class="built_in">Module</span>):</span><br><span class="line">    def __init__(self, embed_size: int, vocab_size: int):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initializes the projection layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            embed_size: embedding size</span></span><br><span class="line"><span class="string">            vocab_size: vocabulary size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ProjectionLayer, self).__init__()</span><br><span class="line">        <span class="comment"># assert embed_size == vocab_size, "Embedding size and vocabulary size must be equal"</span></span><br><span class="line">        self.projection = nn.Linear(embed_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies the projection layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input tensor of shape (batch_size, seq_len, embed_size)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output tensor of shape (batch_size, seq_len, vocab_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_size) -&gt; (batch_size, seq_len, vocab_size)</span></span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(self.projection(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a><strong>Transformer</strong></h3><p><strong>Transformer</strong>的模型可以由上述模块组成，它由<strong>Input Embedding</strong>、<strong>Positional Encoding</strong>、<strong>Encoder</strong>、<strong>Decoder</strong>和<strong>Projection Layer</strong>组成。</p><h4 id="代码实现-10"><a href="#代码实现-10" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight haskell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Transformer</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">src_vocab_size</span>: <span class="title">int</span>, <span class="title">tgt_vocab_size</span>: <span class="title">int</span>, <span class="title">src_seq_len</span>: <span class="title">int</span>, <span class="title">tgt_seq_len</span>: <span class="title">int</span>, <span class="title">embed_size</span>: <span class="title">int</span> = 512, <span class="title">hidden_size</span>: <span class="title">int</span> = 2048, <span class="title">num_heads</span>: <span class="title">int</span> = 8, <span class="title">num_encoder_layers</span>: <span class="title">int</span> = 6, <span class="title">num_decoder_layers</span>: <span class="title">int</span>=6, <span class="title">dropout</span>: <span class="title">float</span> = 0.1):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Initializes</span> the transformer model. <span class="type">The</span> model consists of an encoder and a decoder.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            src_vocab_size: source vocabulary size</span></span><br><span class="line"><span class="class">            tgt_vocab_size: target vocabulary size</span></span><br><span class="line"><span class="class">            src_seq_len: source sequence length</span></span><br><span class="line"><span class="class">            tgt_seq_len: target sequence length</span></span><br><span class="line"><span class="class">            embed_size: embedding size</span></span><br><span class="line"><span class="class">            hidden_size: hidden size</span></span><br><span class="line"><span class="class">            num_heads: number of attention heads</span></span><br><span class="line"><span class="class">            num_encoder_layers: number of encoder layers</span></span><br><span class="line"><span class="class">            num_decoder_layers: number of decoder layers</span></span><br><span class="line"><span class="class">            dropout: dropout rate</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        super(<span class="type">Transformer</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> input embedding layer</span></span><br><span class="line"><span class="class">        self.src_embedding = <span class="type">InputEmbedding</span>(<span class="title">src_vocab_size</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        self.tgt_embedding = <span class="type">InputEmbedding</span>(<span class="title">tgt_vocab_size</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> positional encoding layers</span></span><br><span class="line"><span class="class">        self.src_pos_encoding = <span class="type">PositionalEncoding</span>(<span class="title">embed_size</span>, <span class="title">src_seq_len</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.tgt_pos_encoding = <span class="type">PositionalEncoding</span>(<span class="title">embed_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Create</span> encoder and decoder</span></span><br><span class="line"><span class="class">        self.encoder = <span class="type">Encoder</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">num_encoder_layers</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.decoder = <span class="type">Decoder</span>(<span class="title">embed_size</span>, <span class="title">hidden_size</span>, <span class="title">num_heads</span>, <span class="title">num_decoder_layers</span>, <span class="title">dropout</span>)</span></span><br><span class="line"><span class="class">        self.projection = <span class="type">ProjectionLayer</span>(<span class="title">embed_size</span>, <span class="title">tgt_vocab_size</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Initialize</span> the parameters</span></span><br><span class="line"><span class="class">        for p in self.parameters():</span></span><br><span class="line"><span class="class">            if p.dim() &gt; 1:</span></span><br><span class="line"><span class="class">                nn.init.xavier_uniform_(<span class="title">p</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">src</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">src_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>, <span class="title">tgt_mask</span>: <span class="title">torch</span>.<span class="type">Tensor</span>) -&gt; torch.<span class="type">Tensor</span>:</span></span><br><span class="line"><span class="class">        """<span class="type">Performs</span> a forward pass through the transformer model.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Args</span>:</span></span><br><span class="line"><span class="class">            src (<span class="title">torch</span>.<span class="type">Tensor</span>): source input, shape (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt (<span class="title">torch</span>.<span class="type">Tensor</span>): target input, shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class">            src_mask (<span class="title">torch</span>.<span class="type">Tensor</span>): source mask, shape (<span class="title">batch_size</span>, 1, 1, <span class="title">src_seq_len</span>)</span></span><br><span class="line"><span class="class">            tgt_mask (<span class="title">torch</span>.<span class="type">Tensor</span>): target mask, shape (<span class="title">batch_size</span>, 1, <span class="title">tgt_seq_len</span>, <span class="title">tgt_seq_len</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        <span class="type">Returns</span>:</span></span><br><span class="line"><span class="class">            torch.<span class="type">Tensor</span>: output of the transformer model, shape (<span class="title">batch_size</span>, <span class="title">tgt_seq_len</span>, <span class="title">tgt_vocab_size</span>)</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        # <span class="type">Encoder</span></span></span><br><span class="line"><span class="class">        # (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>) -&gt; (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        src = self.src_embedding(<span class="title">src</span>)</span></span><br><span class="line"><span class="class">        # (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>) -&gt; (<span class="title">batch_size</span>, <span class="title">src_seq_len</span>, <span class="title">embed_size</span>)</span></span><br><span class="line"><span class="class">        src = self.src_pos_encoding(<span class="title">src</span>)</span></span><br><span class="line"><span class="class">        enc_output = self.encoder(<span class="title">src</span>, <span class="title">src_mask</span>)</span></span><br><span class="line"><span class="class">        # <span class="type">Decoder</span></span></span><br><span class="line"><span class="class">        tgt = self.tgt_embedding(<span class="title">tgt</span>)</span></span><br><span class="line"><span class="class">        tgt = self.tgt_pos_encoding(<span class="title">tgt</span>)</span></span><br><span class="line"><span class="class">        output = self.decoder(<span class="title">tgt</span>, <span class="title">enc_output</span>, <span class="title">src_mask</span>, <span class="title">tgt_mask</span>)</span></span><br><span class="line"><span class="class">        # projection</span></span><br><span class="line"><span class="class">        return self.projection(<span class="title">output</span>)</span></span><br></pre></td></tr></tbody></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Epoch</span> <span class="number">0</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">0</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=5.7891]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">1</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">1</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=4.3471]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">2</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">2</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.49it/s, loss=4.2629]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">3</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">3</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:02&lt;00:00,  5.50it/s, loss=5.1802]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">4</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">4</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:01&lt;00:00,  5.50it/s, loss=4.5534]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">5</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">5</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:01&lt;00:00,  5.50it/s, loss=3.7544]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">6</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">6</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=2.6769]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">7</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">7</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:59&lt;00:00,  5.51it/s, loss=3.1740]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">8</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">8</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:59&lt;00:00,  5.52it/s, loss=1.5529]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">9</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">9</span>: <span class="number">100</span>%|█████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=1.5789]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">10</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">10</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [11:00&lt;00:00,  5.51it/s, loss=2.2088]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">11</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">11</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=1.8507]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">12</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">12</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:58&lt;00:00,  5.53it/s, loss=1.6555]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">13</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">13</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=1.7904]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">14</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">14</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:57&lt;00:00,  5.53it/s, loss=2.1260]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">15</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">15</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.6503]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">16</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">16</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.1801]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">17</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">17</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.6803]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">18</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">18</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=2.0962]</span></span><br><span class="line"><span class="attribute">Epoch</span> <span class="number">19</span></span><br><span class="line"><span class="attribute">Processing</span> epoch  <span class="number">19</span>: <span class="number">100</span>%|████| <span class="number">3638</span>/<span class="number">3638</span><span class="meta"> [10:56&lt;00:00,  5.54it/s, loss=1.8696]</span></span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文详细介绍&lt;a href=&quot;https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf&quot;&gt;Attention Is All You Need&lt;/a&gt;论文中Transformer模型架构、各个模块的数学原理、输入输出、参数规模，以及PyTorch实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Roadmap To Learn Large Language Model Of GPT Series</title>
    <link href="http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/"/>
    <id>http://blog.zhaoyongsheng.com/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/</id>
    <published>2024-12-21T08:33:09.000Z</published>
    <updated>2024-12-21T08:33:09.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文主要是OpenAI的GPT系列论文的阅读笔记。</p><span id="more"></span><h2 id="GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="GPT-1: Improving Language Understanding by Generative Pre-Training"></a>GPT-1: Improving Language Understanding by Generative Pre-Training</h2><p>GPT-1的论文在2018年6月发布，GPT-1的模型参数量达到了1.17亿，训练数据量达到了40GB，训练成本达到了12.5万美元。</p><h2 id="GPT-2-Language-Models-are-Unsupervised-Multitask-Learners"><a href="#GPT-2-Language-Models-are-Unsupervised-Multitask-Learners" class="headerlink" title="GPT-2: Language Models are Unsupervised Multitask Learners"></a>GPT-2: Language Models are Unsupervised Multitask Learners</h2><p>GPT-2的论文在2018年11月发布，GPT-2的模型参数量达到了1.5亿，训练数据量达到了1.5TB，训练成本达到了100万美元。</p><h2 id="GPT-3-Language-Models-are-Few-Shot-Learners"><a href="#GPT-3-Language-Models-are-Few-Shot-Learners" class="headerlink" title="GPT-3: Language Models are Few-Shot Learners"></a>GPT-3: Language Models are Few-Shot Learners</h2><p>GPT-3的论文在2021年12月发布，GPT-3的模型参数量达到了1750亿，训练数据量达到了45TB，训练成本达到了1200万美元。GPT-3主要解决的是语言模型中的少样本学习问题。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>In-Context Learning<br>In-Context Learning (ICL)是将预训练模型的文本输入作为一种任务具象化的形式，将自然语言指令或者示例作为条件，然后预测后面输出的文本。</li><li>Zero-shot VS. One-shot VS. Few-shot Learning<br>取决于模型推理时输入的示例数量。</li></ul><p><img alt="In-Context示例数量、模型大小与模型准确度的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/In-Context-Examples-Accuracy.png"><br>从上图可以的出结论：</p><ol><li>模型参数量越大，模型的准确度越高。</li><li>不需要梯度训练和微调，仅仅提供更多的In-Context示例，也能显著提升模型的准确度。</li><li>模型参数量越大，In-Context示例数量的边际收益越大。</li></ol><ul><li>模型参数量<br>GPT-3最大模型的参数量是175B，还有125M和13B的小模型。</li></ul><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>GPT-3的预训练方法包括模型、数据和训练与GPT-2的预训练方法一致，仅仅是提升了模型和数据的规模和复杂度，并延长了训练时间。有以下四个点需要重点讨论：</p><ul><li><ol><li>Fine-tuning<br>Fine-tuning的好处是通过监督样本微调，可以显著提升在指定基准任务上的表现。<br>Fine-tuning的缺点是针对每一个任务，都需要准备数量足够多的监督样本，会降低模型在监督数据分布之外的泛化能力。<br>GPT-3更关注任务无关的性能表现，因此没有使用Fine-tuning。</li></ol></li><li><ol start="2"><li>Few-Shot Learning<br>Few-Shot Learning指的是在模型推理阶段提供几个示例作为条件，并没有用这几个示例做微调和训练。<br>一个完整的示例包含了上下文和期望输出。<br>示例的数量从10到100不等，取决于GPT-3模型的上下文长度。<br>Few-Shot Learning的优点是显著降低了对于任务相关数据的依赖，并避免了模型受窄分布数据影响而泛化性降低的问题。</li></ol></li><li><ol start="3"><li>One-Shot Learning<br>One-Shot Learning与Few-Shot Learning类似，在模型推理时只提供一个示例。它更接近于人类完成语言任务的方式。</li></ol></li><li><ol start="4"><li>Zero-Shot Learning<br>Zero-Shot Learning指的是在模型推理时没有提供任何示例作为条件，仅仅提供语言指令描述任务。</li></ol></li></ul><p><img alt="Few-Shot VS. One-Shot VS. Zero-Shot VS. Fine-tuning" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Few-One-Zero-Shot.png"></p><h4 id="Models-and-Architectures"><a href="#Models-and-Architectures" class="headerlink" title="Models and Architectures"></a>Models and Architectures</h4><p>GPT-3的模型架构与GPT-2的模型架构类似，包括参数初始化方法、预正则化方法、可逆的Token化方法，不同的是Transformer中的Attention模式。GPT-3的Attention模式与Sparse Transformer类似。<br>为了验证不同模型大小对性能的影响，GPT-3总共训练了3个参数量级8个模型，具体如下图所是：<br><img alt="GPT-3模型参数量" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/GPT-3-Model-Parameters.png"><br>GPT-3使用的Context Window为2048。<br>$$embed_size = n_{heads} \times d_{head}$$<br>前向神经网络的参数量是Transformer特征向量的4倍，$d_{ff} = 4 \times embed_size$。</p><h4 id="Training-Datasets"><a href="#Training-Datasets" class="headerlink" title="Training Datasets"></a>Training Datasets</h4><p>用于训练GPT-3的数据集是Common Crawl,共计包含1TB字数的文本数据。为了保障数据集的质量做了如下处理：</p><ul><li><ol><li>根据与高质量语料的相似度进行过滤</li></ol></li><li><ol start="2"><li>在文件层面进行了模糊去重</li></ol></li><li><ol start="3"><li>添加了一些高质量的参考语料，包括扩展版本的WebText数据集、两个互联网书库（Books1和Book2）以及英文版的维基百科。<br>Common Crawl数据集涵盖了2016-2019年的Web数据，过滤前的文本数据约45TB，过滤后有570GB，约有4000亿个Tokens。<br>在训练的时候，质量越高的数据被采样的概率越高，共计用了3000亿个Tokens进行训练（Common Crawl数据集的采样率小于1）。</li></ol></li></ul><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>大参数量的模型需要使用大的Batch Size和小的Learning Rate进行训练。<br>GPT-3是使用V100集群进行分布式训练的，使用了混合模型并行化的方式来节省内存，包括矩阵相乘并行化和层间并行化。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>当模型结构、训练数据和训练方法高效时，模型的性能表现与算力成正相关。</p><h4 id="Language-Modeling-Cloze-and-Completion-Tasks"><a href="#Language-Modeling-Cloze-and-Completion-Tasks" class="headerlink" title="Language Modeling, Cloze, and Completion Tasks"></a>Language Modeling, Cloze, and Completion Tasks</h4><h5 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h5><p>GPT-3仅仅使用Zero-shot Learning进行语言建模,就在PTB数据集上拿到了SOTA。</p><h5 id="LAMBDA"><a href="#LAMBDA" class="headerlink" title="LAMBDA"></a>LAMBDA</h5><p>该数据集预测一段话最后一个字，用此用于测试长范围的语言模型性能。<br>该数据集的测试分析表明，随着模型参数量的增加，模型的Few-Shot性能会显著提升。<br><img alt="模型性能与参数量的关系" data-src="/2024/12/21/Roadmap-To-Learn-Large-Language-Model-Of-GPT-Series/Performance-wrt-Model-Size.png"></p><h4 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h4><p>GPT-2由于模型能力的限制，只在英文数据集上进行了训练。GPT-3的训练数据集有93%的英文数据，有7%的其他语言数据，包括法语、德语和罗马语。</p><h4 id="Winograd-Style-Tasks"><a href="#Winograd-Style-Tasks" class="headerlink" title="Winograd-Style Tasks"></a>Winograd-Style Tasks</h4><p>Winograd-Style Tasks是NLP中的经典任务，主要是根据上下文的语义决定代词的含义。</p><h4 id="Common-Sense-Reasoning"><a href="#Common-Sense-Reasoning" class="headerlink" title="Common Sense Reasoning"></a>Common Sense Reasoning</h4><ul><li>PhysicalQA：物理知识推理</li><li>ARC：3-9年级科学考试中的多选题</li><li>OpenBookQA：阅读理解</li></ul><h4 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h4><h4 id="SuperGLUE"><a href="#SuperGLUE" class="headerlink" title="SuperGLUE"></a>SuperGLUE</h4><h4 id="Natural-Language-Inference"><a href="#Natural-Language-Inference" class="headerlink" title="Natural Language Inference"></a>Natural Language Inference</h4><h4 id="Synthetic-and-Qualitative-Tasks"><a href="#Synthetic-and-Qualitative-Tasks" class="headerlink" title="Synthetic and Qualitative Tasks"></a>Synthetic and Qualitative Tasks</h4><h5 id="Arithmetic"><a href="#Arithmetic" class="headerlink" title="Arithmetic"></a>Arithmetic</h5><h5 id="Word-Scrambling-and-Manipulation-Tasks"><a href="#Word-Scrambling-and-Manipulation-Tasks" class="headerlink" title="Word Scrambling and Manipulation Tasks"></a>Word Scrambling and Manipulation Tasks</h5><ul><li>Cycle letters in word</li><li>Anagrams of all but first and last character</li><li>Anagrams of all but first and last two characters</li><li>Random insertion in word</li><li>Reversed words</li></ul><h5 id="SAT-Analogies"><a href="#SAT-Analogies" class="headerlink" title="SAT Analogies"></a>SAT Analogies</h5><h5 id="News-Article-Generation"><a href="#News-Article-Generation" class="headerlink" title="News Article Generation"></a>News Article Generation</h5><p>通过测试人类是否能区分文章是人写的还是大模型写的来评估大模型在这方面的能力。175B的GPT-3生成的文章参与测试的80个人判断准确率仅52%，意味着GPT-3生成的文章已经基本达到人类水平。</p><h5 id="Correcting-English-Grammar"><a href="#Correcting-English-Grammar" class="headerlink" title="Correcting English Grammar"></a>Correcting English Grammar</h5><h4 id="Measuring-and-Preventing-Memorization-of-Benchmarks"><a href="#Measuring-and-Preventing-Memorization-of-Benchmarks" class="headerlink" title="Measuring and Preventing Memorization of Benchmarks"></a>Measuring and Preventing Memorization of Benchmarks</h4><p>由于训练模型的数据来自互联网，因此基准测试集中的测试数据很可能也包含在模型训练数据中。</p><h2 id="GPT-4-GPT-4-Technical-Report"><a href="#GPT-4-GPT-4-Technical-Report" class="headerlink" title="GPT-4: GPT-4 Technical Report"></a>GPT-4: GPT-4 Technical Report</h2><p>GPT-4是一个多模态大模型，可以同时处理文本和图像，并生成文本。<br>GPT-4是使用RLHF（Reinforcement Learning from Human Feedback）进行微调的。<br>GPT-4的贡献在于OpenAI开发了深度学习栈基础设置和最优化算法，让不同尺度的模型表现变得可以预测。基于这项技术，OpenAI可以以1/1000的算力训练和评估模型。<br>GPT-4训练的数据集来自2021年9月之前的数据。<br>Rule based reward models(RBRM)是GPT-4模型Zero-shot的分类器。这个模型在RLHF微调阶段提供额外的信号，来校准GPT-4的输出行为。<br>RBRM的输入包含三部分：1. Prompt；2. 模型的输出；3. 人类定义的评估规则集合。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul><li><a href="https://commoncrawl.org/get-started">Common Crawl</a><ul><li>Over 250 billion pages spanning 17 years.</li><li>Free and open corpus since 2007.</li><li>Cited in over 10,000 research papers.</li><li>3–5 billion new pages added each month.</li></ul></li><li><a href="https://paperswithcode.com/dataset/webtext">WebText</a><ul><li>WebText is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. The authors scraped all outbound links from Reddit which received at least 3 karma.</li><li>WebText contains the text subset of these 45 million links. It consists of over 8 million documents for a total of 40 GB of text.</li><li>All Wikipedia documents were removed from WebText since it is a common data source for other datasets.</li></ul></li><li><a href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a><ul><li>The release of a beta version of Open WebText – an open source effort to reproduce OpenAI’s WebText dataset.</li><li><a href="https://huggingface.co/datasets/Skylion007/openwebtext">Download</a></li></ul></li><li><a href="https://catalog.ldc.upenn.edu/LDC99T42">Penn TreeBank(PTB)</a><ul><li>The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC95T7) and Treebank-3 (LDC99T42) releases of PTB. Treebank-2 includes the raw text for each story. Three “map” files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER.</li></ul></li><li><a href="https://zenodo.org/records/2630551#.YFJVaWT7S_w">LAMBDA</a><ul><li>We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.</li></ul></li><li><a href="https://rowanzellers.com/hellaswag/">HellaSwag</a><ul><li>Pick the best ending to the context.</li><li>We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers.</li></ul></li><li><a href="https://huggingface.co/datasets/Salesforce/wikitext">WikiText-103</a><ul><li>The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.</li><li>Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger.</li><li>The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.</li></ul></li><li><a href="https://www.gutenberg.org/">Books1</a><ul><li>OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。</li></ul></li><li><a href="https://www.gutenberg.org/">Books2</a><ul><li>OpenAI用于训练GPT-3的图书数据集，因为版权原因，数据集为非开源。</li></ul></li><li><a href="https://huggingface.co/datasets/LSDSem/story_cloze">StoryCloze</a><ul><li>This dataset proposes a new framework for evaluating story understanding and script learning: the ‘Story Cloze Test’. This test requires a system to choose the correct ending to a four-sentence story. </li><li>It contains a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. </li><li>This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation.</li></ul></li><li><a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a><ul><li>To help spur development in open-domain question answering, we have created the Natural Questions (NQ) corpus, along with a challenge website based on this data.</li><li>The NQ corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.</li><li>The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.</li></ul></li><li><a href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">WebQuestions</a><ul><li>This dataset consists of 6,642 question/answer pairs.</li><li>The questions are supposed to be answerable by Freebase, a large knowledge graph.</li><li>The questions are mostly centered around a single named entity.</li><li>The questions are popular ones asked on the web (at least in 2013).</li></ul></li><li><a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a><ul><li>TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples.</li><li>TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.</li></ul></li><li><a href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">Winograd Schema Challenge</a><ul><li>The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.</li></ul></li><li><a href="https://winogrande.allenai.org/">Winograde</a><ul><li>WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.</li></ul></li><li><a href="">PhysicalQA</a><ul><li>PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP.</li></ul></li><li><a href="https://github.com/google-research-datasets/arc">ARC</a></li><li><a href="https://stanfordnlp.github.io/coqa/">CoQA</a></li><li><a href="https://rajpurkar.github.io/SQuAD-explorer/">QuAC</a></li><li><a href="https://github.com/google-research-datasets/drop">DROP</a></li><li><a href="">RACE</a></li><li><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/multinews.jsonl">MultiNews</a></li><li><a href="https://openbookqa.org/">OpenBookQA</a></li><li><a href="https://github.com/tliuhuan/COPA">COPA</a></li><li><a href="https://github.com/allenai/record">ReCoRD</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/boolq.jsonl">BoolQ</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/multiRC.jsonl">MultiRC</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/rte.jsonl">RTE</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/anli.jsonl">ANLI</a></li><li><a href="https://huggingface.co/datasets/webarchive/resolve/main/triviaqa.jsonl">TruthfulQA</a></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU（Bilingual Evaluation Understudy）是自然语言处理 (NLP) 中一种常用的评估指标，主要用于衡量机器翻译系统生成文本的质量，由IBM的研究人员Kenney和Manning在2002年提出。BLEU通过计算生成文本与参考文本之间的相似度来评估翻译的准确性和流畅性。，来评估机器翻译的质量。</p><p>以下是 BLEU 的核心思想及其计算方式：</p><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><ol><li><p><strong>N-gram 匹配</strong>：<br>BLEU 通过计算生成文本与参考文本之间的 n-gram 匹配情况来衡量质量。n-gram 是文本中连续 n 个词的序列（例如，”the cat” 是一个 2-gram）。BLEU 会分别计算 1-gram、2-gram、3-gram 等的匹配情况。</p></li><li><p>**精确率 (Precision)**：<br>BLEU 计算生成文本中 n-gram 与参考文本中 n-gram 的匹配比例。例如，如果生成文本包含 10 个 2-gram，而其中 6 个在参考文本中出现，则 2-gram 的精确率为 60%。</p></li><li><p>**惩罚机制 (Brevity Penalty)**：<br>为了避免生成系统通过过于简短的输出（例如，重复参考中的高频词）获得高分，BLEU 引入了长度惩罚。如果生成文本的长度明显短于参考文本，BLEU 分数会降低。</p></li></ol><h4 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h4><ol><li><p><strong>计算 n-gram 的精确率</strong>：<br>$$<br>P_n = \frac{\text{生成文本中与参考文本匹配的 n-gram 数}}{\text{生成文本中的 n-gram 总数}}<br>$$</p></li><li><p><strong>几何平均精确率</strong>：<br>对不同 n-gram（如 1-gram 到 4-gram）的精确率取几何平均：<br>$$<br>\text{Precision}<em>{\text{geometric mean}} = \left( \prod</em>{n=1}^{N} P_n \right)^{1/N}<br>$$</p></li><li><p>**长度惩罚 (Brevity Penalty, BP)**：<br>如果生成文本长度 ( c ) 小于参考文本长度 ( r )，引入惩罚因子：<br>$$<br>BP =<br>\begin{cases}<br>1 &amp; \text{如果 } c &gt; r \<br>e^{1 - r/c} &amp; \text{如果 } c \leq r<br>\end{cases}<br>$$</p></li><li><p><strong>最终 BLEU 分数</strong>：<br>将几何平均精确率和惩罚因子相结合：<br>$$<br>BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log P_n \right)<br>$$<br>其中 ( w_n ) 是 n-gram 的权重（通常均分，如 ( w_n = 1/N )）。</p></li></ol><h4 id="优点与局限性"><a href="#优点与局限性" class="headerlink" title="优点与局限性"></a>优点与局限性</h4><p><strong>优点</strong>：</p><ul><li>自动化评估，计算快速。</li><li>与参考翻译的直接比较，适用于多种语言。</li></ul><p><strong>局限性</strong>：</p><ol><li><strong>忽略语义</strong>：BLEU 只关心 n-gram 的表面匹配，不考虑语义。</li><li><strong>依赖参考翻译</strong>：如果参考翻译质量不好，BLEU 分数也会受到影响。</li><li><strong>长文本适用性弱</strong>：对段落或整篇文章的翻译，BLEU 可能无法全面反映质量。</li></ol><h3 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h3><p>BPE（Byte Pair Encoding）是一种基于数据压缩技术的<strong>子词分割算法</strong>，广泛应用于自然语言处理（NLP）任务中，尤其是神经机器翻译和预训练语言模型。其核心思想是通过递归地合并频率最高的字符对或子词对，逐步构建适应语料分布的子词单元，从而在减少词表大小和处理未登录词（OOV）问题之间取得平衡。</p><h4 id="为什么需要-BPE？"><a href="#为什么需要-BPE？" class="headerlink" title="为什么需要 BPE？"></a>为什么需要 BPE？</h4><p>传统的分词方法（如按词分割或按字符分割）存在以下问题：</p><ol><li><strong>按词分割</strong>：<ul><li>词表会非常大，难以处理。</li><li>对低频词表现不好，容易导致 OOV（Out-Of-Vocabulary，未登录词）。</li></ul></li><li><strong>按字符分割</strong>：<ul><li>词表很小，但上下文关系的学习难度大。</li><li>生成的序列太长，影响模型效率。</li></ul></li></ol><p><strong>BPE</strong> 通过将高频的字符序列合并为子词单元，既降低了词表大小，又能捕捉常见词的整体信息，解决了上述问题。</p><h4 id="BPE-的基本原理"><a href="#BPE-的基本原理" class="headerlink" title="BPE 的基本原理"></a>BPE 的基本原理</h4><p>BPE 的核心思想是<strong>基于频率的合并</strong>：从初始字符开始，将出现频率最高的字符对逐步合并为新的子词单元，直到达到预定义的分词单元数量或其他停止条件。</p><h5 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h5><ol><li><p><strong>初始化</strong>：</p><ul><li>将训练语料中的每个单词分解为字符序列，并在每个单词的末尾添加特殊标记（如 <code>▁</code> 表示词的开头）。</li><li>例如：<code>hello world</code> → <code>h e l l o ▁ w o r l d</code></li></ul></li><li><p><strong>统计字符对的频率</strong>：</p><ul><li>计算语料中每对相邻字符的频率。</li><li>例如：<code>h e l l o</code> 中，<code>he</code>、<code>el</code>、<code>ll</code>、<code>lo</code> 是字符对，统计其出现次数。</li></ul></li><li><p><strong>合并频率最高的字符对</strong>：</p><ul><li>找到频率最高的字符对，将其合并为新的子词单元。</li><li>例如：如果 <code>ll</code> 是频率最高的对，则合并为 <code>ll</code>，更新序列为：<code>h e ll o</code>.</li></ul></li><li><p><strong>重复步骤 2 和 3</strong>：</p><ul><li>每次合并后重新统计频率，直到达到预定义的子词单元数量。</li></ul></li><li><p><strong>生成子词词表</strong>：</p><ul><li>记录所有生成的子词单元，作为最终的分词词表。</li></ul></li></ol><h5 id="算法实现："><a href="#算法实现：" class="headerlink" title="算法实现："></a>算法实现：</h5><p>BPE 的核心可以表示为以下伪代码：</p><figure class="highlight livecodeserver"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">输入：语料 D，目标子词表大小 V</span><br><span class="line">初始化：将 D 分解为字符序列 S</span><br><span class="line"><span class="keyword">while</span> |S| &lt; V:</span><br><span class="line">    统计 S 中所有相邻字符对的频率</span><br><span class="line">    找到频率最高的字符对 <span class="keyword">a</span>, b</span><br><span class="line">    将 <span class="keyword">a</span>, b 合并为新的子词单元 ab</span><br><span class="line">    更新 S 中的所有出现</span><br><span class="line">输出：最终的子词词表 S</span><br></pre></td></tr></tbody></table></figure><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>假设我们对以下单词应用 BPE：<code>low</code>, <code>lowest</code>, <code>new</code>, <code>newer</code>.</p><ol><li><p><strong>初始状态</strong>：</p><figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l o w</span><br><span class="line">l o w e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>统计字符对频率</strong>：</p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">l</span> o: <span class="number">2</span>, o w: <span class="number">2</span>, w e: <span class="number">2</span>, e s: <span class="number">1</span>, s t: <span class="number">1</span>, n e: <span class="number">2</span>, e w: <span class="number">2</span>, w e: <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>合并频率最高的对</strong>：<br>假设 <code>o w</code> 频率最高，合并为 <code>ow</code>：</p>  <figure class="highlight excel"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l ow</span><br><span class="line">l ow e s <span class="built_in">t</span></span><br><span class="line"><span class="built_in">n</span> e w</span><br><span class="line"><span class="built_in">n</span> e w e r</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>重复以上步骤</strong>：</p><ul><li>合并后重新统计频率并合并，直到达到预定义的子词单元数量。</li></ul></li></ol><h4 id="BPE-在-NLP-中的应用"><a href="#BPE-在-NLP-中的应用" class="headerlink" title="BPE 在 NLP 中的应用"></a>BPE 在 NLP 中的应用</h4><ol><li><strong>机器翻译</strong>：如 Transformer 中，BPE 被用作标准分词方法，以减少 OOV 现象并提高模型的泛化能力。</li><li><strong>预训练模型</strong>：如 GPT 和 BERT，BPE 用于构建子词词表，兼顾词表大小与覆盖率。</li><li><strong>多语言任务</strong>：BPE 在多语言模型中尤其有效，能够生成语言无关的统一词表。</li></ol><h4 id="优点与局限性-1"><a href="#优点与局限性-1" class="headerlink" title="优点与局限性"></a>优点与局限性</h4><p><strong>优点</strong>：</p><ol><li><strong>词表大小灵活</strong>：通过限制词表大小 ( V )，可以在效率与表达能力之间取得平衡。</li><li><strong>处理未登录词（OOV）</strong>：将罕见词分解为子词单元，从而减少 OOV 的发生。</li><li><strong>语言无关性</strong>：BPE 是基于频率的统计方法，不依赖具体语言规则。</li></ol><p><strong>局限性</strong>：</p><ol><li><strong>固定词表</strong>：训练好的子词词表对新领域的适应性有限。</li><li><strong>上下文独立性</strong>：合并规则是基于全局频率的，而不考虑上下文。</li><li><strong>生成效率</strong>：在解码过程中，可能导致生成序列较长，影响模型推断速度。</li></ol><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>BPE 是一种简单而高效的分词算法，能够生成兼具词语信息和灵活性的子词单元，在 NLP 中广泛应用，是现代语言模型的关键预处理步骤之一。</p><h3 id="SuperGLUE基准测试集"><a href="#SuperGLUE基准测试集" class="headerlink" title="SuperGLUE基准测试集"></a>SuperGLUE基准测试集</h3><p>SuperGLUE（<strong>A Stickier Benchmark for General-Purpose Language Understanding Systems</strong>）是自然语言处理（NLP）领域中的一个基准测试集，用于评估模型在广泛的语言理解任务上的能力。它是对 GLUE（General Language Understanding Evaluation）的改进版，专门设计来挑战基于预训练语言模型（如 BERT、GPT）在多种语言理解任务上的性能，涵盖更复杂的推理和语义理解问题。</p><h4 id="SuperGLUE-的核心组成"><a href="#SuperGLUE-的核心组成" class="headerlink" title="SuperGLUE 的核心组成"></a><strong>SuperGLUE 的核心组成</strong></h4><p>SuperGLUE 包含 8 个不同的任务，每个任务对应一种语言理解的核心能力。以下是每个任务的详细说明：</p><h5 id="1-Boolean-Question-BoolQ"><a href="#1-Boolean-Question-BoolQ" class="headerlink" title="1. Boolean Question (BoolQ)"></a>1. <strong>Boolean Question (BoolQ)</strong></h5><ul><li><strong>任务类型</strong>：二分类任务（Yes/No）。</li><li><strong>目标</strong>：根据给定的段落和问题，判断答案是“是”还是“否”。</li><li><strong>输入</strong>：一个段落（context）和一个自然语言问题（question）。</li><li><strong>输出</strong>：布尔值（Yes/No）。</li><li><strong>数据来源</strong>：从真实用户查询生成的问题，例如维基百科。</li><li><strong>挑战</strong>：<ul><li>问题的答案需要通过复杂推理和跨句子语义整合来确定。</li></ul></li></ul><hr><h5 id="2-CommitmentBank-CB"><a href="#2-CommitmentBank-CB" class="headerlink" title="2. CommitmentBank (CB)"></a>2. <strong>CommitmentBank (CB)</strong></h5><ul><li><strong>任务类型</strong>：三分类任务（Entailment / Neutral / Contradiction）。</li><li><strong>目标</strong>：判断一个假设（hypothesis）是否从给定的文本段落（premise）中可以：<ol><li>推出（Entailment），</li><li>不确定（Neutral），</li><li>或矛盾（Contradiction）。</li></ol></li><li><strong>输入</strong>：一个前提句（premise）和一个假设句（hypothesis）。</li><li><strong>输出</strong>：三分类标签。</li><li><strong>数据来源</strong>：对语言学研究中的嵌套从句进行标注。</li><li><strong>挑战</strong>：<ul><li>涉及复杂的语义推理和句法关系。</li></ul></li></ul><hr><h5 id="3-Choice-of-Plausible-Alternatives-COPA"><a href="#3-Choice-of-Plausible-Alternatives-COPA" class="headerlink" title="3. Choice of Plausible Alternatives (COPA)"></a>3. <strong>Choice of Plausible Alternatives (COPA)</strong></h5><ul><li><strong>任务类型</strong>：因果推理（二选一）。</li><li><strong>目标</strong>：判断给定的句子是否是因果链中的原因或结果，并从两个备选答案中选择一个。</li><li><strong>输入</strong>：一个句子和两个备选答案。</li><li><strong>输出</strong>：选择一个更可能的答案（1 或 2）。</li><li><strong>数据来源</strong>：手动创建的因果推理数据。</li><li><strong>挑战</strong>：<ul><li>涉及因果关系的推理。</li><li>候选答案具有一定的迷惑性。</li></ul></li></ul><hr><h5 id="4-Multi-Sentence-Reading-Comprehension-ReCoRD"><a href="#4-Multi-Sentence-Reading-Comprehension-ReCoRD" class="headerlink" title="4. Multi-Sentence Reading Comprehension (ReCoRD)"></a>4. <strong>Multi-Sentence Reading Comprehension (ReCoRD)</strong></h5><ul><li><strong>任务类型</strong>：填空问题。</li><li><strong>目标</strong>：通过阅读长段落，选择填充问题空白的正确词语或短语。</li><li><strong>输入</strong>：一个长段落和包含空白的句子。</li><li><strong>输出</strong>：正确的词语或短语（从段落中选取）。</li><li><strong>数据来源</strong>：新闻语料库。</li><li><strong>挑战</strong>：<ul><li>涉及跨句子关系的理解。</li><li>候选答案可能具有语义相似性，增加了任务难度。</li></ul></li></ul><hr><h5 id="5-Reading-Comprehension-with-Commonsense-Reasoning-RTE"><a href="#5-Reading-Comprehension-with-Commonsense-Reasoning-RTE" class="headerlink" title="5. Reading Comprehension with Commonsense Reasoning (RTE)"></a>5. <strong>Reading Comprehension with Commonsense Reasoning (RTE)</strong></h5><ul><li><strong>任务类型</strong>：二分类任务（Entailment / Not Entailment）。</li><li><strong>目标</strong>：判断假设（hypothesis）是否可以从前提句（premise）中推出。</li><li><strong>输入</strong>：前提句和假设句。</li><li><strong>输出</strong>：二分类标签（Entailment / Not Entailment）。</li><li><strong>数据来源</strong>：多个现有的自然语言推理数据集。</li><li><strong>挑战</strong>：<ul><li>推理涉及隐含的背景知识和常识。</li></ul></li></ul><hr><h5 id="6-Winograd-Schema-Challenge-WiC"><a href="#6-Winograd-Schema-Challenge-WiC" class="headerlink" title="6. Winograd Schema Challenge (WiC)"></a>6. <strong>Winograd Schema Challenge (WiC)</strong></h5><ul><li><strong>任务类型</strong>：词义消歧。</li><li><strong>目标</strong>：判断某个词在两个句子中的含义是否相同。</li><li><strong>输入</strong>：两个句子和目标词。</li><li><strong>输出</strong>：布尔值（Yes/No）。</li><li><strong>数据来源</strong>：词义标注和上下文信息。</li><li><strong>挑战</strong>：<ul><li>涉及对多义词的准确语义判断。</li><li>考验模型对上下文细微差别的理解。</li></ul></li></ul><hr><h5 id="7-Winograd-Schema-Challenge-WSC"><a href="#7-Winograd-Schema-Challenge-WSC" class="headerlink" title="7. Winograd Schema Challenge (WSC)"></a>7. <strong>Winograd Schema Challenge (WSC)</strong></h5><ul><li><strong>任务类型</strong>：共指消解。</li><li><strong>目标</strong>：判断代词或名词短语的指代对象。</li><li><strong>输入</strong>：一个句子，包含一个指代目标和两个候选对象。</li><li><strong>输出</strong>：正确的指代对象。</li><li><strong>数据来源</strong>：手工创建的共指消解数据。</li><li><strong>挑战</strong>：<ul><li>考察模型对代词解析和语义常识的理解。</li></ul></li></ul><hr><h5 id="8-AX-b-BroadCoverage-Diagnostic"><a href="#8-AX-b-BroadCoverage-Diagnostic" class="headerlink" title="8. AX-b (BroadCoverage Diagnostic)"></a>8. <strong>AX-b (BroadCoverage Diagnostic)</strong></h5><ul><li><strong>任务类型</strong>：对文本片段进行语言现象的诊断评估。</li><li><strong>目标</strong>：分析模型在特定语言现象（如逻辑推理或世界知识推理）上的表现。</li><li><strong>输入和输出</strong>：与其他任务类似，具体设计用于测试模型的弱点。</li><li><strong>挑战</strong>：<ul><li>设计针对性强，旨在暴露模型局限。</li></ul></li></ul><h4 id="SuperGLUE-的评价方式"><a href="#SuperGLUE-的评价方式" class="headerlink" title="SuperGLUE 的评价方式"></a><strong>SuperGLUE 的评价方式</strong></h4><ol><li><strong>任务指标</strong>：<ul><li>每个任务有特定的评估指标（如准确率、F1 分数等）。</li></ul></li><li><strong>总体分数</strong>：<ul><li>通过所有任务的加权平均分数评估模型整体性能。</li></ul></li><li><strong>人类基准</strong>：<ul><li>超过 GLUE 基准的人类水平设置为 SuperGLUE 的参考基线。</li></ul></li></ol><h4 id="SuperGLUE-的意义"><a href="#SuperGLUE-的意义" class="headerlink" title="SuperGLUE 的意义"></a><strong>SuperGLUE 的意义</strong></h4><ul><li><strong>更高难度</strong>：SuperGLUE 设计了更复杂的语言理解任务，考验模型的推理能力、常识知识和语义理解。</li><li><strong>推动研究</strong>：提供了统一的评估框架，促进了更强大和通用的语言理解模型的开发。</li><li><strong>领域覆盖广</strong>：任务涉及多种语言理解能力，使模型需具备广泛的适应性。</li></ul><h4 id="模型挑战"><a href="#模型挑战" class="headerlink" title="模型挑战"></a><strong>模型挑战</strong></h4><p>虽然当前模型（如 T5、GPT-4 等）已经在部分任务上接近或超过人类水平，但在涉及常识推理、因果关系和细粒度语义分析的任务上，仍然存在改进空间。SuperGLUE 因此被视为推动 NLP 领域进步的重要基准之一。</p><h3 id="MMLU基准测试集"><a href="#MMLU基准测试集" class="headerlink" title="MMLU基准测试集"></a>MMLU基准测试集</h3><p><strong>MMLU (Massive Multitask Language Understanding)</strong> 是一个用于评估语言模型多任务能力的基准测试集。它专门设计用于测试大语言模型（如 GPT 系列）的 <strong>多任务学习（multitask learning）</strong> 和 <strong>通用语言理解能力（general language understanding）</strong>。MMLU 基准测试集涵盖了多种不同类型的任务，从常见的分类、推理任务到更加专业的知识领域（如医学、法律等）的应用，能够全面地评估语言模型在各种场景下的表现。</p><h4 id="MMLU-基准测试集的构成"><a href="#MMLU-基准测试集的构成" class="headerlink" title="MMLU 基准测试集的构成"></a><strong>MMLU 基准测试集的构成</strong></h4><p>MMLU 基准测试集包括多种类型的任务，以下是几个主要类别的任务和一些常见的子任务：</p><ol><li><p><strong>常识推理</strong>：</p><ul><li>测试语言模型在日常常识方面的理解。</li><li>示例任务：回答关于世界常识的问题（如历史事件、人物、科学常识等）。</li></ul></li><li><p><strong>数学与逻辑推理</strong>：</p><ul><li>包含算术、代数、几何等数学问题的解答。</li><li>示例任务：解决数学运算问题（例如基础的加减乘除，甚至更复杂的几何和代数问题）。</li></ul></li><li><p><strong>科学与技术理解</strong>：</p><ul><li>测试模型在物理、化学、生物学等科学领域的知识理解。</li><li>示例任务：解答关于物理学原理、化学反应、生物学概念等问题。</li></ul></li><li><p><strong>专业领域知识</strong>：</p><ul><li>涉及法律、医学、金融等专业领域的任务。</li><li>示例任务：法律案件分析、医学症状诊断、财务报告分析等。</li></ul></li><li><p><strong>语言学任务</strong>：</p><ul><li>涉及语法、语义分析和推理的任务。</li><li>示例任务：句法分析、语义关系提取、情感分析等。</li></ul></li><li><p><strong>阅读理解与推理</strong>：</p><ul><li>包括从文章或段落中提取信息并进行推理。</li><li>示例任务：阅读理解（给定一段文章后回答相关问题）和推理任务（基于文章进行逻辑推理）。</li></ul></li><li><p><strong>文化与历史</strong>：</p><ul><li>任务涉及对文化、历史、地理和人类社会的理解。</li><li>示例任务：回答历史事件的具体细节或文化现象的解读。</li></ul></li></ol><h4 id="MMLU的主要特点"><a href="#MMLU的主要特点" class="headerlink" title="MMLU的主要特点"></a><strong>MMLU的主要特点</strong></h4><ul><li><strong>多样性</strong>：MMLU 涵盖了从基础的语言理解任务到复杂的推理、专业知识领域的任务，涉及多个领域和知识层面。</li><li><strong>难度范围广</strong>：包含了从容易到困难不等的任务，帮助测试模型在不同难度级别下的能力。</li><li><strong>高质量问题</strong>：问题通常经过精心设计，涵盖了广泛的知识，确保测试的全面性和代表性。</li><li><strong>标准化的评估</strong>：通过统一的任务和标准化的评分体系，可以比较不同语言模型在同一基准上的表现。</li></ul><h4 id="MMLU基准测试集的评价标准"><a href="#MMLU基准测试集的评价标准" class="headerlink" title="MMLU基准测试集的评价标准"></a><strong>MMLU基准测试集的评价标准</strong></h4><p>MMLU 基准测试的性能通常使用 <strong>准确率（Accuracy）</strong> 来衡量。对于每个任务，模型给出的答案会根据是否正确进行评分，最终通过加权平均计算出各任务的综合准确率。</p><ul><li><strong>任务的加权</strong>：MMLU 通常会为不同类型的任务设定不同的权重，以确保最终的评分能够准确反映模型的综合能力。</li><li><strong>样本量</strong>：每个任务中的问题通常有多个样本，这些样本被设计为具有挑战性的，确保评估结果具有代表性。</li></ul><h4 id="MMLU的应用"><a href="#MMLU的应用" class="headerlink" title="MMLU的应用"></a><strong>MMLU的应用</strong></h4><p>MMLU 的设计使其成为测试大型语言模型多任务能力和理解深度的标准工具。它的应用主要体现在以下几个方面：</p><ol><li><strong>语言模型的综合能力评估</strong>：通过全面评估模型在多种任务中的表现，MMLU 可以帮助研究人员和工程师了解模型的通用语言理解能力。</li><li><strong>模型之间的比较</strong>：MMLU 提供了一个统一的测试标准，可以用来比较不同模型的性能，尤其是不同规模、架构的语言模型之间。</li><li><strong>多任务学习研究</strong>：MMLU 对于探索和优化多任务学习有重要意义。它帮助研究人员测试在多个任务上训练模型时的效果，评估模型如何从多个任务中进行泛化。</li></ol><h4 id="MMLU基准测试的例子"><a href="#MMLU基准测试的例子" class="headerlink" title="MMLU基准测试的例子"></a><strong>MMLU基准测试的例子</strong></h4><h5 id="例1：常识推理任务"><a href="#例1：常识推理任务" class="headerlink" title="例1：常识推理任务"></a><strong>例1：常识推理任务</strong></h5><p>任务：哪个国家位于北美洲？</p><ul><li>选项：A) 加拿大 B) 德国 C) 巴西 D) 中国</li><li>正确答案：A) 加拿大</li></ul><h5 id="例2：数学任务"><a href="#例2：数学任务" class="headerlink" title="例2：数学任务"></a><strong>例2：数学任务</strong></h5><p>任务：5x + 3 = 23, 求x的值。</p><ul><li>选项：A) 3 B) 4 C) 5 D) 2</li><li>正确答案：B) 4</li></ul><h5 id="例3：法律任务"><a href="#例3：法律任务" class="headerlink" title="例3：法律任务"></a><strong>例3：法律任务</strong></h5><p>任务：根据美国宪法，谁是最高法院的法官？</p><ul><li>选项：A) 总统 B) 最高法院首席法官 C) 美国国会 D) 各州州长</li><li>正确答案：B) 最高法院首席法官</li></ul><h5 id="例4：科学任务"><a href="#例4：科学任务" class="headerlink" title="例4：科学任务"></a><strong>例4：科学任务</strong></h5><p>任务：光合作用是如何发生的？</p><ul><li>选项：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖 B) 通过细胞呼吸 C) 通过蒸发 D) 通过化学反应</li><li>正确答案：A) 利用太阳光将水和二氧化碳转化为氧气和葡萄糖</li></ul><h4 id="MMLU的挑战和改进"><a href="#MMLU的挑战和改进" class="headerlink" title="MMLU的挑战和改进"></a><strong>MMLU的挑战和改进</strong></h4><ol><li><strong>任务设计的复杂性</strong>：随着语言模型变得越来越强大，测试集的任务设计也需要不断提升，以确保能够衡量模型的深度理解和推理能力。</li><li><strong>多样化的测试</strong>：MMLU 还可以进一步扩展，涵盖更多样化的领域和任务，尤其是跨模态任务（如图像、视频等）的结合。</li><li><strong>不平衡性</strong>：不同任务的难度不均衡，可能导致一些模型在某些任务上表现突出，而在另一些任务上表现较差。</li></ol><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h4><p>MMLU（Massive Multitask Language Understanding）是一个综合性强、覆盖面广的基准测试集，旨在评估语言模型在各种任务上的表现。它通过涉及常识推理、数学、科学、法律等多种任务，全面衡量模型的多任务学习能力，成为衡量大规模语言模型通用能力的重要工具。</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是OpenAI的GPT系列论文的阅读笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://blog.zhaoyongsheng.com/categories/Notes/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.zhaoyongsheng.com/categories/Notes/Deep-Learning/Papers/"/>
    
    
      <category term="GPT" scheme="http://blog.zhaoyongsheng.com/tags/GPT/"/>
    
      <category term="LLM" scheme="http://blog.zhaoyongsheng.com/tags/LLM/"/>
    
      <category term="NLP" scheme="http://blog.zhaoyongsheng.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Vision Servo Control For Robot Manipulation</title>
    <link href="http://blog.zhaoyongsheng.com/2024/10/02/Vision-Servo-Control-For-Robot-Manipulation/"/>
    <id>http://blog.zhaoyongsheng.com/2024/10/02/Vision-Servo-Control-For-Robot-Manipulation/</id>
    <published>2024-10-02T02:14:50.000Z</published>
    <updated>2024-10-02T02:14:50.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>视觉伺服是多自由度机械臂灵巧作业领域经典运动控制算法之一，常用于对精细作业和范化作业，例如利用视觉伺服实现对接线面板和USB接口的插拔。视觉伺服算法的核心思想是利用视觉反馈信息，对机械臂末端执行器进行实时位置和姿态调整，以实现精确的运动控制。视觉伺服通常分为两类，一类是基于图像特征的视觉伺服（IBVS），另一类是基于位置姿态的视觉伺服（PBVS）。本文将介绍这两种视觉伺服算法的基本原理和实现方法。</p><span id="more"></span><h2 id="背景理论"><a href="#背景理论" class="headerlink" title="背景理论"></a>背景理论</h2><p>视觉伺服本质上是一个在坐标转换（平移、旋转）运动空间的非线性优化问题，因此会用到<strong>Gauss-Newton</strong>非线性优化算法和<strong>Jacobian矩阵</strong>。</p><h3 id="非线性优化问题"><a href="#非线性优化问题" class="headerlink" title="非线性优化问题"></a>非线性优化问题</h3><p>将最优化问题的目标函数表示为$f(x)$，其中$f(x)$二阶连续可微，那么最优化的问题可以转化：<br>$${argmin}_{x}f(x)$$<br>即求解$f(x)$的最小值以及对应的$x^{*}$。<br>复杂的非线性最优化问题，$f(x)$一般为高维非线性方程组，$x$为多维向量，即$x \in \mathbb{R}^n$，$f(x) \in \mathbb{R}^m$，其中$m \geq n$。</p><p>对于机械臂视觉伺服运动控制问题，上述非线性优化问题可以表示为：<br>$$x = \begin{bmatrix}t &amp; r \end{bmatrix}$$<br>其中$t$表示坐标转换空间中的平移向量，维度为$[3x1]$，$r$表示坐标转换空间中的旋转向量，维度为$[3x1]$。<br>$$f(x) = x_{target} - x_{current}$$<br>或者<br>$$f(x) = x_{target}^{-1} x_{current}$$<br>公式选择取决于参考坐标系的选择，已经视觉伺服方式的选择，具体请见<strong>视觉伺服</strong>章节。</p><h3 id="Jacobian矩阵"><a href="#Jacobian矩阵" class="headerlink" title="Jacobian矩阵"></a>Jacobian矩阵</h3><p>对于复杂的非线性目标函数，在数学上直接求解解析解比较复杂，一般通过一级泰勒公式展开进行线性化近似，即：<br>$$f(x+\Delta x) \approx f(x) + J(x)\Delta x$$<br>其中，$J(x)$为$f(x)$在$x$处偏导数$\frac{\partial f(x)}{\partial x}$，称为Jacobian矩阵，$\Delta x$为增量。</p><p>假设$f(x) \in \mathbb{R}^m$，$x \in \mathbb{R}^n$，则雅可比矩阵表示的是$\mathbb{R}^n$空间到$\mathbb{R}^m$空间的线性映射，是一个$[m×n]$的矩阵，换句话讲其重要意义在于它表现了一个多变数向量函数的最佳线性逼近，公式为：<br>$$J(x) = \frac{\partial f(x)}{\partial x} =<br>\begin{bmatrix}<br>\frac{\partial f(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f(x)}{\partial x_{n}}<br>\end{bmatrix} =<br>\begin{bmatrix}<br>\frac{\partial f_{1}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{1}(x)}{\partial x_{n}} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial f_{m}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{m}(x)}{\partial x_{n}} \<br>\end{bmatrix}$$<br>一级泰勒公式展开线性逼近公式可以描述为矩阵形式：<br>$$<br>\begin{bmatrix}<br>f_{1}(x + \Delta x) \ \vdots \ f_{m}(x + \Delta x)<br>\end{bmatrix} =<br>\begin{bmatrix}<br>f_{1}(x) \ \vdots \ f_{m}(x)<br>\end{bmatrix} +<br>\begin{bmatrix}<br>\frac{\partial f_{1}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{1}(x)}{\partial x_{n}} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial f_{m}(x)}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial f_{m}(x)}{\partial x_{n}} \<br>\end{bmatrix}<br>\begin{bmatrix}<br>\Delta x_{1} \ \vdots \ \Delta x_{n}<br>\end{bmatrix}<br>$$</p><p>如果 $m = n$，那么雅可比矩阵是一个$[nxn]$方阵。于是我们可以取它的行列式，称为雅可比行列式。</p><ul><li>参考链接</li></ul><p><a href="https://zh.wikipedia.org/zh-hans/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5">Jacobian Matrix</a></p><p><a href="https://blog.csdn.net/gwplovekimi/article/details/104977255">雅各比矩阵和机械臂关节坐标转换</a></p><h3 id="Gauss-Newton非线性优化"><a href="#Gauss-Newton非线性优化" class="headerlink" title="Gauss-Newton非线性优化"></a>Gauss-Newton非线性优化</h3><p>对于大部分非线性优化问题，一般很难直接得到解析解。在数学上可以使用<strong>Gauss-Newton</strong>算法通过循环迭代地方式获取非线性方程组的数值解，其基本思想是从给定的初始值$x^{0}$开始，沿着梯度下降的方向，循环迭代直至目标函数达到或者接近最优值。</p><p>基于一级泰勒公式展开线性逼近方程，可以将最优化问题转换为关于$\Delta x$的线性最小二乘问题：<br>$$argmin_{x}\frac{1}{2} \Vert f(x) + J(x)\Delta x \Vert^2$$<br>其中，$J(x)$为$f(x)$在$x$处偏导数$\frac{\partial f(x)}{\partial x}$，称为Jacobian矩阵，$\Delta x$为增量。</p><p>注意：<strong>Gauss-Newton</strong>算法必须要转换成最小二乘问题进行求解。</p><p>求取上述公式相对于$\Delta x$的导数，并令导数等于0，即可得到最优解。推导过程如下所示：</p><p>已知公式：$\Vert X \Vert^2 = X^TX$，其中$X$为矩阵。</p><p>可以将最小二乘函数转化为：<br>$$<br>\begin{align}<br>\frac{1}{2} \Vert f(x) + J(x)\Delta x \Vert^2 &amp;= (f(x) + J(x)\Delta x)^T(f(x) + J(x)\Delta x) \<br>&amp;= f(x)^Tf(x) + \Delta x^TJ(x)^Tf(x) + f(x)^TJ(x)\Delta x + \Delta x^TJ(x)^TJ(x)\Delta x<br>\end{align}<br>$$<br>已知矩阵求导公式$\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x} = a$，$\frac{}{}$<br>$$</p><p>$$<br>迭代公式为：</p><ul><li>参考链接</li></ul><p><a href="https://zhuanlan.zhihu.com/p/482540286?utm_id=0">Bundle Adjustment 重投影误差模型及相应雅克比公式推导</a></p><p><a href="https://www.cnblogs.com/bingjianing/p/9093054.html">Jacobian矩阵、Hessian矩阵和Newton’s method</a></p><p><a href="https://loopvoid.github.io/2018/04/28/Jacobian%E7%9F%A9%E9%98%B5%E4%B8%8EHessian%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/">Jacobian矩阵与Hessian矩阵与最小二乘</a></p><p><a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导</a></p><h2 id="视觉伺服"><a href="#视觉伺服" class="headerlink" title="视觉伺服"></a>视觉伺服</h2><h3 id="基于位置的视觉伺服（PBVS）"><a href="#基于位置的视觉伺服（PBVS）" class="headerlink" title="基于位置的视觉伺服（PBVS）"></a>基于位置的视觉伺服（PBVS）</h3><h3 id="基于图像的视觉伺服（IBVS）"><a href="#基于图像的视觉伺服（IBVS）" class="headerlink" title="基于图像的视觉伺服（IBVS）"></a>基于图像的视觉伺服（IBVS）</h3></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;视觉伺服是多自由度机械臂灵巧作业领域经典运动控制算法之一，常用于对精细作业和范化作业，例如利用视觉伺服实现对接线面板和USB接口的插拔。视觉伺服算法的核心思想是利用视觉反馈信息，对机械臂末端执行器进行实时位置和姿态调整，以实现精确的运动控制。视觉伺服通常分为两类，一类是基于图像特征的视觉伺服（IBVS），另一类是基于位置姿态的视觉伺服（PBVS）。本文将介绍这两种视觉伺服算法的基本原理和实现方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Algorithms" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Algorithms/"/>
    
      <category term="Robotics" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Algorithms/Robotics/"/>
    
    
      <category term="Vision Servo" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Servo/"/>
    
      <category term="PBVS" scheme="http://blog.zhaoyongsheng.com/tags/PBVS/"/>
    
      <category term="IBVS" scheme="http://blog.zhaoyongsheng.com/tags/IBVS/"/>
    
  </entry>
  
  <entry>
    <title>Usage Of Testing Tool Locust</title>
    <link href="http://blog.zhaoyongsheng.com/2024/08/22/Usage-Of-Testing-Tool-Locust/"/>
    <id>http://blog.zhaoyongsheng.com/2024/08/22/Usage-Of-Testing-Tool-Locust/</id>
    <published>2024-08-22T02:06:02.000Z</published>
    <updated>2024-08-22T02:06:02.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>完成后端服务开发后，在业务上线之前，通常需要进行压力测试，以评估目前的服务器算力在不同QPS请求下的负载承受能力，以此做系统后端可靠性分析。<strong>Locust</strong>是基于Python接口的开源压力测试工具，本文将介绍如何使用该工具对后端服务器进行压力测试。</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>可以直接使用<strong>pip</strong>包管理工具进行安装，如下所示：</p><figure class="highlight cmake"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> locust</span><br></pre></td></tr></tbody></table></figure><p>需要将<strong>Locust</strong>安装目录添加到<strong>PATH</strong>中，这样可以直接在<strong>terminal</strong>中使用<strong>locust</strong>指令。</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.zshrc</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">PATH</span>=<span class="string">"/home/parallels/.local/bin:<span class="variable">$PATH</span>"</span></span><br></pre></td></tr></tbody></table></figure><p>安装完成后，可以通过如下指令查看并确认安装是否成功：</p><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust -V</span></span><br></pre></td></tr></tbody></table></figure><p>如果看到以下信息，则表明安装成功。</p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust</span> <span class="number">2</span>.<span class="number">31</span>.<span class="number">3</span> from /home/parallels/.local/lib/python3.<span class="number">10</span>/site-packages/locust (Python <span class="number">3</span>.<span class="number">10</span>.<span class="number">12</span>, OpenSSL <span class="number">3</span>.<span class="number">0</span>.<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="压测用例"><a href="#压测用例" class="headerlink" title="压测用例"></a>压测用例</h3><ul><li>HttpUser<br>首先创建一个<strong>locustfile.py</strong>文件，并实现一个继承<strong>HttpUser</strong>的自定义测试类，<strong>Locust</strong>程序在同目录启动后会默认执行该测试用例。<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> response</span><br><span class="line"><span class="keyword">from</span> locust <span class="keyword">import</span> HttpUser, task, between, constant</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ServoApiUser</span>(<span class="title class_ inherited__">HttpUser</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    User class that does requests to the locust web server running on localhost,</span></span><br><span class="line"><span class="string">    using the fast HTTP client</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    host = <span class="string">"https://www.yourwebsite.com"</span></span><br><span class="line">    wait_time = between(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="comment"># wait_time = constant(1)</span></span><br><span class="line">    <span class="comment"># some things you can configure on FastHttpUser</span></span><br><span class="line">    <span class="comment"># connection_timeout = 60.0</span></span><br><span class="line">    <span class="comment"># insecure = True</span></span><br><span class="line">    <span class="comment"># max_redirects = 5</span></span><br><span class="line">    <span class="comment"># max_retries = 1</span></span><br><span class="line">    <span class="comment"># network_timeout = 60.0</span></span><br><span class="line">    <span class="comment"># proxy_host = my-proxy.com</span></span><br><span class="line">    <span class="comment"># proxy_port = 8080</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_start</span>(<span class="params">self</span>):</span><br><span class="line">        self.client.headers = {</span><br><span class="line">            <span class="string">'Accept'</span>: <span class="string">'application/json'</span>,</span><br><span class="line">            <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>,</span><br><span class="line">            <span class="string">'Authorization'</span>: <span class="string">'TOKEN xxxxxxxxx'</span>,</span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'csrftoken=xxxxxxx'</span></span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line"><span class="meta">    @task</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_card_info</span>(<span class="params">self</span>):</span><br><span class="line">        url = <span class="string">"/api/card_info/?stage=pregnancy&amp;level=user"</span></span><br><span class="line">        response = self.client.get(url=url, name=<span class="string">'get_card_info'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"status code: {}"</span>.<span class="built_in">format</span>(response.status_code))</span><br><span class="line">        <span class="comment"># print("data: {}".format(response.json()))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># @task</span></span><br><span class="line">    <span class="comment"># def stats(self):</span></span><br><span class="line">    <span class="comment">#     self.client.get("/stats/requests")</span></span><br></pre></td></tr></tbody></table></figure><strong>host</strong>参数可以设置测试服务器的地址。<br><strong>wait_time</strong>参数可以设置每个用户实例完成一个task之后，执行下一个task需要的等待时间，<strong>constant</strong>为固定等待时间，<strong>between</strong>为随机等待时间。该参数可以模拟不同用户随机访问的情况。<br><strong>on_start</strong>函数会在测试任务启动时运行一次，通常用户配置一些频繁使用但是测试过程中保持不变的参数变量，例如header。<br><strong>task</strong>decorator用于定义一个具体的测试任务，通常为一个<strong>api</strong>接口的<strong>POST</strong>、<strong>GET</strong>、<strong>DELETE、</strong>等request请求任务，用于用户自定义测试任务。</li></ul><h3 id="压测"><a href="#压测" class="headerlink" title="压测"></a>压测</h3><p>在<strong>locustfile.py</strong>同目录的<strong>terminal</strong>输入以下指令，即可启动压测。</p><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">locust</span></span><br></pre></td></tr></tbody></table></figure><p>在浏览器中访问<strong><a href="http://0.0.0.0:8089/">http://0.0.0.0:8089</a></strong>即可开始压测。<br><strong>Number of Users</strong>用于配制压测的最高用户数量，配合<strong>wait_time</strong>即可估算出最高QPS<br>$$qps = \frac{UV}{wait_time}$$<br><strong>Ramp Up</strong>用户每秒钟用户增长的数量，用于测试访问用户不断增长情况下，服务器的性能表现。</p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;完成后端服务开发后，在业务上线之前，通常需要进行压力测试，以评估目前的服务器算力在不同QPS请求下的负载承受能力，以此做系统后端可靠性分析。&lt;strong&gt;Locust&lt;/strong&gt;是基于Python接口的开源压力测试工具，本文将介绍如何使用该工具对后端服务器进行压力测试。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Testing Tool" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Testing-Tool/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Testing Tool" scheme="http://blog.zhaoyongsheng.com/tags/Testing-Tool/"/>
    
  </entry>
  
  <entry>
    <title>Learning Note Of Vision Transformer</title>
    <link href="http://blog.zhaoyongsheng.com/2024/07/05/Learning-Note-Of-Vision-Transformer/"/>
    <id>http://blog.zhaoyongsheng.com/2024/07/05/Learning-Note-Of-Vision-Transformer/</id>
    <published>2024-07-04T16:18:01.000Z</published>
    <updated>2024-07-04T16:18:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Vision Transformer (ViT)<strong>是Transformer模型应用到视觉领域的开山之作,在分类识别领域取得了SOTA表现，在<a href="https://github.com/Alibaba-MIIL/ImageNet21K">ImageNet-21K</a>上训练的模型作为骨干基座，被广泛应用在了各个视觉任务中。本文主要介绍</strong>ViT</strong>的模型结构，<a href="https://github.com/jeonsworld/ViT-pytorch?tab=readme-ov-file">pytorch代码实现</a>和训练方法。</p><span id="more"></span><h2 id="ViT整体结构"><a href="#ViT整体结构" class="headerlink" title="ViT整体结构"></a>ViT整体结构</h2><p><img alt="Vision Transformer模型架构图" data-src="/2024/07/05/Learning-Note-Of-Vision-Transformer/vision_transformer.png"><br><strong>Vision Transformer</strong>是在<strong>Transformer</strong>基础之上构建的，关键是如何将二维图像转换为<strong>Transformer</strong>模型可以处理的序列特征。<strong>ViT</strong>首先将图像用固定大小的patch切分，每一个patch作为一个Token，学习一组Embedding特征，图片的多个patch按照自上而下、从左到右的顺序排列，构成序列。然后后在序列最后拼接一个可以学习的类别Token，用于分类任务。最后在序列Embedding特征中增加位置编码，以保留图片patch的位置信息。</p><h2 id="ViT核心模块"><a href="#ViT核心模块" class="headerlink" title="ViT核心模块"></a>ViT核心模块</h2><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>该模块是<strong>ViT</strong>模型的核心idea，主要是将二维图像转换为<strong>Transformer</strong>模型可以处理的序列特征。</p><ul><li><p>超参数 – patch_size，决定了将图像切分为patch的大小<br>图像序列特征的序列长度，即patch的数量，由输入图像的尺寸$[width, height]$和patch的尺寸共同决定：<br>$$seq_len = patch_num = \frac{width}{patch_size} * \frac{height}{patch_size}$$</p></li><li><p>超参数 – hidden_size，决定了embedding特征的长度</p></li><li><p>超参数 – dropout_rate，决定了embedding特征dropout的比率</p></li></ul><p>切分的patch图像通过二维卷积操作提取Embedding特征，拼接上类目编码之后再加上位置编码。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Construct the embeddings from patch, position embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, img_size, in_channels=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.hybrid = <span class="literal">None</span></span><br><span class="line">        img_size = _pair(img_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> config.patches.get(<span class="string">"grid"</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            grid_size = config.patches[<span class="string">"grid"</span>]</span><br><span class="line">            patch_size = (img_size[<span class="number">0</span>] // <span class="number">16</span> // grid_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // <span class="number">16</span> // grid_size[<span class="number">1</span>])</span><br><span class="line">            n_patches = (img_size[<span class="number">0</span>] // <span class="number">16</span>) * (img_size[<span class="number">1</span>] // <span class="number">16</span>)</span><br><span class="line">            self.hybrid = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            patch_size = _pair(config.patches[<span class="string">"size"</span>])</span><br><span class="line">            n_patches = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>]) * (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">            self.hybrid = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.hybrid:</span><br><span class="line">            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,</span><br><span class="line">                                         width_factor=config.resnet.width_factor)</span><br><span class="line">            in_channels = self.hybrid_model.width * <span class="number">16</span></span><br><span class="line">        self.patch_embeddings = Conv2d(in_channels=in_channels,</span><br><span class="line">                                       out_channels=config.hidden_size,</span><br><span class="line">                                       kernel_size=patch_size,</span><br><span class="line">                                       stride=patch_size)</span><br><span class="line">        self.position_embeddings = nn.Parameter(torch.zeros(<span class="number">1</span>, n_patches+<span class="number">1</span>, config.hidden_size))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, config.hidden_size))</span><br><span class="line"></span><br><span class="line">        self.dropout = Dropout(config.transformer[<span class="string">"dropout_rate"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># input x shape is [b,c,h,w]</span></span><br><span class="line">        B = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># cls_token shape: [1, 1, emb_len] ==&gt; [b, 1, emb_len]</span></span><br><span class="line">        cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.hybrid:</span><br><span class="line">            x = self.hybrid_model(x)</span><br><span class="line">        <span class="comment"># x shape: [b,c,h,w] ==&gt; [b, emb_len, h//patch_size, w//patch_size]</span></span><br><span class="line">        x = self.patch_embeddings(x)</span><br><span class="line">        <span class="comment"># x shape: [b, emb_len, h_p, w_p] ==&gt; [b, emb_len, h_p * w_p] ==&gt; [b, emb_len, n_patches]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, emb_len, n_patches] ==&gt; [b, n_patches, emb_len]</span></span><br><span class="line">        x = x.transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, n_patches, emb_len] ==&gt; [b, n_patches+1, emb_len]</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># x shape: [b, n_patches+1, emb_len] ==&gt; [b, n_patches+1, emb_len], add is auto broadcasting</span></span><br><span class="line">        embeddings = x + self.position_embeddings</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></tbody></table></figure><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><strong>Self-Attention</strong>模块是<strong>Transformer</strong>的核心基础模块。</p><ul><li>超参数 – num_heads，多头注意力机制的头数量</li><li>超参数 – hidden_size，决定了输入的embedding特征长度</li></ul><p><strong>hidden_size</strong>和<strong>num_heads</strong>共同决定了多头注意力机制每一头的特征长度。<br>$$attention_head_size = \frac{hidden_size}{num_heads}$$<br>一般需要确保上述公式是整除的。</p><ul><li>超参数 – attention_dropout_rate，决定了attention_score和output的dropout比率<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, vis</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.vis = vis</span><br><span class="line">        self.num_attention_heads = config.transformer[<span class="string">"num_heads"</span>]</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / self.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.out = Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.attn_dropout = Dropout(config.transformer[<span class="string">"attention_dropout_rate"</span>])</span><br><span class="line">        self.proj_dropout = Dropout(config.transformer[<span class="string">"attention_dropout_rate"</span>])</span><br><span class="line"></span><br><span class="line">        self.softmax = Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x size: [b, seq, all_head_len], new_x_shape: [b, seq, num_heads, head_len]</span></span><br><span class="line">        new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        <span class="comment"># reshape: [b, seq, all_head_len] ==&gt; [b, seq, num_heads, head_len]</span></span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="comment"># x: [b, seq, num_heads, head_len] ==&gt; [b, hum_heads, seq, head_len]</span></span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        <span class="comment"># input: [b, seq, emb_len] ==&gt; [b, seq, all_head_len]</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">        mixed_key_layer = self.key(hidden_states)</span><br><span class="line">        mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose_for_scores: 切分为多头</span></span><br><span class="line">        <span class="comment"># input: [b, seq, all_head_len] ==&gt; [b, num_heads, seq, head_len]</span></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, head_len] * [b, num_heads, head_len, seq] ==&gt; [b, num_heads, seq, seq]</span></span><br><span class="line">        <span class="comment"># 本质上是在num_heads个通道上，计算一个[seq, seq]维度的协方差矩阵，表征序列特征之间的相似度</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># scaling factor: sqrt(head_len)</span></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="comment"># 在最后维度softmax</span></span><br><span class="line">        attention_probs = self.softmax(attention_scores)</span><br><span class="line">        <span class="comment"># 返回可视化的attention_score [b, num_heads, seq, seq]</span></span><br><span class="line">        weights = attention_probs <span class="keyword">if</span> self.vis <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        attention_probs = self.attn_dropout(attention_probs)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, seq] * [b, num_heads, seq, head_len] ==&gt; [b, num_heads, seq, head_len]</span></span><br><span class="line">        <span class="comment"># 本质上是用序列相似度加权平均序列特征</span></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line">        <span class="comment"># [b, num_heads, seq, head_len] ==&gt; [b, seq, num_heads, head_len]</span></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># [b, seq, num_heads, head_len] ==&gt; [b, seq, all_head_size]</span></span><br><span class="line">        <span class="comment"># 合并多头，形成attention特征</span></span><br><span class="line">        new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line">        <span class="comment"># 线性变换</span></span><br><span class="line">        attention_output = self.out(context_layer)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        attention_output = self.proj_dropout(attention_output)</span><br><span class="line">        <span class="keyword">return</span> attention_output, weights</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="Mlp"><a href="#Mlp" class="headerlink" title="Mlp"></a>Mlp</h3><p>该模块为两层全联接层，本质上是将Attention输出的特征映射到更高维度（第一层）之后再映射回统一维度（第二层）。</p><ul><li>超参数 – hidden_state，定义了输入输出的特征维度</li><li>超参数 –</li></ul><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h3><p><a href="https://nvidia.github.io/apex/index.html">Apex</a>是NVIDIA开源的一个用于混合精度训练的库，可以通过数据并行化等方式加速训练速度，降低显存占用，提升模型精度。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/apex</span><br><span class="line"><span class="built_in">cd</span> apex</span><br><span class="line"><span class="comment"># if pip &gt;= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... </span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings <span class="string">"--build-option=--cpp_ext"</span> --config-settings <span class="string">"--build-option=--cuda_ext"</span> ./</span><br><span class="line"><span class="comment"># otherwise</span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=<span class="string">"--cpp_ext"</span> --global-option=<span class="string">"--cuda_ext"</span> ./</span><br></pre></td></tr></tbody></table></figure><h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br><span class="line">model, optimizer = amp.initialize(model, optimizer, opt_level=<span class="string">"O1"</span>)</span><br><span class="line"><span class="keyword">with</span> amp.scale_loss(loss, optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">    scaled_loss.backward()</span><br></pre></td></tr></tbody></table></figure><h3 id="Mixed-Precision"><a href="#Mixed-Precision" class="headerlink" title="Mixed Precision"></a>Mixed Precision</h3><h4 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h4></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Vision Transformer (ViT)&lt;strong&gt;是Transformer模型应用到视觉领域的开山之作,在分类识别领域取得了SOTA表现，在&lt;a href=&quot;https://github.com/Alibaba-MIIL/ImageNet21K&quot;&gt;ImageNet-21K&lt;/a&gt;上训练的模型作为骨干基座，被广泛应用在了各个视觉任务中。本文主要介绍&lt;/strong&gt;ViT&lt;/strong&gt;的模型结构，&lt;a href=&quot;https://github.com/jeonsworld/ViT-pytorch?tab=readme-ov-file&quot;&gt;pytorch代码实现&lt;/a&gt;和训练方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/Vision-Transformer/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
      <category term="Vision Transformer" scheme="http://blog.zhaoyongsheng.com/tags/Vision-Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Training A Model Using Pytorch</title>
    <link href="http://blog.zhaoyongsheng.com/2024/07/01/Training-A-Model-Using-Pytorch/"/>
    <id>http://blog.zhaoyongsheng.com/2024/07/01/Training-A-Model-Using-Pytorch/</id>
    <published>2024-07-01T01:26:39.000Z</published>
    <updated>2024-07-01T01:26:39.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Pytorch</strong>是目前主流的深度学习框架，本文主要介绍使用<strong>Pytorch</strong>训练模型的主要流程，包括超参数定义、训练数据准备、配置GPU、加载模型、加在模型参数、训练日志打印、模型参数保存等步骤。</p><span id="more"></span><h2 id="主流程"><a href="#主流程" class="headerlink" title="主流程"></a>主流程</h2><p>训练模型的主流程一般定义一个<strong>main</strong>函数作为函数执行的入口函数，具体如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><h3 id="定义训练模型的超参数"><a href="#定义训练模型的超参数" class="headerlink" title="定义训练模型的超参数"></a>定义训练模型的超参数</h3><p>通过<strong>argparser</strong>定义模型训练的超参数，具体如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"><span class="comment"># Required parameters</span></span><br><span class="line">parser.add_argument(<span class="string">"--name"</span>, required=<span class="literal">True</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Name of this run. Used for monitoring."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--dataset"</span>, choices=[<span class="string">"cifar10"</span>, <span class="string">"cifar100"</span>], default=<span class="string">"cifar10"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Which downstream task."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--model_type"</span>, choices=[<span class="string">"ViT-B_16"</span>, <span class="string">"ViT-B_32"</span>, <span class="string">"ViT-L_16"</span>,</span><br><span class="line">                                                <span class="string">"ViT-L_32"</span>, <span class="string">"ViT-H_14"</span>, <span class="string">"R50-ViT-B_16"</span>],</span><br><span class="line">                    default=<span class="string">"ViT-B_16"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Which variant to use."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pretrained_dir"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">"checkpoint/ViT-B_16.npz"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Where to search for pretrained ViT models."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--output_dir"</span>, default=<span class="string">"output"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"The output directory where checkpoints will be written."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--img_size"</span>, default=<span class="number">224</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Resolution size"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--train_batch_size"</span>, default=<span class="number">512</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total batch size for training."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--eval_batch_size"</span>, default=<span class="number">64</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total batch size for eval."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--eval_every"</span>, default=<span class="number">100</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Run prediction on validation set every so many steps."</span></span><br><span class="line">                            <span class="string">"Will always run one evaluation at the end of training."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--learning_rate"</span>, default=<span class="number">3e-2</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"The initial learning rate for SGD."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--weight_decay"</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Weight deay if we apply some."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--num_steps"</span>, default=<span class="number">10000</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Total number of training epochs to perform."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--decay_type"</span>, choices=[<span class="string">"cosine"</span>, <span class="string">"linear"</span>], default=<span class="string">"cosine"</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"How to decay the learning rate."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--warmup_steps"</span>, default=<span class="number">500</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Step of training to perform learning rate warmup for."</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--max_grad_norm"</span>, default=<span class="number">1.0</span>, <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Max gradient norm."</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--local_rank"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=-<span class="number">1</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"local_rank for distributed training on gpus"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--seed'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"random seed for initialization"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--gradient_accumulation_steps'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Number of updates steps to accumulate before performing a backward/update pass."</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--fp16'</span>, action=<span class="string">'store_true'</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Whether to use 16-bit float precision instead of 32-bit"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--fp16_opt_level'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">'O2'</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."</span></span><br><span class="line">                            <span class="string">"See details at https://nvidia.github.io/apex/amp.html"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--loss_scale'</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n"</span></span><br><span class="line">                            <span class="string">"0 (default value): dynamic loss scaling.\n"</span></span><br><span class="line">                            <span class="string">"Positive power of 2: static loss scaling value.\n"</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></tbody></table></figure><h3 id="配置训练模型的GPU显卡"><a href="#配置训练模型的GPU显卡" class="headerlink" title="配置训练模型的GPU显卡"></a>配置训练模型的GPU显卡</h3><p>支持多卡和单卡训练，配置方法略有差异。</p><h4 id="单卡训练"><a href="#单卡训练" class="headerlink" title="单卡训练"></a>单卡训练</h4><p>单卡训练，默认的local_rank=-1</p><figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch<span class="selector-class">.device</span>(<span class="string">"cuda"</span> <span class="keyword">if</span> torch<span class="selector-class">.cuda</span><span class="selector-class">.is_available</span>() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">args<span class="selector-class">.n_gpu</span> = torch<span class="selector-class">.cuda</span><span class="selector-class">.device_count</span>()</span><br><span class="line">args<span class="selector-class">.device</span> = device</span><br></pre></td></tr></tbody></table></figure><h4 id="多卡训练"><a href="#多卡训练" class="headerlink" title="多卡训练"></a>多卡训练</h4><p>多卡训练的实现方式有多种，推荐使用<strong>分布式数据并行（Distributed Data Parallel）</strong>方式，既可以用于单机多卡训练，又可以用于多机多卡训练。由多进程实现，可以从以下方面理解：</p><ol><li><p>从一开始就会启动多个进程(进程数等于GPU数)，每个进程独享一个GPU，每个进程都会独立地执行代码。这意味着每个进程都独立地初始化模型、训练，当然，在每次迭代过程中会通过进程间通信共享梯度，整合梯度，然后独立地更新参数。</p></li><li><p>每个进程都会初始化一份训练数据集，当然它们会使用数据集中的不同记录做训练，这相当于同样的模型喂进去不同的数据做训练，也就是所谓的数据并行。这是通过torch.utils.data.distributed.DistributedSampler函数实现的，不过逻辑上也不难想到，只要做一下数据partition，不同进程拿到不同的partition就可以了。</p></li><li><p>进程通过local_rank变量来标识自己，local_rank为0的为master，其他是slave。local_rank表示的是当前的进程在当前节点的编号，因为我们设置了2个进程，因此进程的编号就是0和1。在使用启动命令时，torch.distributed.launch工具会默认地根据nproc_per_node传入local_rank参数。</p></li></ol><ul><li><p><a href="https://blog.csdn.net/Mr_health/article/details/122822483">pytorch多GPU训练的两种模式</a></p></li><li><p><a href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">pytorch-multi-gpu-training</a></p></li><li><p><a href="https://blog.csdn.net/lgzlgz3102/article/details/112856081">PyTorch 单机多卡操作总结：分布式DataParallel，混合精度，Horovod</a></p></li><li><p>通过<strong>argparser</strong>定义local_rank参数</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pars<span class="number">*e</span>r.add_argument(<span class="string">"--local_rank"</span>, <span class="attribute">type</span>=int, <span class="attribute">default</span>=-1,</span><br><span class="line">                    <span class="attribute">help</span>=<span class="string">"local_rank for distributed training on gpus"</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>命令行注入local_rank参数</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch <span class="attribute">--nproc_per_node</span>=4 <span class="attribute">--nnodes</span>=1 train.py</span><br></pre></td></tr></tbody></table></figure><p>上述命令表示启动一个训练节点，每个节点4张GPU卡。<strong>torch.distributed.launch</strong>就以命令行参数的方式将args.local_rank变量注入到每个进程中，每个进程得到的变量值都不相同。比如使用 4 个GPU的话，则 4 个进程获得的args.local_rank值分别为0、1、2、3。</p></li></ul><p>torch.distributed.launch在未来会被废弃，推荐使用<strong>torchrun</strong>工具。</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun <span class="attribute">--nproc_per_node</span>=4 <span class="attribute">--nnodes</span>=1 train.py --args 0</span><br></pre></td></tr></tbody></table></figure><ul><li>调用GPU显卡<figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.set<span class="constructor">_device(<span class="params">args</span>.<span class="params">local_rank</span>)</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span>, args.local_rank)</span><br><span class="line">torch.distributed.init<span class="constructor">_process_group(<span class="params">backend</span>='<span class="params">nccl</span>', <span class="params">timeout</span>=<span class="params">timedelta</span>(<span class="params">minutes</span>=60)</span>)</span><br><span class="line">args.n_gpu = <span class="number">1</span></span><br><span class="line">args.device = device</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="设置随机数种子"><a href="#设置随机数种子" class="headerlink" title="设置随机数种子"></a>设置随机数种子</h3><p>随机数影响了模型初始化，进而影响模型的训练效果，为了保证多次训练的结果一致性，通常将随机数种子设置为固定值。</p><figure class="highlight maxima"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def set_seed(<span class="built_in">args</span>):</span><br><span class="line">    <span class="built_in">random</span>.seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    <span class="built_in">np</span>.<span class="built_in">random</span>.seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    torch.manual_seed(<span class="built_in">args</span>.seed)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">args</span>.n_gpu &gt; <span class="number">0</span>:</span><br><span class="line">        torch.cuda.manual_seed_all(<span class="built_in">args</span>.seed)</span><br></pre></td></tr></tbody></table></figure><h3 id="配置模型"><a href="#配置模型" class="headerlink" title="配置模型"></a>配置模型</h3><p>同一个模型结构，一般会预留一些超参数，用于生成模型参数量的模型。</p><h4 id="配置模型参数"><a href="#配置模型参数" class="headerlink" title="配置模型参数"></a>配置模型参数</h4><p>推荐使用<strong>ml_collections</strong>实现模型参数配置，具体如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ml_collections</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_config</span>():</span><br><span class="line">    <span class="string">"""Returns a minimal configuration for testing."""</span></span><br><span class="line">    config = ml_collections.ConfigDict()</span><br><span class="line">    config.patches = ml_collections.ConfigDict({<span class="string">'size'</span>: (<span class="number">16</span>, <span class="number">16</span>)})</span><br><span class="line">    config.hidden_size = <span class="number">1</span></span><br><span class="line">    config.transformer = ml_collections.ConfigDict()</span><br><span class="line">    config.transformer.mlp_dim = <span class="number">1</span></span><br><span class="line">    config.transformer.num_heads = <span class="number">1</span></span><br><span class="line">    config.transformer.num_layers = <span class="number">1</span></span><br><span class="line">    config.transformer.attention_dropout_rate = <span class="number">0.0</span></span><br><span class="line">    config.transformer.dropout_rate = <span class="number">0.1</span></span><br><span class="line">    config.classifier = <span class="string">'token'</span></span><br><span class="line">    config.representation_size = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> config</span><br></pre></td></tr></tbody></table></figure><p>上述代码可以生成如下所示的配置。</p><figure class="highlight nestedtext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">classifier</span><span class="punctuation">:</span> <span class="string">token</span></span><br><span class="line"><span class="attribute">hidden_size</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line"><span class="attribute">patches</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">size</span><span class="punctuation">:</span> <span class="string">!!python/tuple</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">16</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">16</span></span><br><span class="line"><span class="attribute">representation_size</span><span class="punctuation">:</span> <span class="string">null</span></span><br><span class="line"><span class="attribute">transformer</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">attention_dropout_rate</span><span class="punctuation">:</span> <span class="string">0.0</span></span><br><span class="line">  <span class="attribute">dropout_rate</span><span class="punctuation">:</span> <span class="string">0.1</span></span><br><span class="line">  <span class="attribute">mlp_dim</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line">  <span class="attribute">num_heads</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line">  <span class="attribute">num_layers</span><span class="punctuation">:</span> <span class="string">1</span></span><br></pre></td></tr></tbody></table></figure><p>模型的多个配置，可以使用一个dict记录。</p><figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CONFIGS = {</span><br><span class="line">    <span class="string">'ViT-B_16'</span>: configs<span class="selector-class">.get_b16_config</span>(),</span><br><span class="line">    <span class="string">'ViT-B_32'</span>: configs<span class="selector-class">.get_b32_config</span>(),</span><br><span class="line">    <span class="string">'ViT-L_16'</span>: configs<span class="selector-class">.get_l16_config</span>(),</span><br><span class="line">    <span class="string">'ViT-L_32'</span>: configs<span class="selector-class">.get_l32_config</span>(),</span><br><span class="line">    <span class="string">'ViT-H_14'</span>: configs<span class="selector-class">.get_h14_config</span>(),</span><br><span class="line">    <span class="string">'R50-ViT-B_16'</span>: configs<span class="selector-class">.get_r50_b16_config</span>(),</span><br><span class="line">    <span class="string">'testing'</span>: configs<span class="selector-class">.get_testing</span>(),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="生成并加载模型"><a href="#生成并加载模型" class="headerlink" title="生成并加载模型"></a>生成并加载模型</h4><p>首先，将配置传入模型构造函数中，生成模型；其次，模型可以载入之前保存好的参数；最后，将模型加载到device中。</p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="constructor">VisionTransformer(<span class="params">config</span>, <span class="params">args</span>.<span class="params">img_size</span>, <span class="params">zero_head</span>=True, <span class="params">num_classes</span>=<span class="params">num_classes</span>)</span></span><br><span class="line">model.load<span class="constructor">_from(<span class="params">np</span>.<span class="params">load</span>(<span class="params">args</span>.<span class="params">pretrained_dir</span>)</span>)</span><br><span class="line">model.<span class="keyword">to</span>(args.device)</span><br></pre></td></tr></tbody></table></figure><h4 id="统计模型参数量"><a href="#统计模型参数量" class="headerlink" title="统计模型参数量"></a>统计模型参数量</h4><p>模型参数一般指的是有梯度的参数。</p><figure class="highlight stan"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def count_parameters(<span class="title">model</span>):</span><br><span class="line">    params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> <span class="title">model</span>.<span class="title">parameters</span>() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">    <span class="keyword">return</span> params/<span class="number">1000000</span> <span class="comment"># in million（M）</span></span><br></pre></td></tr></tbody></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>模型训练一般由一个<strong>train</strong>函数实现。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">args, model</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></tbody></table></figure><h4 id="保存训练过程"><a href="#保存训练过程" class="headerlink" title="保存训练过程"></a>保存训练过程</h4><ul><li><p>通过<strong>SummaryWriter</strong>保存训练过程数据日志，并可以通过<strong>TensorBoard</strong>查看。<br>一般由单卡或者master卡保存训练日志，多卡训练的slave节点不保存日志。</p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">in</span> <span class="literal">[-<span class="number">1</span>, <span class="number">0</span>]</span>:</span><br><span class="line">    os.makedirs(args.output_dir, exist_ok=True)</span><br><span class="line">    writer = <span class="constructor">SummaryWriter(<span class="params">log_dir</span>=<span class="params">os</span>.<span class="params">path</span>.<span class="params">join</span>(<span class="string">"logs"</span>, <span class="params">args</span>.<span class="params">name</span>)</span>)</span><br><span class="line"></span><br><span class="line"># train loop</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.local_rank <span class="keyword">in</span> <span class="literal">[-<span class="number">1</span>, <span class="number">0</span>]</span>:</span><br><span class="line">        writer.close<span class="literal">()</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>在terminal中打印日志<br>推荐使用<strong>logging</strong>包实现，具体如下所示：</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import<span class="built_in"> logging</span></span><br><span class="line"><span class="built_in"></span>logger = logging.getLogger(__name__)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Total optimization steps = %d"</span>, args.num_steps)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Instantaneous batch size per GPU = %d"</span>, args.train_batch_size)</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Total train batch size (w. parallel, distributed &amp; accumulation) = %d"</span>,</span><br><span class="line">                args.train_batch_size * args.gradient_accumulation_steps * (</span><br><span class="line">                    torch.distributed.get_world_size() <span class="keyword">if</span> args.local_rank != -1 <span class="keyword">else</span> 1))</span><br><span class="line">logger.<span class="built_in">info</span>(<span class="string">"  Gradient Accumulation steps = %d"</span>, args.gradient_accumulation_steps)</span><br></pre></td></tr></tbody></table></figure></li></ul><h4 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h4><p>通过一个<strong>data_loader</strong>函数实现，返回<strong>train_loader</strong>和<strong>test_loader</strong>，具体参照！(数据集准备)[]</p><figure class="highlight autohotkey"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">train_loader,</span> test_loader = dat<span class="built_in">a_loader</span>(args)</span><br></pre></td></tr></tbody></table></figure><h4 id="准备优化器和训练计划"><a href="#准备优化器和训练计划" class="headerlink" title="准备优化器和训练计划"></a>准备优化器和训练计划</h4><p>需要使用<strong>apex</strong>库。</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apex import amp</span><br><span class="line"><span class="keyword">from</span> apex.parallel import DistributedDataParallel as DDP</span><br><span class="line"><span class="comment"># Prepare optimizer and scheduler</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),</span><br><span class="line">                            <span class="attribute">lr</span>=args.learning_rate,</span><br><span class="line">                            <span class="attribute">momentum</span>=0.9,</span><br><span class="line">                            <span class="attribute">weight_decay</span>=args.weight_decay)</span><br><span class="line">t_total = args.num_steps</span><br><span class="line"><span class="keyword">if</span> args.decay_type == <span class="string">"cosine"</span>:</span><br><span class="line">   <span class="built_in"> scheduler </span>= WarmupCosineSchedule(optimizer, <span class="attribute">warmup_steps</span>=args.warmup_steps, <span class="attribute">t_total</span>=t_total)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="built_in"> scheduler </span>= WarmupLinearSchedule(optimizer, <span class="attribute">warmup_steps</span>=args.warmup_steps, <span class="attribute">t_total</span>=t_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.fp16:</span><br><span class="line">    model, optimizer = amp.initialize(<span class="attribute">models</span>=model,</span><br><span class="line">                                        <span class="attribute">optimizers</span>=optimizer,</span><br><span class="line">                                        <span class="attribute">opt_level</span>=args.fp16_opt_level)</span><br><span class="line">    amp._amp_state.loss_scalers[0]._loss_scale = 2*<span class="number">*20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Distributed training</span></span><br><span class="line"><span class="keyword">if</span> args.local_rank != -1:</span><br><span class="line">    model = DDP(model, <span class="attribute">message_size</span>=250000000, <span class="attribute">gradient_predivide_factor</span>=get_world_size())</span><br></pre></td></tr></tbody></table></figure><h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><ul><li>valid<br>在训练过程中一般会每隔一定时间用测试集评估一下模型的训练结果，过程如下所示：<figure class="highlight scss"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">valid</span>(args, model, writer, test_loader, global_step):</span><br><span class="line">    # Validation!</span><br><span class="line">    eval_losses = <span class="built_in">AverageMeter</span>()</span><br><span class="line"></span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"***** Running Validation *****"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"  Num steps = %d"</span>, <span class="built_in">len</span>(test_loader))</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"  Batch size = %d"</span>, args.eval_batch_size)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    all_preds, all_label = [], []</span><br><span class="line">    epoch_iterator = <span class="built_in">tqdm</span>(test_loader,</span><br><span class="line">                          desc=<span class="string">"Validating... (loss=X.X)"</span>,</span><br><span class="line">                          bar_format=<span class="string">"{l_bar}{r_bar}"</span>,</span><br><span class="line">                          dynamic_ncols=True,</span><br><span class="line">                          disable=args.local_rank not in [-<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    loss_fct = torch.nn.<span class="built_in">CrossEntropyLoss</span>()</span><br><span class="line">    for step, batch in <span class="built_in">enumerate</span>(epoch_iterator):</span><br><span class="line">        batch = <span class="built_in">tuple</span>(t.<span class="built_in">to</span>(args.device) for t in batch)</span><br><span class="line">        x, y = batch</span><br><span class="line">        with torch.<span class="built_in">no_grad</span>():</span><br><span class="line">            logits = <span class="built_in">model</span>(x)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            eval_loss = <span class="built_in">loss_fct</span>(logits, y)</span><br><span class="line">            eval_losses.<span class="built_in">update</span>(eval_loss.<span class="built_in">item</span>())</span><br><span class="line"></span><br><span class="line">            preds = torch.<span class="built_in">argmax</span>(logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        if <span class="built_in">len</span>(all_preds) == <span class="number">0</span>:</span><br><span class="line">            all_preds.<span class="built_in">append</span>(preds.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>())</span><br><span class="line">            all_label.<span class="built_in">append</span>(y.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>())</span><br><span class="line">        else:</span><br><span class="line">            all_preds[<span class="number">0</span>] = np.<span class="built_in">append</span>(</span><br><span class="line">                all_preds[<span class="number">0</span>], preds.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>(), axis=<span class="number">0</span></span><br><span class="line">            )</span><br><span class="line">            all_label[<span class="number">0</span>] = np.<span class="built_in">append</span>(</span><br><span class="line">                all_label[<span class="number">0</span>], y.<span class="built_in">detach</span>().<span class="built_in">cpu</span>().<span class="built_in">numpy</span>(), axis=<span class="number">0</span></span><br><span class="line">            )</span><br><span class="line">        epoch_iterator.<span class="built_in">set_description</span>(<span class="string">"Validating... (loss=%2.5f)"</span> % eval_losses.val)</span><br><span class="line"></span><br><span class="line">    all_preds, all_label = all_preds[<span class="number">0</span>], all_label[<span class="number">0</span>]</span><br><span class="line">    accuracy = <span class="built_in">simple_accuracy</span>(all_preds, all_label)</span><br><span class="line"></span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"\n"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Validation Results"</span>)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Global Steps: %d"</span> % global_step)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Valid Loss: %2.5f"</span> % eval_losses.avg)</span><br><span class="line">    logger.<span class="built_in">info</span>(<span class="string">"Valid Accuracy: %2.5f"</span> % accuracy)</span><br><span class="line"></span><br><span class="line">    writer.<span class="built_in">add_scalar</span>(<span class="string">"test/accuracy"</span>, scalar_value=accuracy, global_step=global_step)</span><br><span class="line">    return accuracy</span><br><span class="line"></span><br><span class="line">def <span class="built_in">simple_accuracy</span>(preds, labels):</span><br><span class="line">    return (preds == labels).<span class="built_in">mean</span>()</span><br></pre></td></tr></tbody></table></figure></li><li>train<br>首先，在while循环开始训练之前，定义loss, global_step, best_acc等全局变量；然后开启while循环，开始训练；最后通过global_step和total_step结束训练。训练过程中需要用到<strong>tqdm</strong>库。<figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">model.zero_grad()</span><br><span class="line">set_seed(args)  # Added here <span class="keyword">for</span> reproducibility (even between python 2 <span class="keyword">and</span> 3)</span><br><span class="line">losses = AverageMeter()</span><br><span class="line">global_step, best_acc = 0, 0</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_iterator = tqdm(train_loader,</span><br><span class="line">                            <span class="attribute">desc</span>=<span class="string">"Training (X / X Steps) (loss=X.X)"</span>,</span><br><span class="line">                            <span class="attribute">bar_format</span>=<span class="string">"{l_bar}{r_bar}"</span>,</span><br><span class="line">                            <span class="attribute">dynamic_ncols</span>=<span class="literal">True</span>,</span><br><span class="line">                            <span class="attribute">disable</span>=args.local_rank <span class="keyword">not</span> <span class="keyword">in</span> [-1, 0])</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">step</span>, batch <span class="keyword">in</span> enumerate(epoch_iterator):</span><br><span class="line">        batch = tuple(t.<span class="keyword">to</span>(args.device) <span class="keyword">for</span> t <span class="keyword">in</span> batch)</span><br><span class="line">        x, y = batch</span><br><span class="line">        loss = model(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.gradient_accumulation_steps &gt; 1:</span><br><span class="line">            loss = loss / args.gradient_accumulation_steps</span><br><span class="line">        <span class="keyword">if</span> args.fp16:</span><br><span class="line">            with amp.scale_loss(loss, optimizer) as scaled_loss:</span><br><span class="line">                scaled_loss.backward()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">step</span> + 1) % args.gradient_accumulation_steps == 0:</span><br><span class="line">            losses.update(loss.item()<span class="number">*a</span>rgs.gradient_accumulation_steps)</span><br><span class="line">            <span class="keyword">if</span> args.fp16:</span><br><span class="line">                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)</span><br><span class="line">            scheduler.<span class="keyword">step</span>()</span><br><span class="line">            optimizer.<span class="keyword">step</span>()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            global_step += 1</span><br><span class="line"></span><br><span class="line">            epoch_iterator.set_description(</span><br><span class="line">                <span class="string">"Training (%d / %d Steps) (loss=%2.5f)"</span> % (global_step, t_total, losses.val)</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> args.local_rank <span class="keyword">in</span> [-1, 0]:</span><br><span class="line">                writer.add_scalar(<span class="string">"train/loss"</span>, <span class="attribute">scalar_value</span>=losses.val, <span class="attribute">global_step</span>=global_step)</span><br><span class="line">                writer.add_scalar(<span class="string">"train/lr"</span>, <span class="attribute">scalar_value</span>=scheduler.get_lr()[0], <span class="attribute">global_step</span>=global_step)</span><br><span class="line">            <span class="keyword">if</span> global_step % args.eval_every == 0 <span class="keyword">and</span> args.local_rank <span class="keyword">in</span> [-1, 0]:</span><br><span class="line">                accuracy = valid(args, model, writer, test_loader, global_step)</span><br><span class="line">                <span class="keyword">if</span> best_acc &lt; accuracy:</span><br><span class="line">                    save_model(args, model)</span><br><span class="line">                    best_acc = accuracy</span><br><span class="line">                model.train()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> global_step % t_total == 0:</span><br><span class="line">                break</span><br><span class="line">    losses.reset()</span><br><span class="line">    <span class="keyword">if</span> global_step % t_total == 0:</span><br><span class="line">        break</span><br></pre></td></tr></tbody></table></figure></li></ul></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;是目前主流的深度学习框架，本文主要介绍使用&lt;strong&gt;Pytorch&lt;/strong&gt;训练模型的主要流程，包括超参数定义、训练数据准备、配置GPU、加载模型、加在模型参数、训练日志打印、模型参数保存等步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Deep-Learning/Pytorch/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.zhaoyongsheng.com/tags/Deep-Learning/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Pytorch" scheme="http://blog.zhaoyongsheng.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Using Ngnix To Deploy A File Share Service</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/28/Using-Ngnix-To-Deploy-A-File-Share-Service/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/28/Using-Ngnix-To-Deploy-A-File-Share-Service/</id>
    <published>2024-06-28T14:34:58.000Z</published>
    <updated>2024-06-28T14:34:58.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>使用Nginx，可以在服务器上搭建一个文件分享服务，通过网络可以下载指定文件夹下的文件。</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install nginx</span><br></pre></td></tr></tbody></table></figure><h3 id="配置服务"><a href="#配置服务" class="headerlink" title="配置服务"></a>配置服务</h3><p>安装后nginx会自动运行，监听80端口，我们需要修改默认配置，指定端口和root目录，并开启autoindex。</p><figure class="highlight gradle"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/etc/</span>nginx<span class="regexp">/sites-enabled/</span><span class="keyword">default</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> {</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">30001</span> default_server;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">30001</span> default_server;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># SSL configuration</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># listen 443 ssl default_server;</span></span><br><span class="line">        <span class="comment"># listen [::]:443 ssl default_server;</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Note: You should disable gzip for SSL traffic.</span></span><br><span class="line">        <span class="comment"># See: https://bugs.debian.org/773332</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Read up on ssl_ciphers to ensure a secure configuration.</span></span><br><span class="line">        <span class="comment"># See: https://bugs.debian.org/765782</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Self signed certs generated by the ssl-cert package</span></span><br><span class="line">        <span class="comment"># Don't use them in a production server!</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># include snippets/snakeoil.conf;</span></span><br><span class="line"></span><br><span class="line">        <span class="attribute">root</span> /apprun/datasets/;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add index.php to the list if you are using PHP</span></span><br><span class="line">        <span class="attribute">index</span> index.html index.htm index.nginx-debian.html;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">server_name</span> _;</span><br><span class="line"></span><br><span class="line">        <span class="section">location</span> / {</span><br><span class="line">                <span class="comment"># First attempt to serve request as file, then</span></span><br><span class="line">                <span class="comment"># as directory, then fall back to displaying a 404.</span></span><br><span class="line">                <span class="comment"># try_files $uri $uri/ =404;</span></span><br><span class="line">                <span class="attribute">autoindex</span> <span class="literal">on</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass PHP scripts to FastCGI server</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#location ~ \.php$ {</span></span><br><span class="line">        <span class="comment">#       include snippets/fastcgi-php.conf;</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#       # With php-fpm (or other unix sockets):</span></span><br><span class="line">        <span class="comment">#       fastcgi_pass unix:/var/run/php/php7.4-fpm.sock;</span></span><br><span class="line">        <span class="comment">#       # With php-cgi (or other tcp sockets):</span></span><br><span class="line">        <span class="comment">#       fastcgi_pass 127.0.0.1:9000;</span></span><br><span class="line">        <span class="comment">#}</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># deny access to .htaccess files, if Apache's document root</span></span><br><span class="line">        <span class="comment"># concurs with nginx's one</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#location ~ /\.ht {</span></span><br><span class="line">        <span class="comment">#       deny all;</span></span><br><span class="line">        <span class="comment">#}</span></span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h3><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo nginx -t</span></span><br><span class="line"><span class="attribute">sudo nginx -s reload</span></span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Nginx，可以在服务器上搭建一个文件分享服务，通过网络可以下载指定文件夹下的文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/"/>
    
      <category term="Nginx" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/Nginx/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/tags/Ubuntu/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Nginx" scheme="http://blog.zhaoyongsheng.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Using Baidu Yun Python Client In Ubuntu Server</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/28/Using-Baidu-Yun-Python-Client-In-Ubuntu-Server/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/28/Using-Baidu-Yun-Python-Client-In-Ubuntu-Server/</id>
    <published>2024-06-28T02:05:08.000Z</published>
    <updated>2024-06-28T02:05:08.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>百度云盘为Ubuntu Server提供了一个Python客户端<strong>pyby</strong>，可以在服务器上上传和下载百度云盘里面的文件。</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests bypy -i https:<span class="regexp">//</span>pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure><h3 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h3><p>运行以下指令会生成一个用于授权的链接，复制链接在浏览器中打开并登陆，则会生成授权码，按照提示复制到terminal中即可。</p><figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">python</span> -m bypy <span class="literal">info</span></span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>百度云盘会为<strong>bypy</strong>客户端单独生成一个文件夹，使用相关指令可以查看文件夹的信息、上传文件、下载文件等。</p><h4 id="查看百度云盘的空间"><a href="#查看百度云盘的空间" class="headerlink" title="查看百度云盘的空间"></a>查看百度云盘的空间</h4><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bypy</span> info</span><br><span class="line"><span class="attribute">Quota</span>: <span class="number">5</span>.<span class="number">005</span>TB</span><br><span class="line"><span class="attribute">Used</span>: <span class="number">2</span>.<span class="number">416</span>GB</span><br></pre></td></tr></tbody></table></figure><h4 id="查看百度云盘的列表"><a href="#查看百度云盘的列表" class="headerlink" title="查看百度云盘的列表"></a>查看百度云盘的列表</h4><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bypy list</span></span><br></pre></td></tr></tbody></table></figure><h4 id="上传文件"><a href="#上传文件" class="headerlink" title="上传文件"></a>上传文件</h4><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bypy -v upload <span class="built_in">file</span>.path /remote_dir</span><br></pre></td></tr></tbody></table></figure><h4 id="其他指令"><a href="#其他指令" class="headerlink" title="其他指令"></a>其他指令</h4><figure class="highlight powershell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> bypy downdir k8s <span class="comment">#直接下载bypy目录下k8s</span></span><br><span class="line"><span class="variable">$</span> bypy syncup <span class="comment">#把当前目录同步到云盘</span></span><br><span class="line"><span class="variable">$</span> bypy syncdown <span class="comment">#把云盘内容同步到当前目录</span></span><br><span class="line"><span class="variable">$</span> bypy downdir / <span class="comment">#把云盘内容同步到当前目录</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$</span> bypy <span class="built_in">compare</span> <span class="comment">#比较本地当前目录和云盘（程序的）根目录</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-v</span> <span class="comment">#运行时添加-v参数，会显示进度详情。</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-d</span> <span class="comment">#运行时添加-d，会显示一些调试信息</span></span><br><span class="line"><span class="variable">$</span> bypy <span class="literal">-ddd</span> <span class="comment">#显示更多http通讯信息</span></span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;百度云盘为Ubuntu Server提供了一个Python客户端&lt;strong&gt;pyby&lt;/strong&gt;，可以在服务器上上传和下载百度云盘里面的文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Ubuntu/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.zhaoyongsheng.com/tags/Ubuntu/"/>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
  </entry>
  
  <entry>
    <title>Using Python Virtual Environment In Jupyter Notebook</title>
    <link href="http://blog.zhaoyongsheng.com/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/"/>
    <id>http://blog.zhaoyongsheng.com/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/</id>
    <published>2024-06-27T15:16:01.000Z</published>
    <updated>2024-06-27T15:16:01.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>介绍如何在<strong>Jupyter Notebook</strong>中使用Python的虚拟环境。</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --<span class="keyword">user</span> <span class="title">ipykernel</span> -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure><h3 id="添加虚拟环境"><a href="#添加虚拟环境" class="headerlink" title="添加虚拟环境"></a>添加虚拟环境</h3><p>创建python虚拟环境请参照<a href="https://youngsonzhao.github.io/2024/02/21/Using-Python-Virtual-Environment/">Using Python Virtual Environment</a><br>创建好虚拟环境后，可以使用一下命令在Jupyter中注册虚拟环境。</p><figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m ipykernel install --<span class="keyword">user</span> <span class="title">--name</span>=.sam</span><br></pre></td></tr></tbody></table></figure><h3 id="在Jupyter-Notebook中使用虚拟环境"><a href="#在Jupyter-Notebook中使用虚拟环境" class="headerlink" title="在Jupyter Notebook中使用虚拟环境"></a>在Jupyter Notebook中使用虚拟环境</h3><p>在<strong>Jupyter Notebook</strong>页面的左下角按钮，可以切换kernel，如下图所示：<br><img alt="切换kernel" data-src="/2024/06/27/Using-Python-Virtual-Environment-In-Jupyter-Notebook/ipykernel.png"></p></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍如何在&lt;strong&gt;Jupyter Notebook&lt;/strong&gt;中使用Python的虚拟环境。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/"/>
    
      <category term="Jupyter Notebook" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/Jupyter-Notebook/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/tags/Python/"/>
    
      <category term="Jupyter Notebook" scheme="http://blog.zhaoyongsheng.com/tags/Jupyter-Notebook/"/>
    
      <category term="Venv" scheme="http://blog.zhaoyongsheng.com/tags/Venv/"/>
    
  </entry>
  
  <entry>
    <title>Using Python Virtual Environment</title>
    <link href="http://blog.zhaoyongsheng.com/2024/02/21/Using-Python-Virtual-Environment/"/>
    <id>http://blog.zhaoyongsheng.com/2024/02/21/Using-Python-Virtual-Environment/</id>
    <published>2024-02-21T14:18:48.000Z</published>
    <updated>2024-02-21T14:18:48.000Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>在Python中使用虚拟环境可以将包安装在独立的环境中，既不会污染主环境，也不会相互影响。因此在跑测试环境的时候，推荐使用虚拟环境。本文主要介绍虚拟环境的使用方法。</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>使用以下指令安装虚拟环境：</p><figure class="highlight cmake"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> virtualenv</span><br><span class="line">sudo apt-get <span class="keyword">install</span> python3.<span class="number">10</span>-venv</span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="初始化环境"><a href="#初始化环境" class="headerlink" title="初始化环境"></a>初始化环境</h4><p>安装成功后，使用以下指令可以创建一个虚拟环境。</p><figure class="highlight jboss-cli"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv <span class="string">.venv</span></span><br></pre></td></tr></tbody></table></figure><p>所创建的虚拟环境目录为：**.venv**</p><h4 id="激活环境"><a href="#激活环境" class="headerlink" title="激活环境"></a>激活环境</h4><p>创建环境后，可以使用以下指令激活环境，让Python在虚拟环境中运行。</p><figure class="highlight gradle"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> ~<span class="regexp">/.venv/</span>bin/active</span><br></pre></td></tr></tbody></table></figure><p>激活后，我们可以在虚拟环境中正常运行python代码以及使用pip安装包。</p><h4 id="失活环境"><a href="#失活环境" class="headerlink" title="失活环境"></a>失活环境</h4><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">deactive</span></span><br></pre></td></tr></tbody></table></figure></body></html>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Python中使用虚拟环境可以将包安装在独立的环境中，既不会污染主环境，也不会相互影响。因此在跑测试环境的时候，推荐使用虚拟环境。本文主要介绍虚拟环境的使用方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/categories/Tutorials/Python/"/>
    
    
      <category term="Tutorials" scheme="http://blog.zhaoyongsheng.com/tags/Tutorials/"/>
    
      <category term="Python" scheme="http://blog.zhaoyongsheng.com/tags/Python/"/>
    
      <category term="Venv" scheme="http://blog.zhaoyongsheng.com/tags/Venv/"/>
    
  </entry>
  
</feed>
