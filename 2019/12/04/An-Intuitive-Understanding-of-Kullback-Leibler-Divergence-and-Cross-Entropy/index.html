<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="msvalidate.01" content="3caebbd6f719430c88b5a8dc11df5a98">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Helvetica:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://blog.zhaoyongsheng.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"default"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":true,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: true,
    lazyload: true,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Information Theory is initially proposed in a landmark paper A Mathematical Theory of Communication published by Claude Shannon. One of the most important concepts in information theory is Entropy. It">
<meta property="og:type" content="article">
<meta property="og:title" content="An Intuitive Understanding of Entropy, Cross Entropy and Kullback Leibler Divergence">
<meta property="og:url" content="http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/index.html">
<meta property="og:site_name" content="Dr. Zhao&#39;s Blog">
<meta property="og:description" content="Information Theory is initially proposed in a landmark paper A Mathematical Theory of Communication published by Claude Shannon. One of the most important concepts in information theory is Entropy. It">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-04T15:23:38.000Z">
<meta property="article:modified_time" content="2019-12-04T15:23:38.000Z">
<meta property="article:author" content="Dr. Zhao">
<meta property="article:tag" content="Maths">
<meta property="article:tag" content="Probability">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>An Intuitive Understanding of Entropy, Cross Entropy and Kullback Leibler Divergence | Dr. Zhao's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153610516-2"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-153610516-2');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?01357a85fc2ab892b21c9ec34514f480";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dr. Zhao's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">Focus on Scientific Research</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Dr. Zhao">
      <meta itemprop="description" content="A technical blog for sharing articles related to Computer Vision, Robotics & Deep Learning.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dr. Zhao's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          An Intuitive Understanding of Entropy, Cross Entropy and Kullback Leibler Divergence
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-04 23:23:38" itemprop="dateCreated datePublished" datetime="2019-12-04T23:23:38+08:00">2019-12-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability/" itemprop="url" rel="index">
                    <span itemprop="name">Probability</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Information-Theory/" itemprop="url" rel="index">
                    <span itemprop="name">Information Theory</span>
                  </a>
                </span>
            </span>

          
            <span id="/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/" class="post-meta-item leancloud_visitors" data-flag-title="An Intuitive Understanding of Entropy, Cross Entropy and Kullback Leibler Divergence" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <html><head></head><body><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Information_theory">Information Theory</a> is initially proposed in a landmark paper <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication">A Mathematical Theory of Communication</a> published by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>. One of the most important concepts in information theory is <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Entropy">Entropy</a>. It is used to describe the degree of disorder or uncertainty, which is analogous to the definition used in statistical thermodynamics. From the point view of probability, entropy evaluates how much information a probability distribution can carry. Based on the definition of entropy, people further proposed <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cross_entropy">Cross Entropy</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback Leibler (KL) Divergence</a> to address the issues of information analysis between two different probability distributions. They are good metric that could measure the similarity or distance between two probability distributions, therefore they are widely used in machine learning and deep learning. In this article, I want to provide you an intuitive understanding of entropy, cross entropy and kullback-leibler divergence.</p>
<span id="more"></span>
<h2 id="entropy">Entropy</h2>
<p>In the context of Information Theory, how to understand the definition of entropy, the degree of disorder or uncertainty? I try to present an intuitive explanation from the following several aspects. ### Probability, Certainty and Uncertainty In mathematics, a probability is defined to describe the chance an event will happen. The larger a probability is, the more chance an event will happen. And the more chance an event will happen, the more certainty an event is, correspondingly, the less uncertainty an event is. Define the probability of an event is <span class="math display">\[p\]</span>, then the certainty is <span class="math display">\[p\]</span> and the uncertainty is <span class="math display">\[\frac{1}{p}\]</span>. Note that <span class="math display">\[p=0\]</span> means that the uncertainty is infinity. A probability distribution describe the chances of a series of events will happen, which is more general in real world.</p>
<h3 id="information">Information</h3>
<p>What's the definition of information? There must be different definitions of information from different point of view. However, in my opinion, information is what we do not know or do not expect. For example, if we don't know how to use an device, then its instruction manual will give us the right information. What's the relationship between information and certainty from the point of view of probability? The more certainty an event is, the more information the event contains and the less extra information we need to clear the event's uncertainty. For example, assuming we live in a world that the probability of rainy day is 1, then we have all the information regarding to rainy probability and the weather forecast is useless in this scenario because we do not need extra information to clear the uncertainty of the rainy event. It is the same when we live in a world that the probability of rainy day is 0. As far as we can see, certainty is positively related to information. The larger the certainty of an event is, the more information that the event has and the less extra information we need to make it clear.</p>
<h3 id="uncertainty-and-entropy">Uncertainty and Entropy</h3>
<p>I believe you already understand the basic concepts of probability, certainty, uncertainty, information and their correlations with each other. Let's take another example to further explain their relationships with respect to uncertainty and entropy. If Jack tells us he lives in China. We can say that the probability he lives in South China is 0.5, North China is 0.5. The message Jack tells us has more uncertainty, and we need extra information to know whether he lives in South China or not. If Jack tells us he lives in Shanghai, the message Jack tells us has less uncertainty because we already know he lives in South China without extra information.</p>
<p>The extra information is what we called uncertainty. Information theory is proposed by Claude Shannon to address the issues how to efficiently communicate the extra information with others. In modern digital world, information theory address the issues of how many bits we need to clear the uncertainty when we communicate with others. As far as we know, one bit has two states 0 or 1. Therefore, we can use one bit to reduce the uncertainty by dividing 2. That is to say we can use the bits to measure the uncertainty. The more bits we need, the larger the uncertainty is.</p>
<p>For example, if the weather has two equal-probability events, rainy and sunny, each one has 0.5 probability to happen, then we can use 1 bit to clear the rainy event's uncertainty or sunny event's uncertainty. The value of the one bit is 1 means that the weather is rainy and 0 means that the weather is not rainy. The same is for sunny event. We can formulate the above conclusion as Eq. <span class="math inline">\(\eqref{eq1}\)</span>. <span class="math display">\[
\begin{equation}
\frac{1}{0.5}=2^1
\label{eq1}
\end{equation}
\]</span> The expected uncertainty (bit count) of the weather with two equal-probability events is <span class="math display">\[0.5*1 + 0.5*1 = 1\]</span>.</p>
<p>Similarly, if the weather has eight equal-probability events, each one has 0.125 probability to happen, then we can use 3 bit to clear the each event's uncertainty. We can formulate it as Eq. <span class="math inline">\(\eqref{eq2}\)</span>. <span class="math display">\[
\begin{equation}
\frac{1}{0.125}=2^3
\label{eq2}
\end{equation}
\]</span> The expected uncertainty (bits count) of the weather with eight equal-probability events is <span class="math display">\[\sum_{i=1}^{8}{0.125*3} = 3\]</span>.</p>
<p>In conclusion, if the probability of one event is <span class="math display">\[p\]</span>, then we can use Eg. <span class="math inline">\(\eqref{eq3}\)</span> compute the bits we need to clear the event's uncertainty. <span class="math display">\[
\begin{equation}
\frac{1}{p}=2^n
\label{eq3}
\end{equation}
\]</span> where <span class="math display">\[n=\log_{2}{\frac{1}{p}}\]</span> denotes the bit count.</p>
<p>If the weather has two nonequal-probability events, rainy event has 0.25 probability to happen and sunny event has 0.75 probability to happen, how many bits we need to clear the weather's uncertainty? We first compute the needed bits to clear the rainy event's uncertainty, as shown in Eg. <span class="math inline">\(\eqref{eq4}\)</span> <span class="math display">\[
\begin{equation}
n_{rainy}=\log_{2}{\frac{1}{0.25}}
\label{eq4}
\end{equation}
\]</span> Then we compute the needed bits to clear the sunny event's uncertainty, as shown in Eg. <span class="math inline">\(\eqref{eq5}\)</span> <span class="math display">\[
\begin{equation}
n_{sunny}=\log_{2}{\frac{1}{0.75}}
\label{eq5}
\end{equation}
\]</span> The expected uncertainty (bits count) of the weather is <span class="math display">\[0.25*n_{rainy} + 0.75*n_{sunny} \approx 0.81\]</span>.</p>
<p>In the context of Information Theory, Entropy is defined to evaluate the amount of extra information we need to clear the uncertainty of a series of events given its probability distribution. According to Eq. <span class="math inline">\(\eqref{eq3}\)</span>, <span class="math inline">\(\eqref{eq4}\)</span>, <span class="math inline">\(\eqref{eq5}\)</span>, we derive the mathematical formulation of entropy as Eg. <span class="math inline">\(\eqref{6}\)</span> <span class="math display">\[
\begin{equation}
H(P)=-\sum_{i}{p_{i}log_{2}{p_{i}}}
\label{eq6}
\end{equation}
\]</span> where <span class="math display">\[H(P)\]</span> denotes the entropy given <span class="math display">\[P\]</span> probability distribution.</p>
<p>Generally, we use <span class="math display">\[log\]</span> to replace <span class="math display">\[log_{2}\]</span>, we derive the general formulation of entropy as Eq. <span class="math inline">\(\eqref{eq7}\)</span>. <span class="math display">\[
\begin{equation}
H(P)=-\sum_{i}{p_{i}\log{p_{i}}}
\label{eq7}
\end{equation}
\]</span></p>
<h2 id="cross-entropy">Cross Entropy</h2>
<p>According to the preceding sections, we understand that entropy <span class="math display">\[H(P)\]</span> evaluates the degree of uncertainty of a probability distribution <span class="math display">\[P\]</span>. That is to say, it evaluates the distance in bits between the uncertain state formulated by a probability distribution and the corresponding certain state. Analogically, cross entropy evaluates the distance in bits between the uncertain state formulated by a probability distribution <span class="math display">\[Q\]</span> and the corresponding certain state, given a target probability distribution <span class="math display">\[P\]</span>. Eq. <span class="math inline">\(\eqref{eq8}\)</span> defines the cross entropy. <span class="math display">\[
\begin{equation}
H({P,\,Q}) =  - \sum_{i}{p_{i} \log{q_{i}}}
\label{eq8}
\end{equation}
\]</span> where <span class="math display">\[\log{q_{i}}\]</span> denotes the uncertainty (bits count) of each events formulated by the probability distribution <span class="math display">\[Q\]</span> and <span class="math display">\[p_{i}\]</span> denotes the target real probability distribution. Note that the cross entropy is an absolute entropy of probability distribution <span class="math display">\[Q\]</span> given probability distribution <span class="math display">\[P\]</span> because we use the absolute uncertainty <span class="math display">\[\log{q_{i}}\]</span> for each event. To some extent, it is similar to the conditional probability distribution. It measures the total bits we need to clear the uncertainty of probability distribution <span class="math display">\[Q\]</span>, given <span class="math display">\[P\]</span>.</p>
<h2 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h2>
<p>Both entropy and cross entropy evaluate the total bits needed to clear the total uncertainty of a probability distribution. How much bits we need in order to make a probability distribution <span class="math display">\[Q\]</span> identical to the target one <span class="math display">\[P\]</span>. Kullback–Leibler Divergence is proposed to address this issue. It is a measurement of how one probability distribution <span class="math display">\[Q\]</span> is similar with a target probability distribution <span class="math display">\[P\]</span>. <span class="math display">\[
\begin{equation}
{D_{kl}}({p \| q }) = -\sum_{i = 0} {p_{i}[ {\log {q_{i}} - \log{p_{i}}}]}
\label{eq9}
\end{equation}
\]</span> Eq. <span class="math inline">\(\eqref{eq9}\)</span> defines the Kullback-Leibler Divergence.</p>
<p>The more common way to see Kullback-Leibler divergence written is as follows: <span class="math display">\[
\begin{equation}
{D_{kl}}({p \| q }) = -\sum_{i = 0} {p_{i}[ {\log\frac{q_{i}}{p_{i}}}]}
\label{eq10}
\end{equation}
\]</span></p>
<p>Kullback-Leibler divergence is a relative entropy of a probability distribution <span class="math display">\[Q\]</span> with respect to <span class="math display">\[P\]</span>. If <span class="math display">\[{D_{kl}}({p \| q })\]</span> is equal to <span class="math display">\[0\]</span>, then we can say that the probability distribution <span class="math display">\[Q\]</span> is identical to <span class="math display">\[P\]</span>.</p>
<p>Compare Eq. <span class="math inline">\(\eqref{eq9}\)</span>, <span class="math inline">\(\eqref{eq7}\)</span>, and <span class="math inline">\(\eqref{eq8}\)</span>, we can derive: <span class="math display">\[
\begin{equation}
{D_{kl}}( {P \| Q}) = H({P,\,Q}) - H(P)
\label{eq11}
\end{equation}
\]</span> From Eq. <span class="math inline">\(\eqref{eq11}\)</span>, we draw a conclusion that the cross entropy of <span class="math display">\[Q\]</span> and <span class="math display">\[P\]</span> is same with the entropy <span class="math display">\[P\]</span> when <span class="math display">\[{D_{kl}}({p \| q })\]</span> is equal to <span class="math display">\[0\]</span>.</p>
<p>In the context of machine learning and deep learning, cross entropy loss is a famous loss in solving classification problems, which we want the predicted probability distribution same as the real one. Eq. <span class="math inline">\(\eqref{eq11}\)</span> gives us a clear explanation why we can obtain the optimal solution by minimizing the cross entropy.</p>
<h3 id="properties">Properties</h3>
<ul>
<li>The Kullback–Leibler divergence is always non-negative. <span class="math display">\[
\begin{equation}
{D_{kl}}\left( {P\left\| Q \right.} \right) \ge 0
\end{equation}
\]</span></li>
<li>The Kullback–Leibler divergence does not conform to commutative law. <span class="math display">\[
\begin{equation}
{D_{kl}}\left( {P\left\| Q \right.} \right) \ne {D_{kl}}\left( {Q\left\| P \right.} \right)
\end{equation}
\]</span></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://eranraviv.com/intuitive-explanation-of-entropy/">https://eranraviv.com/intuitive-explanation-of-entropy/</a></li>
<li><a target="_blank" rel="noopener" href="https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy">https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy</a></li>
<li><a target="_blank" rel="noopener" href="http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf">http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf</a></li>
<li><span class="citation" data-cites="cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a">[https://medium.com/@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a]</span>(https://medium.com/<span class="citation" data-cites="cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a">@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a</span>)</li>
</ul>
</body></html>
    </div>

    
    
    
        <div class="reward-container">
  <div>Please buy me a cup of coffee if this article helps you.</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="Dr. Zhao WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Dr. Zhao Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Dr. Zhao
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/" title="An Intuitive Understanding of Entropy, Cross Entropy and Kullback Leibler Divergence">http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Maths/" rel="tag"><i class="fa fa-tag"></i> Maths</a>
              <a href="/tags/Probability/" rel="tag"><i class="fa fa-tag"></i> Probability</a>
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/12/04/Usage-Experiences-and-Skills-of-Ubuntu/" rel="prev" title="Usage Experiences and Skills of Ubuntu">
      <i class="fa fa-chevron-left"></i> Usage Experiences and Skills of Ubuntu
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/12/07/Ubuntu-19-04-Installation-and-Development-Environment-Setups/" rel="next" title="Ubuntu 18.04 Installation and Development Environment Setups">
      Ubuntu 18.04 Installation and Development Environment Setups <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#entropy"><span class="nav-number">1.</span> <span class="nav-text">Entropy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#information"><span class="nav-number">1.1.</span> <span class="nav-text">Information</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#uncertainty-and-entropy"><span class="nav-number">1.2.</span> <span class="nav-text">Uncertainty and Entropy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cross-entropy"><span class="nav-number">2.</span> <span class="nav-text">Cross Entropy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kullback-leibler-divergence"><span class="nav-number">3.</span> <span class="nav-text">Kullback-Leibler Divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#properties"><span class="nav-number">3.1.</span> <span class="nav-text">Properties</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dr. Zhao"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Dr. Zhao</p>
  <div class="site-description" itemprop="description">A technical blog for sharing articles related to Computer Vision, Robotics & Deep Learning.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YoungsonZhao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YoungsonZhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyongsheng@zju.edu.cn" title="E-Mail → mailto:zhaoyongsheng@zju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yongshengzhao" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yongshengzhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/yourname" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:swardman@126.com?add" title="Skype → skype:swardman@126.com?add" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2023003977号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dr. Zhao</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">264k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">4:01</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5de3d9a492e22c4b" async="async"></script>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.9.0/lozad.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>














  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout: 3000,
        priority: true,
        ignores: [uri => uri.includes('#'),uri => uri == 'http://blog.zhaoyongsheng.com/2019/12/04/An-Intuitive-Understanding-of-Kullback-Leibler-Divergence-and-Cross-Entropy/',]
      });
      });
  </script>


<script>
NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'n9rtSD8wdvwav5LXeOerMH5C-gzGzoHsz',
    appKey: 'xcMirBCoS6Ecbog1fFmXEwhu',
    placeholder: "You are welcome to leave a comment.",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
